{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データベースをパース "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-10T07:36:39.571375Z",
     "start_time": "2020-05-10T07:36:38.399505Z"
    }
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import datetime\n",
    "import pprint\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-10T07:37:00.553458Z",
     "start_time": "2020-05-10T07:37:00.526524Z"
    }
   },
   "outputs": [],
   "source": [
    "from pdf_parser_ver2 import PdfParser, DirectoryPdfParser, PdfParserCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データベースの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-10T07:38:38.403059Z",
     "start_time": "2020-05-10T07:38:38.369154Z"
    }
   },
   "outputs": [],
   "source": [
    "client = MongoClient('localhost', 27017)\n",
    "db = client[\"db\"]\n",
    "collection = db[\"papers\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データベースから正規表現で検索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-10T07:39:58.655883Z",
     "start_time": "2020-05-10T07:39:58.632783Z"
    }
   },
   "outputs": [],
   "source": [
    "count_patterns = [re.compile(\"ディープラーニング|深層学習\"),\n",
    "                  re.compile(\"CNN|ニューラルネットワーク\"),\n",
    "                  re.compile(\"VAE|変分オートエンコーダ\"),\n",
    "                  re.compile(\"GAN\"),\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 複数の正規表現で検索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-10T07:44:08.476423Z",
     "start_time": "2020-05-10T07:44:08.073522Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "find_result = collection.find({\"$or\":[{\"content.序論\":re.compile(\"VAE|変分オートエンコーダ\")},\n",
    "                                      {\"content.序論\":re.compile(\"GAN\")}]})\n",
    "\n",
    "counter = 0\n",
    "for i in find_result:\n",
    "    counter += 1\n",
    "    #pprint.pprint(i)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### パターンのリストで指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-10T07:48:34.882534Z",
     "start_time": "2020-05-10T07:48:34.333996Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n"
     ]
    }
   ],
   "source": [
    "content_name = \"序論\"\n",
    "find_result = collection.find(filter={\"$or\":[{\"content.\"+content_name:i} for i in count_patterns]}).sort(\"date\")  # dateはないけど\n",
    "counter = 0\n",
    "for i in find_result:\n",
    "    counter += 1\n",
    "    #pprint.pprint(i)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データベースから取得し，検索するクラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-10T07:52:32.674949Z",
     "start_time": "2020-05-10T07:52:32.648021Z"
    }
   },
   "outputs": [],
   "source": [
    "class PaperDataBaseLoadPattern():\n",
    "        def __init__(self, collection, find_patterns, content_name, find_max=100):\n",
    "            self.collection = collection\n",
    "            self.find_patterns = find_patterns\n",
    "            self.content_name = content_name\n",
    "            self.find_max = find_max\n",
    "        def load(self):\n",
    "            \"\"\"\n",
    "            検索結果からfind_max分だけ検索．呼んだ回数だけ検索結果が進む\n",
    "            \"\"\"\n",
    "            result = collection.find(filter={\"$or\":[{\"content.\"+self.content_name:i} for i in self.find_patterns]}).sort(\"date\")  # dateはないけど\n",
    "            out_list = []\n",
    "            iter_counter = 0\n",
    "            for paper in result:\n",
    "                out_list.append(paper)\n",
    "                iter_counter += 1\n",
    "                if iter_counter >= self.find_max-1:\n",
    "                    yield out_list\n",
    "                    out_list = []  # リストの初期化\n",
    "                    iter_counter = 0\n",
    "            yield out_list #全て終わったときに返す"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テストコード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-10T07:53:57.816239Z",
     "start_time": "2020-05-10T07:53:57.796291Z"
    }
   },
   "outputs": [],
   "source": [
    "find_patterns = [re.compile(\"ディープラーニング|深層学習\"),\n",
    "                 re.compile(\"CNN|ニューラルネットワーク\"),\n",
    "                 re.compile(\"VAE|変分オートエンコーダ\"),\n",
    "                 re.compile(\"GAN\"),\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-10T07:54:56.960930Z",
     "start_time": "2020-05-10T07:54:56.940994Z"
    }
   },
   "outputs": [],
   "source": [
    "database_loader = PaperDataBaseLoadPattern(collection=collection,\n",
    "                                           content_name=\"序論\",\n",
    "                                           find_patterns=find_patterns,\n",
    "                                           find_max=10\n",
    "                                           )\n",
    "database_loader_generator = database_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-10T07:57:48.882676Z",
     "start_time": "2020-05-10T07:57:47.568193Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'_id': ObjectId('5eb6bcab60e378f730f5b62f'),\n",
      "  'conf_name': 'SSII2019',\n",
      "  'content': {'序論': '近年，自動車の自動運転化が進んでおり，自動運転に\\n'\n",
      "                    '関する研究，開発が盛んに行われている．人の判断や\\n'\n",
      "                    '操作を必要としない完全な自動運転では，周辺環境を\\n'\n",
      "                    '認識し，事故リスク推定を行う必要がある. 認識の分野\\n'\n",
      "                    'では，深層学習を用いた検出手法 [1][2] 等にて，高い精\\n'\n",
      "                    '度が報告されている．一方，事故リスク推定としては，\\n'\n",
      "                    '手法 [3] 等が存在するが，実用に足る精度の手法が確立\\n'\n",
      "                    'しておらず，事故リスク推定手法の需要が高まってい\\n'\n",
      "                    'る．また、二輪車の事故が発生した際の致死率は，四輪\\n'\n",
      "                    '車の約 2 倍以上となっている [4]．しかし，二輪車に安\\n'\n",
      "                    '全運転支援の機能を搭載することは困難である．二輪\\n'\n",
      "                    '車は重心の移動によって自立, 旋回が可能であるため,\\n'\n",
      "                    '四輪車と異なり重心の制御も行う必要がある．ところ\\n'\n",
      "                    'が，重心は乗車している人の動作によって変化するため\\n'\n",
      "                    '汎用的な重心制御の技術が必要となる. しかしながら,\\n'\n",
      "                    '二輪車はその車体特性上, 多くの重心制御の部品 (バラ\\n'\n",
      "                    'ンサー) を搭載することは現実的ではない. そのため，\\n'\n",
      "                    '二輪車の安全運転支援機能の搭載は四輪車よりもコス\\n'\n",
      "                    'トが高く, 研究も四輪車に比べ少ないのが現状である．\\n'\n",
      "                    'よって，近い将来，完全自動運転の四輪車と人が運転\\n'\n",
      "                    'する二輪車が存在する車社会が予期され，完全自動運\\n'\n",
      "                    '転の四輪車は，危険な二輪車との交通事故を回避する\\n'\n",
      "                    '必要性が出てくる．そこで本研究では, 再帰型深層学習を用いて, 車載カ\\n'\n",
      "                    'メラ映像の時系列画像データから自動車の交通事故リス\\n'\n",
      "                    'クを推定する手法 [3] に位置情報を付加することで, よ\\n'\n",
      "                    'り高精度な二輪車との交通事故リスク推定を実現する.2 '\n",
      "                    '物体領域に注目した交通事故リスク推定車載カメラ映像データ解析には，再帰型深層学習の 1\\n'\n",
      "                    'つである Long Short Term Memory(LSTM)[5] を利用\\n'\n",
      "                    'する. LSTM は，時系列データを解析可能であり，通\\n'\n",
      "                    '常，LSTM の入力には, 映像中の任意のフレーム画像\\n'\n",
      "                    'を Convolutional \\u3000 Neural \\u3000 Network(CNN:畳み込み\\n'\n",
      "                    'ニューラルネットワーク) に通し, 生成したフレーム特\\n'\n",
      "                    '徴量 xF を用いる．しかし，二輪車は四輪車と比べ, 車\\n'\n",
      "                    '体が小さいため, 二輪車の特徴は画像特徴量として表れCopyright © SSII 2019. All '\n",
      "                    'Rights Reserved.- IS1-24 -図 1: 従来手法 [3] '\n",
      "                    'における交通事故リスク推定器の構成にくい．そこで，手法 [3] にて提案されている画像中の\\n'\n",
      "                    '注目した物体領域の特徴量を抽出する Dynamic Spatial\\n'\n",
      "                    'Attention(DSA) を用いる．従来手法では，映像中の任\\n'\n",
      "                    '意のフレーム画像から抽出したフレーム特徴量 xF と物\\n'\n",
      "                    '体特徴量を xD を結合し、LSTM の入力とすることで，\\n'\n",
      "                    '周辺環境と物体領域に注目した交通事故リスク推定が\\n'\n",
      "                    '可能となっている (図 1). 本章では，従来手法における\\n'\n",
      "                    '特徴量の抽出手法および，LSTM について説明する．\\n'\n",
      "                    '2.1 フレーム特徴量フレーム画像一枚から特徴を得る際に，使用される手\\n'\n",
      "                    '法として，図 2 に示すような，CNN が存在する．CNN\\n'\n",
      "                    'は画像分類に用いられることが多く，一般的に，入力\\n'\n",
      "                    '層，畳み込み層，プーリング層，全結合層，出力層に分\\n'\n",
      "                    'かれている．CNN において，入力層は入力画像に相当\\n'\n",
      "                    'する．畳み込み層では，畳み込み処理によって，フィル\\n'\n",
      "                    'タをかけ，フィルタの表す特徴パターンを特徴マップに\\n'\n",
      "                    '出力する．畳み込み処理により，入力画像中のどこに\\n'\n",
      "                    'どのような特徴パターンが存在するかを抽出すること\\n'\n",
      "                    'ができる．プーリング層では，畳み込み処理によって\\n'\n",
      "                    '抽出された特徴マップに対し，一定空間内の特徴量の\\n'\n",
      "                    '最大値等を抽出する．プーリングによって，特徴を集\\n'\n",
      "                    '約するとともに，畳み込み処理によって生じた微小な\\n'\n",
      "                    '位置変化に頑健となる．全結合層では，特徴量を固定\\n'\n",
      "                    '次元へ削減するとともに，特徴量に基づく分類を行う\\n'\n",
      "                    'ために，1 つ前の層の全てのニューロンの出力から，重\\n'\n",
      "                    'み付き線形和とる，これを，固定次元数回繰り返すこ\\n'\n",
      "                    'とで，特徴量を維持した固定次元ベクトルとなる．出\\n'\n",
      "                    '力層では，一般的に，全結合，または，プーリングに\\n'\n",
      "                    'よって，任意の固定次元に次元削減された特徴量が出IS1-24\\n'\n",
      "                    'SO1-24第25回画像センシングシンポジウム図 2: CNN の構成力される．フレーム特徴量 xF は，出力層の '\n",
      "                    '1，2 層手\\n'\n",
      "                    '前の層から抽出され，フレーム画像全体の特徴を有し\\n'\n",
      "                    'ているため，周辺環境を考慮することが可能である．2.2 物体特徴量2.2.1 Faster R-CNN従来手法 '\n",
      "                    '[3] では，物体の領域に注目するために，物\\n'\n",
      "                    '体検出器 Faster R-CNN[1] を用いる．Faster R-CNN\\n'\n",
      "                    'は end to end で学習を行える深層学習の一つである．\\n'\n",
      "                    '図 3 に示すように，Faster R-CNN では，2.1 で述べた\\n'\n",
      "                    '特徴マップを抽出する CNN に Region Proposal Net-\\n'\n",
      "                    'work(RPN)，RoI Pooling を追加することで，物体検\\n'\n",
      "                    '出を可能としている．画像内の物体候補となる箇所を\\n'\n",
      "                    '抽出するために，RPN では物体の矩形候補中心となる\\n'\n",
      "                    'アンカーを一定間隔で生成する．生成したアンカーか\\n'\n",
      "                    'ら，図 4 に示すように，物体の矩形として最適な矩形を\\n'\n",
      "                    '推定する．RPN によって，矩形を抽出した後，抽出し\\n'\n",
      "                    'た矩形領域の特徴量からクラス分類を行うために全結\\n'\n",
      "                    '合層へとつなげるが，全結合層の入力は固定次元であ\\n'\n",
      "                    'るため，大きさの異なる領域の特徴量を固定次元に次\\n'\n",
      "                    '元削減する必要がある．そのため，ROI Pooling によっ\\n'\n",
      "                    'て，各領域の可変長な特徴量を，固定次元に次元削減\\n'\n",
      "                    'する．ROI Pooling によって得られた特徴量を，全結合\\n'\n",
      "                    '層に通し，矩形回帰，クラス分類回帰を実行し，最終\\n'\n",
      "                    '的な物体の矩形，および，物体のクラスが出力される．図 3: Faster R-CNN の構成 '\n",
      "                    '[1]Copyright © SSII 2019. All Rights Reserved.- IS1-24 -図 '\n",
      "                    '4: Faster R-CNN 矩形回帰とクラス分類回帰 [1]2.2.2 Dynamic Soft '\n",
      "                    'Attention(DSA)物体の領域に注目して特徴量を抽出する場合，物体\\n'\n",
      "                    'はフレーム画像一枚中に複数存在することがある．し\\n'\n",
      "                    'かし，LSTM 等の学習器の入力は，一般的に固定次元\\n'\n",
      "                    'であるため，複数の特徴量を固定次元に次元削減する\\n'\n",
      "                    '必要がある．Dynamic Soft Attention(DSA) では，検\\n'\n",
      "                    '出された各危険候補物体の画像特徴量ごとに重要度を\\n'\n",
      "                    '算出し，各危険候補物体の画像特徴量との重み付き線\\n'\n",
      "                    '形和をとることで，複数の危険候補物体に注目した固定\\n'\n",
      "                    '次元の物体特徴量 xD の抽出を可能とする．DSA の構\\n'\n",
      "                    '成を，図 5 に示す．物体検出器 Faster R-CNN の全結合\\n'\n",
      "                    '層から抽出した各危険候補物体の画像特徴量と，LSTM\\n'\n",
      "                    'の一時刻前の中間活性化特徴量から，各危険候補物体\\n'\n",
      "                    'の重要度は算出される．時刻 t のフレーム画像一枚から，検出された各危険候\\n'\n",
      "                    '補物体の画像特徴量を xt = {ˆxj\\n'\n",
      "                    '}，各危険候補物体の重\\n'\n",
      "                    '} とする．ここで，J は危険候補物体\\n'\n",
      "                    '要度を αt = {αj\\n'\n",
      "                    'の数である (j ∈ 1, .., J)．このとき，DSA によって得ら\\n'\n",
      "                    'れる物体特徴量 xD は，式 1 のように表せられる．重要\\n'\n",
      "                    '} は，式 2 のように計算される．式 2 中の\\n'\n",
      "                    '度 αt = {αj\\n'\n",
      "                    'e は式 3 によって計算される．ここで，w, W e, U e, be\\n'\n",
      "                    'は，それぞれ学習パラメータであり，ht−1 は，LSTM\\n'\n",
      "                    'の一時刻前の中間活性化特徴量である．ttt∑ jJαj\\n'\n",
      "                    't ˆxjxD =(1)t∑=1exp(ej\\n'\n",
      "                    't )\\n'\n",
      "                    '=1 exp(ej\\n'\n",
      "                    't )αj(2)t =J jt = wT tanh(W eht−1 + U exj\\n'\n",
      "                    'ej(3)t + be)IS1-24\\n'\n",
      "                    'SO1-24第25回画像センシングシンポジウム図 5: DSA の構成2.3 Long Short Term '\n",
      "                    'Memory(LSTM)時系列情報を扱う手法として，再帰型深層学習が存\\n'\n",
      "                    '在する．特に，動画像の場合には，再帰型深層学習の\\n'\n",
      "                    '一つである Long Short Term Memory(LSTM)[5] が用\\n'\n",
      "                    'いられることが多い. LSTM の基となったリカレント\\n'\n",
      "                    'ニューラルネットワーク（RNN）では，図 6 に示すよ\\n'\n",
      "                    'うに，時刻 t の中間層の中間活性化特徴を，時刻 t+1 の\\n'\n",
      "                    '中間層にも伝播させることで，時系列解析を可能とし\\n'\n",
      "                    'た．しかし，基本的な構造では，長期の時系列データの\\n'\n",
      "                    '場合，学習のための勾配が消失してしまうという問題\\n'\n",
      "                    'がある．そこで，LSTM では，中間層のニューロンを，\\n'\n",
      "                    'LSTMblock(図 7) に置き換えることで，この問題を解\\n'\n",
      "                    '決した．LSTMblock は，忘却ゲート，入力ゲート，出\\n'\n",
      "                    '力ゲートにて構成される．時刻 t における LSTMblock\\n'\n",
      "                    'の内部情報と出力を，時刻 t + 1 の LSTMblock にも伝\\n'\n",
      "                    '播させ，忘却ゲートにて，一時刻前の内部情報をどの\\n'\n",
      "                    '程度維持，忘却するか．入力ゲートにて，入力を内部\\n'\n",
      "                    '情報として，どの程度取り入れるか．出力ゲートにて，\\n'\n",
      "                    '内部情報をもとに，入力から得た値をどの程度出力す\\n'\n",
      "                    'るかを LSTMblock 内部で学習することで，RNN より\\n'\n",
      "                    'も高精度な時系列データ解析が可能となっている．図 7 中の xt は時刻 t における入力データ，ht−1 は一\\n'\n",
      "                    '時刻前の出力である．また，it，f t, ct, ot, ht は，式\\n'\n",
      "                    '4-8 にて表される．ここで，W ∗，U∗，V ∗，b∗ は学習\\n'\n",
      "                    'パラメータ，σ ，ρ は活性化関数，⊙ は要素積である．Copyright © SSII 2019. All '\n",
      "                    'Rights Reserved.- IS1-24 -図 6: RNN の基本構造図 7: LSTMblock '\n",
      "                    'の構成f t = σ\\u3000 (W f xt + U f ht−1 + V f ct−1 + bf '\n",
      "                    ')(4)it = σ\\u3000 (W ixt + U iht−1 + V ict−1 + bi)\\n'\n",
      "                    '⊙ ct−1 + it ⊙ ρ(W cxt + U cht−1 + bc)\\n'\n",
      "                    'ot = σ\\u3000 (W oxt + U oht−1 + V oct−1 + bo)(5)ct = f '\n",
      "                    't(6)(7)ht = ot ⊙ ρ(ct)(8)2.4 損失関数従来手法では，LSTM '\n",
      "                    '学習のための損失関数として\\n'\n",
      "                    'Anticipation loss[3] を提案している．事故動画の損失関\\n'\n",
      "                    '数は式 9，非事故動画の損失関数は式 10 にて表される．∑ tLp({at}) '\n",
      "                    '=−e−max(0,y−t)log(ap\\n'\n",
      "                    't )∑ t(9)Ln({at}) =−log(1 − an\\n'\n",
      "                    't )(10)ここで，apt は事故動画の時刻 t における LSTM の出\\n'\n",
      "                    '力した交通事故リスク，同様に an\\n'\n",
      "                    't は非事故動画の時刻\\n'\n",
      "                    't における LSTM の出力した交通事故リスクである．y\\n'\n",
      "                    'は事故の起きる時刻である．全ての時刻において勾配\\n'\n",
      "                    'を同一にすると，事故の直前の特徴に注目した学習が\\n'\n",
      "                    'できない．そこで，式 9 では，事故発生時刻に近づく\\n'\n",
      "                    'につれて勾配が大きくなるように設計されている．一\\n'\n",
      "                    '方，式 10 では，常に一様な勾配となるように設計され\\n'\n",
      "                    'ている．IS1-24\\n'\n",
      "                    'SO1-24第25回画像センシングシンポジウム図 8: 位置特徴量抽出手法3 '\n",
      "                    '位置特徴量交通事故総合分析センターの調査による二輪車事故\\n'\n",
      "                    'の特徴 [6] より，二輪車と自車の接触事故が発生する場\\n'\n",
      "                    '合，出会い頭，右左折時が事故状況として多いことが分\\n'\n",
      "                    'かっている．これは自車と二輪車が接触事故を起こす\\n'\n",
      "                    '際，自車の前方の車載カメラから得られる映像上では，\\n'\n",
      "                    '二輪車が画面上の左右から現れ，画面上を左右に移動\\n'\n",
      "                    'していることが多いということである．そのため，接触\\n'\n",
      "                    '事故が発生した際の二輪車の左右の位置情報，運動量\\n'\n",
      "                    'は，交通事故リスク推定の精度向上に有効であると考\\n'\n",
      "                    'えられる．そこで本手法では，CNN の特徴マップから\\n'\n",
      "                    '位置情報を抽出する手法を提案し，従来手法の LSTM\\n'\n",
      "                    'の入力に加える．CNN を用いて，フレーム特徴量を抽出する際，畳み\\n'\n",
      "                    '込み処理によって得られる特徴マップには，元画像の位\\n'\n",
      "                    '置情報が維持されている．そこで，図 8 に示すように，\\n'\n",
      "                    '特徴マップに対し，Faster R-CNN によって得られた矩\\n'\n",
      "                    '形情報を，特徴マップに反映し，二輪車の矩形が存在\\n'\n",
      "                    'する箇所のみ特徴を残すマスク処理を行う．マスク処\\n'\n",
      "                    '理によって，矩形の存在しない箇所の特徴量は 0 ベク\\n'\n",
      "                    'トルとなった特徴マップが得られる．マスク処理後の\\n'\n",
      "                    '特徴マップを，全結合層に通すことで，位置特徴量 xP\\n'\n",
      "                    'の抽出を行った．4 評価実験4.1 実装\\n'\n",
      "                    '4.1.1 交通事故リスク推定器の構成提案する交通事故リスク推定器の構成を図 9 に示す．\\n'\n",
      "                    '入力として，フレーム特徴量 xF，物体特徴量 xD，位置\\n'\n",
      "                    '特徴量 xP を LSTM へ与える形とした．LSTM は，損失\\n'\n",
      "                    '関数を含め，従来手法 [3] と同様の仕様とした．フレー\\n'\n",
      "                    'ム特徴量の抽出および，Faster R-CNN の CNN として，\\n'\n",
      "                    'MSCOCO[7] にて事前学習を行った 50 層の ResNet50[8]\\n'\n",
      "                    'を用いた．また，位置特徴量の抽出において用いる CNN\\n'\n",
      "                    'は 16 層の VGG16[9] とした．これは ResNet50 では，特\\n'\n",
      "                    '徴マップ生成後の次元削減に，特徴マップの 1 チャンネ\\n'\n",
      "                    'ル全体に対しプーリングを行う Global Average Pooling\\n'\n",
      "                    'が行われているため，位置情報が消失すると考えられ\\n'\n",
      "                    'たからである．Copyright © SSII 2019. All Rights Reserved.- '\n",
      "                    'IS1-24 -図 9: 交通事故リスク推定器の構成 (提案)4.1.2 Dashcam Accident '\n",
      "                    'Dataset(DAD)Dashcam Accident Dataset(DAD)[3] は台湾にて撮\\n'\n",
      "                    '影された危険シーンを含むデータセットである．1 つの\\n'\n",
      "                    '動画は，5 秒 100 フレームから構成され，事故動画で\\n'\n",
      "                    'は 80 フレーム目で事故が発生する．DAD の問題点と\\n'\n",
      "                    'して，事故動画に他車と他車の事故動画が多数含まれ\\n'\n",
      "                    'ることがあげられる．自車のドライブレコーダーの映\\n'\n",
      "                    '像から交通事故リスクを推定する場合，自車と他車の\\n'\n",
      "                    '関係性が重要であり，他車と他車の事故動画は学習の\\n'\n",
      "                    '妨げになると考えられる．そのため，自車 (四輪車，自\\n'\n",
      "                    '動二輪車) と二輪車との事故動画 49 動画，無事故動画\\n'\n",
      "                    '151 動画を評価実験のデータセットとして用いた．4.2 '\n",
      "                    '二輪車の交通事故リスク推定の定量評価提案手法の有効性を確認するために, LSTM に入力\\n'\n",
      "                    'する特徴量の組合せを表 1 のように変化させ，定量評\\n'\n",
      "                    '価実験を行った．評価には，学習 50 エポック中の最大\\n'\n",
      "                    'の Area Under Curve(AUC) および，最大 AUC 時の\\n'\n",
      "                    'Average time to accident(ATTA) を用いた. 5 分割交\\n'\n",
      "                    '差検定により得られた評価結果を表 1 にまとめる.4.2.1 Area Under '\n",
      "                    'Curve(AUC)交通事故リスク推定器から時刻 t における交通事故\\n'\n",
      "                    'リスク at を出力し，閾値 r(0 ≤ r ≤ 1) よりも交通事\\n'\n",
      "                    '故リスク at が大きいとき，事故あり動画として判定す\\n'\n",
      "                    'る．この閾値 r ごとに適合率，再現率を算出し，適合\\n'\n",
      "                    '率-再現率曲線 (図 10) を作成する．この曲線の面積か\\n'\n",
      "                    'ら AUC を算出することができる．この指標によって，\\n'\n",
      "                    '交通事故リスク推定の正確性を評価可能である．4.2.2 Average time to '\n",
      "                    'accident(ATTA)交通事故リスク推定器から時刻 t における交通事故\\n'\n",
      "                    'リスク at を出力し，閾値 r(0 ≤ r ≤ 1) を最初に超えた\\n'\n",
      "                    '時刻を事故あり判定の時刻 (TTA) とする．閾値 r のと\\n'\n",
      "                    'きの事故あり判定時刻の平均から ATTA を算出するこ\\n'\n",
      "                    'とができる．この指標によって，事故リスク推定の事\\n'\n",
      "                    '前予測の早さを評価可能である．IS1-24\\n'\n",
      "                    'SO1-24第25回画像センシングシンポジウム表 1: 5 分割交差検定比較結果特徴量DSA Frame 手法 '\n",
      "                    '[3] 提案 1 提案 2 提案 3\\n'\n",
      "                    '〇物体特徴量 xDフレーム特徴量 xF〇位置特徴量 '\n",
      "                    'xPAUC0.2820.761ATTA[s]3.9501.240図 10: 適合率-再現率曲線例4.3 '\n",
      "                    '二輪車の交通事故リスク推定の定性評価手法 [3]，提案 1,2,3 の特徴量の組合せで学習を行っ\\n'\n",
      "                    'た二輪車との交通事故リスク推定器の各手法のリスク\\n'\n",
      "                    '推定推移と，実際の動画の事故状況との関係性につい\\n'\n",
      "                    'て，定性評価を行った．画面上にて左右から二輪車が\\n'\n",
      "                    '飛び出してくる交通事故動画での事故リスク推定の様\\n'\n",
      "                    '子を図 11，図 12 に，画面中央の二輪車と追突する事故\\n'\n",
      "                    '動画での交通事故リスク推定の様子を図 13 に示す．ま\\n'\n",
      "                    'た，非事故動画の事故リスク推定の様子を図 14 に示す．4.4 実験結果・考察4.2 '\n",
      "                    '節の結果から，位置情報を考慮することで，AUC\\n'\n",
      "                    'が上昇し，交通事故リスク推定の正確性が向上するこ\\n'\n",
      "                    'とを確認できた．物体特徴量 xD のみでは，二輪車が小\\n'\n",
      "                    'さく，二輪車の状態変化の特徴だけでは，推定はうまく\\n'\n",
      "                    'いかない．しかし，位置特徴量 xP を追加することで，\\n'\n",
      "                    '精度が上昇する．これは，位置特徴量により，画面上の\\n'\n",
      "                    'ある箇所になんらかの特徴があることがわかり，その\\n'\n",
      "                    '箇所のさらに詳細な物体特徴までわかることで，どの\\n'\n",
      "                    '位置にどのような特徴が存在するかを考慮できるので，\\n'\n",
      "                    '精度が向上するからだと考えられる．一方，ATTA に\\n'\n",
      "                    'ついては，位置特徴量を加えた場合，事故と予測する時\\n'\n",
      "                    '刻は遅くなることが確認できる．物体特徴量は Faster\\n'\n",
      "                    'R-CNN の検出結果に依存するため，危険候補物体の矩\\n'\n",
      "                    '形が得られない限りは交通事故リスクを正しく推定で\\n'\n",
      "                    'きないことから，事故予測が遅れると考えられる．以\\n'\n",
      "                    '上の 4.2 節の結果から，位置特徴量を加えることで，事\\n'\n",
      "                    '故予測は遅くなるが交通事故リスク推定の正確さは上Copyright © SSII 2019. All Rights '\n",
      "                    'Reserved.- IS1-24 -〇〇\\n'\n",
      "                    '〇〇\\n'\n",
      "                    '〇\\n'\n",
      "                    '〇〇\\n'\n",
      "                    '〇〇0.7980.7980.8040.7521.1311.2030.9281.314昇することがわかる．4.3 '\n",
      "                    '節の結果から，位置情報を考慮することで，手法\\n'\n",
      "                    '[3] よりも左右の運動量に強く反応していることが確認\\n'\n",
      "                    'できる．図 11, 図 12 から，従来手法では事故動画に対\\n'\n",
      "                    'し，交通事故リスクの値は高くなっているが，事故発\\n'\n",
      "                    '生のタイミングを正しく推定できていない．一方，位\\n'\n",
      "                    '置情報を考慮した提案 1,2,3 の手法では，事故発生 80\\n'\n",
      "                    'フレームの手前である 60 フレーム前後，事故発生のお\\n'\n",
      "                    'よそ 1 秒前にて，交通事故リスクが上がっており，位\\n'\n",
      "                    '置特徴量の有効性を確認できる．また，位置特徴量は，\\n'\n",
      "                    'Faster R-CNN の矩形の大きさに依存するので，物体の\\n'\n",
      "                    '大きさも考慮できる可能性があり，図 11 の結果から，\\n'\n",
      "                    '自車と同速度で左右から接近し，徐々に危険候補物体\\n'\n",
      "                    'の矩形が大きくなる場合の事故リスクにも，位置特徴\\n'\n",
      "                    '量 xP は有効であったと考えられる．しかし，図 13 の\\n'\n",
      "                    '結果から，追突に近い状況では，二輪車の後方からの画\\n'\n",
      "                    '像特徴量では交通事故リスクを判定できておらず，ま\\n'\n",
      "                    'た接近しすぎると危険候補物体矩形が得られないため，\\n'\n",
      "                    '位置特徴量も意味をなさない．したがって，追突など\\n'\n",
      "                    'の事故状況には弱いと考えれれる．そのため，二輪車\\n'\n",
      "                    'に乗車している人間も危険候補物体として検出し，接\\n'\n",
      "                    '近しても位置特徴量 xP が得られるようにするなどの改\\n'\n",
      "                    '善が必要である．5 まとめ本研究では，二輪車に焦点をあて，車載カメラ映像\\n'\n",
      "                    'からの二輪車との交通事故リスク推定の精度向上のた\\n'\n",
      "                    'めに，画像中の危険候補物体の位置や動きを考慮する\\n'\n",
      "                    '位置特徴量 xP の抽出手法を提案した．提案した位置特\\n'\n",
      "                    '徴量の有効性を調べるために，従来手法 [3] の交通事故\\n'\n",
      "                    'リスク推定器の LSTM の入力に位置特徴量 xP を追加\\n'\n",
      "                    'し，位置特徴量 xP がある場合，ない場合とで比較実験\\n'\n",
      "                    'を行った．実験結果から，位置特徴量 xP を付与するこ\\n'\n",
      "                    'とで，左右の運動量に対し，強く反応し，事故リスク\\n'\n",
      "                    '推定精度が向上することが確認できた．しかし，正面\\n'\n",
      "                    'からの追突などには，比較的弱いことがわかった．そ\\n'\n",
      "                    'のため，危険候補物体の接近を考慮する等の改善が必\\n'\n",
      "                    '要である．また，現在は動画ごとに事故，無事故のア\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    'ノテーションが付与されているが，フレームごとに危IS1-24\\n'\n",
      "                    'SO1-24第25回画像センシングシンポジウム険度アノテーションを付与し，事故発生までの時刻を\\n'\n",
      "                    '考慮した学習などを今後行っていく．参考文献[1] Shaoqing Ren，Kaiming He，Ross '\n",
      "                    'Girshick，Jian\\n'\n",
      "                    'Sun，“ Faster R-CNN: Towards Real-Time Object\\n'\n",
      "                    'Detection with Region Proposal Networks ”，Neu-\\n'\n",
      "                    'ral Information Processing Systems 2015，2015.[2] Wei Liu, '\n",
      "                    'Dragomir Anguelov,Dumitru Er-han,Christian Szegedy,Scott '\n",
      "                    'Reed, Cheng-Yang\\n'\n",
      "                    'Fu, Alexander C. Berg,“SSD:Single ShotMulti Box\\n'\n",
      "                    'Detector ”,European Conference on Computer\\n'\n",
      "                    'Vision 2016,pp21-37,2016.[3] Fu-Hsiang Chan, Yu-Ting '\n",
      "                    'Chen, Yu Xiang, Min\\n'\n",
      "                    'Sun,“Anticipating Accidents in Dashcam Videos”,\\n'\n",
      "                    'ACCV2016, pp.136-153, 2016.[4] 警察庁交通局，“ 平成 29 '\n",
      "                    '年度中の交通事故の発生状況 ”，2018.[5] Hochreiter，Sepp and Jurgen '\n",
      "                    'Schmidhuber．”Long\\n'\n",
      "                    'short-term memory ”Neural computation，Vol.9，\\n'\n",
      "                    'pp.1735-1780，1997．[6] (財) 交通事故総合分析センター,“ 二輪車事故の特徴 '\n",
      "                    '”，ITARDA No.91，2011.[7] Tsung-Yi Lin，Michael Maire，Serge '\n",
      "                    'Belongie，\\n'\n",
      "                    'Lubomir Bourdev，Ross Girshick，James Hays，\\n'\n",
      "                    'Pietro Perona，Deva Ramanan，C. Lawrence Zit-図 11: '\n",
      "                    '事故動画と事故リスクの推移例 1(飛び出し)Copyright © SSII 2019. All Rights '\n",
      "                    'Reserved.- IS1-24 -nick，Piotr Doll´ar，“ Microsoft coco: '\n",
      "                    'Common ob-\\n'\n",
      "                    'jects in context ”，European Conference on Com-\\n'\n",
      "                    'puter Vision 2014，2014.[8] Kaiming He，Xiangyu '\n",
      "                    'Zhang，Shaoqing Ren，Jian\\n'\n",
      "                    'Sun，“ Deep Residual Learning for Image Recogni-\\n'\n",
      "                    'tion ”，Computer Vision and Pattern Recognition\\n'\n",
      "                    '2016，pp.770-778，2016．[9] Karen Simonyan，Andrew '\n",
      "                    'Zisserman，“ VERY\\n'\n",
      "                    'DEEP CONVOLUTIONAL NETWORKS FOR\\n'\n",
      "                    'LARGE-SCALE IMAGE RECOGNITION ”，In-\\n'\n",
      "                    'ternational Conference for Learning Representa-\\n'\n",
      "                    'tions 2015，2015．IS1-24\\n'\n",
      "                    'SO1-24第25回画像センシングシンポジウム図 12: 事故動画と事故リスクの推移例 2(飛び出し)図 13: '\n",
      "                    '事故動画と事故リスクの推移例 3(追突)Copyright © SSII 2019. All Rights '\n",
      "                    'Reserved.- IS1-24 -IS1-24\\n'\n",
      "                    'SO1-24第25回画像センシングシンポジウム図 14: 非事故動画と事故リスクの推移例 1Copyright © '\n",
      "                    'SSII 2019. All Rights Reserved.- IS1-24 -',\n",
      "              '結論': '本研究では，二輪車に焦点をあて，車載カメラ映像\\n'\n",
      "                    'からの二輪車との交通事故リスク推定の精度向上のた\\n'\n",
      "                    'めに，画像中の危険候補物体の位置や動きを考慮する\\n'\n",
      "                    '位置特徴量 xP の抽出手法を提案した．提案した位置特\\n'\n",
      "                    '徴量の有効性を調べるために，従来手法 [3] の交通事故\\n'\n",
      "                    'リスク推定器の LSTM の入力に位置特徴量 xP を追加\\n'\n",
      "                    'し，位置特徴量 xP がある場合，ない場合とで比較実験\\n'\n",
      "                    'を行った．実験結果から，位置特徴量 xP を付与するこ\\n'\n",
      "                    'とで，左右の運動量に対し，強く反応し，事故リスク\\n'\n",
      "                    '推定精度が向上することが確認できた．しかし，正面\\n'\n",
      "                    'からの追突などには，比較的弱いことがわかった．そ\\n'\n",
      "                    'のため，危険候補物体の接近を考慮する等の改善が必\\n'\n",
      "                    '要である．また，現在は動画ごとに事故，無事故のア\\n'\n",
      "                    'ノテーションが付与されているが，フレームごとに危IS1-24\\n'\n",
      "                    'SO1-24第25回画像センシングシンポジウム険度アノテーションを付与し，事故発生までの時刻を\\n'\n",
      "                    '考慮した学習などを今後行っていく．'},\n",
      "  'date': '2000-01-01 00:00:00',\n",
      "  'paper_title': '車載カメラ映像からの二輪車との交通事故リスク推定',\n",
      "  'pdf_name': 'IS1-24'},\n",
      " {'_id': ObjectId('5eb6bcab60e378f730f5b630'),\n",
      "  'conf_name': 'SSII2019',\n",
      "  'content': {'序論': '日本を始め，高齢化社会を迎えている先進諸国にお\\n'\n",
      "                    'いて，年齢を問わず安全・安心に活動できる社会イン\\n'\n",
      "                    'フラの整備は喫緊の課題である．考慮されるべき事柄\\n'\n",
      "                    'は多岐に渡るが，最も基本的かつ関心の高いものの一\\n'\n",
      "                    'つは，日常の移動手段（モビリティ）であろう．特に，\\n'\n",
      "                    '自由度が高く，また日常的な生活・活動の範囲を大幅\\n'\n",
      "                    'に広げることができる自動車の運転に対する関心は非\\n'\n",
      "                    '常に高く，高齢者の免許取得率も年々増加傾向にある．\\n'\n",
      "                    'しかしながら，加齢に伴う注視領域の狭窄，動体視力\\n'\n",
      "                    'や認知・判断機能の低下，さらには身体能力の低下に\\n'\n",
      "                    'よる操作の遅延など，運転技能や関連する能力は高齢\\n'\n",
      "                    '化と共に低下していくことが知られており [1, 2]，これ\\n'\n",
      "                    'が高齢者が関わる運転事故等のリスクを高める要因の\\n'\n",
      "                    '一つとなっている．このような背景を受け，高齢者に\\n'\n",
      "                    'よる自動車の安全・安心な運転を支援することのでき\\n'\n",
      "                    'る画像センシング技術への期待は，ますます高まって\\n'\n",
      "                    'いると言えるであろう．本研究の動機は，運転時の高齢者の注視傾向を推定\\n'\n",
      "                    'することによって，その認知・判断をサポートするこ\\n'\n",
      "                    'とのできる画像センシング技術を実現することにある．\\n'\n",
      "                    '仮に，あるシーン（環境）を運転中の高齢者が目を向\\n'\n",
      "                    'けやすい領域 (Focus of Attention: FoA) が推定でき\\n'\n",
      "                    'たとすると，同シーンにおいて安全の観点から本来注\\n'\n",
      "                    '意すべき領域，例えば飛び出しが起こりやすい物陰や\\n'\n",
      "                    '死角等のある箇所などとの差異から，高齢者が見落と\\n'\n",
      "                    'しがちな危険領域を事前に検知することが可能となり，\\n'\n",
      "                    '注意を促すことができるようになるであろう．既存研究に着目すると，これまでにも数多くの注視\\n'\n",
      "                    '領域推定手法が提案されてきている．特に最近では推\\n'\n",
      "                    '定モデルとして深層ニューラルネットを用いる手法の研\\n'\n",
      "                    '究が盛んであり，例えば，[3] では Convolutional Long-\\n'\n",
      "                    'Short Term Memory による手法が，また [4] では 3 次\\n'\n",
      "                    '元畳み込みニューラルネットワークを用いた手法が提\\n'\n",
      "                    '案されている．しかしながら，従来の多くの手法は，主\\n'\n",
      "                    'として非高齢者の成人一般の注視を対象としており，そ\\n'\n",
      "                    'のモデルは必ずしも高齢者の注視領域を正しく推定す\\n'\n",
      "                    'ることができない（以降，本稿では呼称を簡略化する\\n'\n",
      "                    '目的で，誤解の生じない限り「非高齢者の成人」を指Copyright © SSII 2019. All Rights '\n",
      "                    'Reserved.- IS1-25 -z 東京大学して単に「成人」と呼ぶ）．先に述べた通り，成人と高\\n'\n",
      "                    '齢者では視覚や認知・判断の能力に隔たりがあり，結果\\n'\n",
      "                    'としてその注視領域は一致しないことが多い．一つの\\n'\n",
      "                    'アプローチは，深層ニューラルネットを高齢者の注視\\n'\n",
      "                    'データを用いて学習することであろう．しかしながら，\\n'\n",
      "                    '深層ニューラルネットの学習には通常十分な規模の学\\n'\n",
      "                    '習用データが必要であり，これに足るだけの大量の視\\n'\n",
      "                    '線データの収録を高齢者に強いることは，高齢者の身\\n'\n",
      "                    '体能力・健康面も鑑みると必ずしも容易なことではな\\n'\n",
      "                    'い．したがって，よりデータ効率のいい学習手法が求\\n'\n",
      "                    'められると考える．本稿では，新しいアプローチによりこの問題の解決\\n'\n",
      "                    'を図る．成人と高齢者では，あるシーンを観測した下\\n'\n",
      "                    'での注視領域そのものに違いはあるものの，これらの\\n'\n",
      "                    '関係性については一定の統計的傾向を有していると考\\n'\n",
      "                    'えられる．この仮説に基づいて，成人の注視マップを知\\n'\n",
      "                    '識として活用し，これを高齢者のものに変換する画像\\n'\n",
      "                    '変換に基づくアプローチを提案する．提案法は，シー\\n'\n",
      "                    'ンが与えられた下で，まず成人の注視マップを推定し，\\n'\n",
      "                    '続いて encoder-decoder 型の畳み込みニューラルネット\\n'\n",
      "                    'によりこれを高齢者のものに変換する．この変換モデ\\n'\n",
      "                    'ルは，推定した注視マップが正解の注視マップに近づ\\n'\n",
      "                    'くことを要請する通常の教師有り型の回帰学習に加え，\\n'\n",
      "                    'シーン所与の下での成人と高齢者それぞれの注視マッ\\n'\n",
      "                    'プの条件付同時確率を学習する敵対的学習に基づいて\\n'\n",
      "                    '学習する．ドライビングシミュレータを用いて収録し\\n'\n",
      "                    'た高齢者の注視データにより評価を行った結果，提案\\n'\n",
      "                    '法は少量の学習データからでも高い精度で高齢者の注\\n'\n",
      "                    '視マップを推定できることを確認した．2 手法運転者視点から見えるシーンを写した映像フレーム\\n'\n",
      "                    '(すなわち，すなわちドライバーズビューの RGB 画像)\\n'\n",
      "                    '系列が与えられたとする．我々の問題は，各フレームの\\n'\n",
      "                    '各画素に対する高齢者の注視のしやすさを確率として\\n'\n",
      "                    '推定した注視マップを求めることである．以降，n 番目\\n'\n",
      "                    'の映像フレームを fn と表し，当該フレームに対応する\\n'\n",
      "                    '成人の注視マップを an，高齢者の注視マップを en とそ\\n'\n",
      "                    'れぞれ表記する．連続する k 枚の映像フレームの系列をIS1-25\\n'\n",
      "                    'SO1-25第25回画像センシングシンポジウム図 1 提案法の概要図．Fn = ffn(cid:0)k+1; '\n",
      "                    'fn(cid:0)k+2:::; fng と表す．本稿では k = 16\\n'\n",
      "                    'と固定する．図 1 に提案法の概要図を示す．Fn が与えられた下，\\n'\n",
      "                    '提案法は大きく 2 段階の処理を経て en を推定する．ま\\n'\n",
      "                    'ず，Fn を予測ネットワーク (predictor network) に入力\\n'\n",
      "                    'し，成人の注視マップ an を推定する．予測ネットワー\\n'\n",
      "                    'クの構成はどのようなものでも構わないが，本稿では成\\n'\n",
      "                    '人の運転時注視モデルを予測するために提案された [4]\\n'\n",
      "                    'の手法を用いる．続いて，変換ネットワーク (translator\\n'\n",
      "                    'network) を用いて，an を高齢者の注視マップ en へと\\n'\n",
      "                    '変換する．もう一つ，学習時に限り，敵対的学習の枠組\\n'\n",
      "                    'みに基づいて変換ネットワークの学習を促進するため，\\n'\n",
      "                    '識別ネットワーク (discriminator network) を導入する．\\n'\n",
      "                    '識別ネットワークは，変換ネットワークが推定した高\\n'\n",
      "                    '齢者の注視マップ en と，正解の注視マップ e\\n'\n",
      "                    'を見分\\n'\n",
      "                    'けるよう学習され，反対に変換ネットワークは識別不\\n'\n",
      "                    '可能な注視マップへと変換できるよう学習する．以降，\\n'\n",
      "                    '変換ネットワーク及び識別ネットワークの詳細，およ\\n'\n",
      "                    'び，学習方法について順に説明する．\\n'\n",
      "                    '2.1 ネットワークの構成(cid:3)n変換ネットワーク，識別ネットワーク共に CNN '\n",
      "                    'として構成する．具体的な構成は下記の通りである．変換ネットワーク．変換ネットワークは '\n",
      "                    'encoder-decoder\\n'\n",
      "                    '型のアーキテクチャを持つ．Encoder 部分は 8 層の 4(cid:2)4\\n'\n",
      "                    '畳み込み層により構成し，各層には活性化関数として\\n'\n",
      "                    'ReLU を用いる．各層のチャネル数は，最初の 3 層が順に\\n'\n",
      "                    '64，128，256，4 層目以降は 512 で固定する．Decoder 部\\n'\n",
      "                    '分は encoder 部分と対称的な構造を持つように設計する．\\n'\n",
      "                    '具体的には，8 層の 4(cid:2) 4 逆畳み込み層 (upconvolution\\n'\n",
      "                    'layer) により構成し，チャネル数は順に 512，256，128，\\n'\n",
      "                    '以降 7 層目までは 64，最終層のみ注視マップのチャネ\\n'\n",
      "                    'ル数と同じ 1 としている．さらに，推定される注視マッ\\n'\n",
      "                    'プの画質を改善する目的で encoder と decoder の空間\\n'\n",
      "                    '解像度が等しい層の間は各々バイパスを持つように設Copyright © SSII 2019. All Rights '\n",
      "                    'Reserved.- IS1-25 -計している (skip '\n",
      "                    'connection)．識別ネットワーク．識別ネットワークへの入力は，成人\\n'\n",
      "                    'の注視マップ an と，変換ネットワークが推定した高齢\\n'\n",
      "                    '者の注視マップ en あるいは正解の注視マップ e\\n'\n",
      "                    'のペア\\n'\n",
      "                    'であり，これを基に，入力された高齢者の注視マップが\\n'\n",
      "                    '推定されたものであるか，正解のマップであるかを識別\\n'\n",
      "                    'する．識別ネットワークは 5 層の畳み込み層により構\\n'\n",
      "                    '成する．畳み込み層のチャネル数は順に 64，128，256，\\n'\n",
      "                    '512，1 であり，活性化関数は最初の 4 層には ReLU，最\\n'\n",
      "                    '終層にはシグモイド関数を用いる．[5] と同様，識別は\\n'\n",
      "                    '領域（パッチ）単位で出力され，然るに学習時には損\\n'\n",
      "                    '失も領域単位で評価される．\\n'\n",
      "                    '2.2 学習(cid:3)n簡単のため，変換ネットワークを T ，識別ネットワー\\n'\n",
      "                    'クを D で表す．提案法では，下記の問題を T 及び D の\\n'\n",
      "                    'パラメータについて最適化することにより学習を行う．minmaxLAdv(T; D) + (cid:21)LKL(T '\n",
      "                    '):(1)TDこの目的関数は大まかに推定損失 LKL と敵対的損失\\n'\n",
      "                    'LAdv の二つの損失関数の重み付け和として構成されて\\n'\n",
      "                    'いる．変換ネットワーク T はこれを最小化するように，\\n'\n",
      "                    '識別ネットワーク D は最大化するように学習する．以\\n'\n",
      "                    '降，両損失関数について説明する．推定損失．推定損失は，変換ネットワークにより推定さ\\n'\n",
      "                    'れた高齢者の注視マップ en = T (an) と，真の注視マッ\\n'\n",
      "                    'との差異を評価する損失関数である．en，並びに\\n'\n",
      "                    'プ e\\n'\n",
      "                    'の i 番めの画素値（推定確率）をそれぞれ en;i 及び\\n'\n",
      "                    'e\\n'\n",
      "                    ';i は双方確率値であることを鑑み，\\n'\n",
      "                    ';i と表す．en;i，e\\n'\n",
      "                    'e\\n'\n",
      "                    'カルバック・ライブラー情報量 (KL divergence) を用\\n'\n",
      "                    'い以下のように定義する．(cid:3)n(cid:3)n(cid:3)n(cid:3)n∑ i\\n'\n",
      "                    '∑ n;i) (cid:0) log(en;i))(cid:3) n(cid:3) n;i(log(eeLKL(T '\n",
      "                    ') =(2)敵対的損失．1 章で議論したように，我々のアプローチ\\n'\n",
      "                    'は，シーン所与の下で成人と高齢者の注視マップの差異\\n'\n",
      "                    'に一定の傾向があるという知見に動機づけられている．\\n'\n",
      "                    'この仮定に基づけば，変換ネットワークは，（条件付）同\\n'\n",
      "                    '時確率分布 p(an; e\\n'\n",
      "                    '; Fn) を再現するよう学習されるこ\\n'\n",
      "                    'とが好ましい．一方，先に述べた推定損失は，単純に\\n'\n",
      "                    '; Fn) を得ようとしているにすぎ\\n'\n",
      "                    'e\\n'\n",
      "                    '; Fn) の再現を要請するよ\\n'\n",
      "                    'ず，然るに同時確率 p(an; e\\n'\n",
      "                    'うなものではない．そこで我々はさらに，p(an; e\\n'\n",
      "                    '; Fn)\\n'\n",
      "                    'を陽に推定することを要請する損失関数として，下記\\n'\n",
      "                    'の敵対的損失 [6] を導入する．(cid:3)n(cid:3)n(cid:3)nの条件付確率 '\n",
      "                    'p(e(cid:3)n(cid:3)nIS1-25\\n'\n",
      "                    'SO1-25第25回画像センシングシンポジウム図 2 （上段）ドライビングシミュレータを用い\\n'\n",
      "                    'た視線計測の様子．運転者の視線は運転席前方に\\n'\n",
      "                    '設置された 3 つの視線計測機により計測される．\\n'\n",
      "                    '（下段）収録データの一例．映像フレームに注視\\n'\n",
      "                    'マップを重畳して提示している．(cid:3) nLAdv(T; D) = '\n",
      "                    'E(an;e(cid:3)n)(cid:24)p(an;e(cid:3))]n;Fn)[log D(an; '\n",
      "                    'e+Ean(cid:24)p(an;Fn);en(cid:24)p(enjan)[1 (cid:0) log '\n",
      "                    'D(an; en)] (3)\\n'\n",
      "                    '(cid:3) の下，本損失関数を変換\\n'\n",
      "                    '理想的な識別ネットワーク D\\n'\n",
      "                    'ネットワーク T について最小化することにより，我々の\\n'\n",
      "                    'モデル p(an; en; Fn) = p(an; Fn)p(enjan) は，イェンセ\\n'\n",
      "                    'ン・シャノン情報量 (JS divergence) の意味で真の同時\\n'\n",
      "                    '; Fn) に一致するよう学習される．実\\n'\n",
      "                    '確率分布 p(an; e\\n'\n",
      "                    '(cid:3) を得るこ\\n'\n",
      "                    '際には，事前に理想的な識別ネットワーク D\\n'\n",
      "                    'とはできないため，T と D を交互最適化するように学\\n'\n",
      "                    '習を進める．(cid:3)n3 実験提案法の有効性を実験により評価する．運転時の注\\n'\n",
      "                    '視マップデータを収録し，そのデータに基づき注視マッ\\n'\n",
      "                    'プ推定精度を評価する．\\n'\n",
      "                    '3.1 データ収録・作成注視マップ推定手法を評価するためのデータセットは\\n'\n",
      "                    'これまでにもいくつか作成・公開されてきている [7, 8, 9]．\\n'\n",
      "                    'しかしながら，その多くは 18 歳〜40 歳位の成人を対象\\n'\n",
      "                    'に収録したデータであり，高齢者の視線データを収録\\n'\n",
      "                    'したものはほとんどなく，まして本研究で対象とする\\n'\n",
      "                    '運転時の注視を扱ったデータは我々が知る限り存在し\\n'\n",
      "                    'ない．そこで，我々はドライビングシミュレータを用\\n'\n",
      "                    'い，独自にデータの収録と作成を行った．以下，収録\\n'\n",
      "                    '条件とデータ作成手順を簡単に説明する．収録条件．成人 9 名，高齢者 9 名の計 18 名の参加者を\\n'\n",
      "                    '募った．参加者はいずれも正常な視覚機能を有し，日\\n'\n",
      "                    '本の運転免許証又は国際ドライバーライセンスを 5 年\\n'\n",
      "                    '以上保有している．各年齢グループの平均年齢はそれCopyright © SSII 2019. All Rights '\n",
      "                    'Reserved.- IS1-25 -ぞれ 26 歳と 75 歳であった．各参加者には，ドライビ\\n'\n",
      "                    'ングシミュレータを操作し，提示された目的地まで一\\n'\n",
      "                    '般的な規則を順守し安全に運転するよう指示を与えた．\\n'\n",
      "                    '収録の様子を図 2 の上段に示す．シミュレータを通じ\\n'\n",
      "                    'て運転者に提示するシーンは，計 10; 000 フレームの映\\n'\n",
      "                    '像として構成した．各フレームは道路や建設物，信号，\\n'\n",
      "                    '道路標識の他，車や歩行者等の物体を含んでいる．運\\n'\n",
      "                    '転中の参加者の視線は，Smart-Eye という視線計測装\\n'\n",
      "                    '置を用い，映像フレームと同期して実時間収録した．データ作成．収録した視線データを基に，成人，高齢者\\n'\n",
      "                    'それぞれの注視マップを得る．このためには，各年齢グ\\n'\n",
      "                    'ループに属する参加者の，同一の映像フレームに対す\\n'\n",
      "                    'る注視点の統計量を求める必要がある．全ての参加者\\n'\n",
      "                    'には同じ映像フレームを提示し，同じ仮想道路を運転\\n'\n",
      "                    'させたが，参加者ごとに運転のスピードが異なるため，\\n'\n",
      "                    '映像フレームは参加者間で時刻的にずれたものとなって\\n'\n",
      "                    'いる．このずれを補正するため，18 名の内無作為に選\\n'\n",
      "                    'んだ 1 名のデータを参照シーケンスとし，これに残りの\\n'\n",
      "                    '17 名の映像フレームを dynamic time warping (DTW)\\n'\n",
      "                    'による系列マッチングを施して時間同期を行った．な\\n'\n",
      "                    'お，DTW の類似度は映像フレームに対して SIFT キー\\n'\n",
      "                    'ポイントのマッチング及び幾何検証を経て得られた対\\n'\n",
      "                    '応点数により定めた．最後に，映像フレームに対応す\\n'\n",
      "                    'る時刻の各参加者の視線データを映像フレームに投影\\n'\n",
      "                    'して注視点群を求めた後，注視推定の一般的なプロト\\n'\n",
      "                    'コル [9] に従いガウスフィルタを畳み込むことにより注\\n'\n",
      "                    '視マップを得た．以上の手続きにより，計 9; 652 の映\\n'\n",
      "                    '像フレームと，対応する成人および高齢者の注視マッ\\n'\n",
      "                    'プを得た．このうち，7; 722 フレーム分を学習用に，残\\n'\n",
      "                    'りを評価用に用いる．\\n'\n",
      "                    '3.2 評価条件得られた学習データを用い，変換ネットワークと識\\n'\n",
      "                    '別ネットワークをスクラッチ学習した．学習には Adam\\n'\n",
      "                    'を用い，学習率 0:0002，モーメンタム係数 0:5 の条件\\n'\n",
      "                    'の下，100 エポック分行った．損失関数の重みである (cid:21)\\n'\n",
      "                    'は 100 とした.評価は提案法の他，比較のため [10, 11, 12, 4] の 4 種\\n'\n",
      "                    '類の既存手法も評価する．この中で，我々が予測ネット\\n'\n",
      "                    'ワークとして用いた [4] については，本研究にて生成し\\n'\n",
      "                    'た高齢者の注視マップデータを用いて (cid:12)ne-tuning した\\n'\n",
      "                    '場合の性能も評価する．\\n'\n",
      "                    '3.3 '\n",
      "                    '定量評価結果まず定量評価結果について述べる．評価指標として，注視マップの推定精度として用いられることの多い次の\\n'\n",
      "                    '3 つの指標を用いる [13]。(1) ピアソン相関係数 (CC)：\\n'\n",
      "                    '正解及び推定した注視マップをベクトルと見做した場合\\n'\n",
      "                    'の相関係数．高ければ高いほど正確に推定できているこ\\n'\n",
      "                    'とを表す．(2) ヒストグラムインターセクション (SIM)：IS1-25\\n'\n",
      "                    'SO1-25第25回画像センシングシンポジウム図 3 '\n",
      "                    '定性評価．各手法により推定された注視マップの例．左列より順に入力映像フレーム（シーン），正\\n'\n",
      "                    '解となる高齢者の注視マップ，提案法による推定結果，[4] による推定結果, [10] による推定結果．表 1 '\n",
      "                    '各手法の精度評価結果．各評価尺度横の矢\\n'\n",
      "                    '印は向きにより \"：高いほど良い尺度，#：低いほ\\n'\n",
      "                    'ど良い尺度を表している．\\n'\n",
      "                    'CC\"\\n'\n",
      "                    '0.1305SIM\"\\n'\n",
      "                    '0.2232KL#\\n'\n",
      "                    '5.60Algorithm[10][11]\\n'\n",
      "                    '[12]0.0901\\n'\n",
      "                    '0.14780.2595\\n'\n",
      "                    '0.29404.90-[4]\\n'\n",
      "                    '[4] ((cid:12)ne-tuned)\\n'\n",
      "                    'Ours0.6386\\n'\n",
      "                    '0.6575\\n'\n",
      "                    '0.77170.5324\\n'\n",
      "                    '0.5535\\n'\n",
      "                    '0.68324.06-2.182 つの注視マップをヒストグラムと見做した場合の一致\\n'\n",
      "                    '度合．高ければ高いほど正確に推定できていることを\\n'\n",
      "                    '表す．(3) カルバック・ライブラー情報量 (KL)：2 つの\\n'\n",
      "                    '注視マップの確率分布間偽距離．結果を表 1 に示す．まず，いずれに指標においても\\n'\n",
      "                    '提案法が最も高い精度を達成していることがわかる．\\n'\n",
      "                    '[10, 11, 12] の 3 つの方法の精度は非常に低い．これら\\n'\n",
      "                    'の方法はボトムアップな顕著性モデルに基づくもので\\n'\n",
      "                    'あり，運転時のような高次情報処理が介在するコンテ\\n'\n",
      "                    'キストにおける注視傾向の推定には不向きであること\\n'\n",
      "                    'が理由であると考えられる．提案法や [4] は運転時の注\\n'\n",
      "                    '視傾向を学習・モデル化しているため，本稿の評価シナ\\n'\n",
      "                    'リオにおいてはこれらのモデルよりも高い推定精度を\\n'\n",
      "                    '達成することができている．[4] に対する提案法の優位\\n'\n",
      "                    '性の要因の一つは，[4] が成人の注視マップをモデル化\\n'\n",
      "                    'しているのに対し，提案法では高齢者の注視マップを陽\\n'\n",
      "                    'に学習していることである．一方，[4] を高齢者の注視\\n'\n",
      "                    'マップデータを用いて (cid:12)ne-tuning した場合であっても，\\n'\n",
      "                    '僅かに提案法の方が高い精度を達成している．[4] は比\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    '較的複雑な深層ニューラルネットの組み合わせで構成\\n'\n",
      "                    'されており，約 55 万フレームの成人注視マップデータ\\n'\n",
      "                    'を用いて学習されている．対して本稿の学習データは 1\\n'\n",
      "                    '万を下回る規模であり，(cid:12)ne-tuning する場合であって\\n'\n",
      "                    'も十分に高い汎化性能を獲得しにくかったことが原因Copyright © SSII 2019. All Rights '\n",
      "                    'Reserved.- IS1-25 -として考えられる．上記の結果より，提案法の有用性\\n'\n",
      "                    'が確認できたと言える．\\n'\n",
      "                    '3.4 定性評価結果続いて，定性的な評価結果として，図 3 に提案法，[4]，\\n'\n",
      "                    '及び [10] の推定結果例を示す．提案法の推定結果が正\\n'\n",
      "                    '解の注視マップに近いことが確認できる．最初の例（1\\n'\n",
      "                    '行目）は，右折動作中のシーンであり，他の手法は広\\n'\n",
      "                    'く注視領域を推定する傾向にある．一方，高齢者の注\\n'\n",
      "                    '視は交差点を渡ろうとする歩行者付近に集中しており，\\n'\n",
      "                    '提案法はこの傾向を反映した推定結果となっている．ま\\n'\n",
      "                    'た，二番目，三番目の例では，左折しようとする前方\\n'\n",
      "                    'の車に注意が集中する傾向を反映した推定結果となっ\\n'\n",
      "                    'ている．[4] も比較的同様の推定結果を出しているもの\\n'\n",
      "                    'の，成人の傾向である注視領域が広がりやすい傾向を\\n'\n",
      "                    '反映したものになっており，提案法の方が高齢者の傾\\n'\n",
      "                    '向との整合性は高い．以上の通り，提案法の有用性は\\n'\n",
      "                    '定性的にもうかがえる結果を確認できた．4 おわりに本稿では運転時の高齢者の注視マップ推定の問題を\\n'\n",
      "                    '議論し，深層画像変換に基づく新しい推定手法を提案\\n'\n",
      "                    'した．提案法は，成人の注視マップの推定結果を基に，\\n'\n",
      "                    'これを画像変換によって高齢者のものへと変換する．ド\\n'\n",
      "                    'ライビングシミュレータを用いて収録した高齢者注視\\n'\n",
      "                    'マップデータを基に評価を行った結果，提案法は従来\\n'\n",
      "                    '法よりも高い推定精度が得られることが確認できた．\\n'\n",
      "                    '今後の方向性としては，実際のドライビングデータ\\n'\n",
      "                    'での評価検証や，運転支援システムへの適用・評価が\\n'\n",
      "                    '挙げられる．運転時以外のコンテキストにおける手法\\n'\n",
      "                    'の有効性検証も興味深い展開の一つであろう．謝辞．ドライビングシミュレータを用いた注視データ\\n'\n",
      "                    '収録に関してご支援頂いた東京大学 中野 公彦教授，並\\n'\n",
      "                    'びに，同研究室の皆様に感謝いたします．IS1-25\\n'\n",
      "                    'SO1-25第25回画像センシングシンポジウム参考文献[1] Stefan Dowiasch,Svenja '\n",
      "                    'Marx, Wolfgang\\n'\n",
      "                    '\\\\Eﬀects ofEinh(cid:127)auser, and Frank Bremmer,aging on '\n",
      "                    'eye movements in the real world,\"\\n'\n",
      "                    'Frontiers in human neuroscience, vol. 9, pp. 46,\\n'\n",
      "                    '2015.[2] Alper A(cid:24)c(cid:16)k, Adjmal Sarwary, '\n",
      "                    'Rafael Schultze-\\n'\n",
      "                    '\\\\De-Kraft, Selim Onat, and Peter '\n",
      "                    'K(cid:127)onig,velopmental changes in natural viewing '\n",
      "                    'behav-\\n'\n",
      "                    'ior: bottom-up and top-down diﬀerences between\\n'\n",
      "                    'children, young adults and older adults,\" Fron-tiers in '\n",
      "                    'psychology, vol. 1, pp. 207, 2010.[3] Marcella Cornia, '\n",
      "                    'Lorenzo Baraldi, GiuseppeSerra, and Rita Cucchiara, '\n",
      "                    '\\\\Predicting Human\\n'\n",
      "                    'Eye Fixations via an LSTM-based Saliency At-\\n'\n",
      "                    'IEEE Transactions on Image\\n'\n",
      "                    'tentive Model,\"Processing, vol. 27, no. 10, pp. '\n",
      "                    '5142{5154, 2018.\\n'\n",
      "                    '[4] Andrea Palazzi, Davide Abati, Simone '\n",
      "                    'Calderara,Francesco Solera, and Rita Cucchiara, '\n",
      "                    '\\\\Predict-\\n'\n",
      "                    'ing the driver’s focus of attention: the dr(eye)ve\\n'\n",
      "                    'project,\" IEEE transactions on pattern analysisand '\n",
      "                    'machine intelligence, 2018.[5] Chuan Li and Michael Wand, '\n",
      "                    '\\\\Precomputed real-time texture synthesis with markovian '\n",
      "                    'generative\\n'\n",
      "                    'adversarial networks,\" in European Conference\\n'\n",
      "                    'on Computer Vision. Springer, 2016, pp. 702{716.[6] Ian '\n",
      "                    'Goodfellow, Jean Pouget-Abadie, MehdiMirza, Bing Xu, '\n",
      "                    'David Warde-Farley, Sher-\\n'\n",
      "                    'jil Ozair, Aaron Courville, and Yoshua Ben-\\n'\n",
      "                    'gio,\\n'\n",
      "                    'in Ad-\\\\Generative adversarial nets,\"vances in Neural '\n",
      "                    'Information Processing Systems\\n'\n",
      "                    '(NeurIPS), 2014.[7] Shaojing Fan, Zhiqi Shen, Ming Jiang, '\n",
      "                    'Bryan LKoenig, Juan Xu, Mohan S Kankanhalli, and\\n'\n",
      "                    'Qi Zhao, \\\\Emotional attention: A study of im-age '\n",
      "                    'sentiment and visual attention,\"\\n'\n",
      "                    'in 2018\\n'\n",
      "                    'IEEE/CVF Conference on Computer Vision and\\n'\n",
      "                    'Pattern Recognition. IEEE, 2018, pp. 7521{7531.[8] Ali '\n",
      "                    'Borji and Laurent Itti,\\\\Cat2000: A\\n'\n",
      "                    'large scale (cid:12)xation dataset for boosting '\n",
      "                    'saliencyresearch,\"\\n'\n",
      "                    'ture of Datasets\",\\n'\n",
      "                    'arXiv:1505.03581.CVPR 2015 workshop on \"Fu-\\n'\n",
      "                    'arXiv preprint2015,[9] Zoya Bylinskii, Tilke Judd, Ali '\n",
      "                    'Borji, Laurent\\n'\n",
      "                    'Itti, Fr(cid:19)edo Durand, Aude Oliva, and '\n",
      "                    'AntonioTorralba,\\n'\n",
      "                    'http://saliency.mit.edu/datasets.html.\\\\Mit saliency '\n",
      "                    'benchmark dataset,\"Copyright © SSII 2019. All Rights '\n",
      "                    'Reserved.- IS1-25 -[10] Wenguan Wang, Jianbing Shen, and '\n",
      "                    'Fatih\\n'\n",
      "                    '\\\\Saliency-aware geodesic video '\n",
      "                    'objectPorikli,segmentation,\" in Proceedings of the IEEE '\n",
      "                    'con-\\n'\n",
      "                    'ference on computer vision and pattern recogni-tion, '\n",
      "                    '2015, pp. 3395{3402.[11] Wenguan Wang, Jianbing Shen, and '\n",
      "                    'Ling Shao,\\n'\n",
      "                    '\\\\Consistent video saliency using local '\n",
      "                    'gradient(cid:13)ow optimization and global '\n",
      "                    're(cid:12)nement,\" IEEE\\n'\n",
      "                    'Transactions on Image Processing, vol. 24, no. 11,pp. '\n",
      "                    '4185{4196, 2015.[12] Marcella Cornia, Lorenzo Baraldi, '\n",
      "                    'Giuseppe\\n'\n",
      "                    '\\\\A Deep Multi-Serra, and Rita Cucchiara,Level Network '\n",
      "                    'for Saliency Prediction,\" in In-\\n'\n",
      "                    'ternational Conference on Pattern Recognition(ICPR), '\n",
      "                    '2016.[13] Zoya Bylinskii, Tilke Judd, Aude Oliva, '\n",
      "                    'Antonio\\n'\n",
      "                    'Torralba, and Fr(cid:19)edo Durand, \\\\What do diﬀer-ent '\n",
      "                    'evaluation metrics tell us about saliency mod-\\n'\n",
      "                    'els?,\" arXiv preprint arXiv:1604.03605, 2016.',\n",
      "              '結論': '本稿では運転時の高齢者の注視マップ推定の問題を\\n'\n",
      "                    '議論し，深層画像変換に基づく新しい推定手法を提案\\n'\n",
      "                    'した．提案法は，成人の注視マップの推定結果を基に，\\n'\n",
      "                    'これを画像変換によって高齢者のものへと変換する．ド\\n'\n",
      "                    'ライビングシミュレータを用いて収録した高齢者注視\\n'\n",
      "                    'マップデータを基に評価を行った結果，提案法は従来\\n'\n",
      "                    '法よりも高い推定精度が得られることが確認できた．\\n'\n",
      "                    '今後の方向性としては，実際のドライビングデータ\\n'\n",
      "                    'での評価検証や，運転支援システムへの適用・評価が\\n'\n",
      "                    '挙げられる．運転時以外のコンテキストにおける手法\\n'\n",
      "                    'の有効性検証も興味深い展開の一つであろう．'},\n",
      "  'date': '2000-01-01 00:00:00',\n",
      "  'paper_title': '画像変換に基づく高齢者の運転時注視マップの推定',\n",
      "  'pdf_name': 'IS1-25'},\n",
      " {'_id': ObjectId('5eb6bcab60e378f730f5b631'),\n",
      "  'conf_name': 'SSII2019',\n",
      "  'content': {'序論': '水産養殖の生産性を向上させるためには，養殖魚の\\n'\n",
      "                    '状況を観察し，適切な養殖環境になるように調整する\\n'\n",
      "                    '必要がある．養殖魚の成長状況を把握する上での基本\\n'\n",
      "                    'となるのは尾数の把握である．特に，クロマグロは外\\n'\n",
      "                    '部からの刺激に敏感であるため，その養殖において魚\\n'\n",
      "                    'に刺激を与えない尾数計測が求められている．しかし，\\n'\n",
      "                    '多数の魚が洋上の生簀の中で遊泳している場合に，そ\\n'\n",
      "                    'の尾数を非侵襲・非接触で把握することは困難である．\\n'\n",
      "                    'そこで，ダイバーが生簀の中から撮影した水中映像か\\n'\n",
      "                    'ら遊泳しているクロマグロ成魚に相当する領域（以下，\\n'\n",
      "                    '成魚領域）とそれ以外の領域分割を行い，ラベリング\\n'\n",
      "                    '処理を行い連結領域に何匹いるかを畳み込みニューラ\\n'\n",
      "                    'ルネットワーク（CNN）を用いて尾数推定を行う手法\\n'\n",
      "                    'を提案する．取得した映像では，91%という高い精度を\\n'\n",
      "                    '示した．1 背景・目的養殖場や水族館において，生育状況や飼育環境の把\\n'\n",
      "                    '握のために尾数計測が頻繁に行われている．現在，奄\\n'\n",
      "                    '美実験場にあるクロマグロの養殖場では，手作業で尾\\n'\n",
      "                    '数計測が行われており，人手を要することや，魚への\\n'\n",
      "                    '外傷により死滅に繋がる危険性がある．これらの問題\\n'\n",
      "                    'を解決するために，生簀をカメラで撮影し，その映像\\n'\n",
      "                    'からクロマグロを検知し尾数を自動で計測することが\\n'\n",
      "                    'できれば，問題を減らすことに繋がる情報が得られる\\n'\n",
      "                    'と期待される．2 関連手法と問題点高木らは，個体数計数システムの開発の研究として，\\n'\n",
      "                    'PTV 解析によって魚の動きを捉えて尾数の推定を行なっ\\n'\n",
      "                    'ているが [1]，水中映像の太陽の光や影，波といった外\\n'\n",
      "                    '的要因の影響を強く受けることが問題となっている．そ\\n'\n",
      "                    'こで，生簀内の尾数を把握するには，外的要因の影響\\n'\n",
      "                    'を受けずに尾数を推定できる手法が必要になる．また，\\n'\n",
      "                    'クロマグロの稚魚を検出する研究が行われている [2][3]．\\n'\n",
      "                    'この研究は画像上で数画素の大きさしかない小さい対Copyright © SSII 2019. All Rights '\n",
      "                    'Reserved.- IS1-26 -象を検出することを目的としているが，本研究では，大\\n'\n",
      "                    'きなマグロ成魚を扱うため，これらとは異なり領域分\\n'\n",
      "                    '割を行うことにした．',\n",
      "              '結論': '本研究では，遊泳しているマグロ成魚の水中映像か\\n'\n",
      "                    'らマグロ成魚の領域抽出をする手法を提案した．4.3 節\\n'\n",
      "                    'より，適合率 92.16%, 再現率 89.96%と概ね良好な結果\\n'\n",
      "                    'を得ることができた．4.4 節より，深層学習は用いる構\\n'\n",
      "                    '造，学習回数，データ数の変動で結果を変わるため精\\n'\n",
      "                    '度よく領域抽出を行うためにはデータセットの作成や\\n'\n",
      "                    '深層学習の構造などを設定する必要がある．尾数推定\\n'\n",
      "                    'も概ね概ねできているが，5 匹以上の学習データを作成\\n'\n",
      "                    'するのは現在の作成方法では困難である．5 匹以上の魚\\n'\n",
      "                    'が連結領域内に存在する場合に対して，時系列情報を\\n'\n",
      "                    '用いることで対応できる考えられる．今後の課題として，時系列情報の利用が考えられる．\\n'\n",
      "                    '時系列情報を用いることで CNN 単体の性能より尾数\\n'\n",
      "                    '推定の精度向上も期待される．また，擬似モデルを作\\n'\n",
      "                    '成できなかった 5 匹以上の推定も可能になると考えら\\n'\n",
      "                    'れる．'},\n",
      "  'date': '2000-01-01 00:00:00',\n",
      "  'paper_title': '深層学習による水中映像からのマグロ成魚の尾数推定',\n",
      "  'pdf_name': 'IS1-26'},\n",
      " {'_id': ObjectId('5eb6bd1360e378f730f5b632'),\n",
      "  'conf_name': 'SSII2019',\n",
      "  'content': {'序論': '近年，少子高齢化による人手不足と人手による誤差の軽減を目的とした，外観検査の自動化が進めら\\n'\n",
      "                    'れている．ここで，外観検査の自動化のために 2 種類\\n'\n",
      "                    'の手法がある．1 つ目は，画像処理を用いたアルゴリ\\n'\n",
      "                    'ズムによる外観検査の自動化である[1]．2 つ目は，\\n'\n",
      "                    'CNN による外観検査の自動化である[2][3][4]．しか\\n'\n",
      "                    'し，上記 2 種類の手法にはそれぞれ特有の問題があ\\n'\n",
      "                    'る．画像処理による外観検査の自動化の問題点が 2\\n'\n",
      "                    '点挙げられる．1 点目は，対象物特有の特徴量を設\\n'\n",
      "                    '計しなければならないという問題である．2 点目は，画\\n'\n",
      "                    '像処理による処理コストが大きいという問題である．ま\\n'\n",
      "                    'た，CNN による外観検査の自動化の問題点が 1 点\\n'\n",
      "                    '挙げられる．それは，学習用データとして多量の良品画像と不良品画像が必要になるという問題である．生産ラインにおいて，不良品の発生頻度は低く，不良\\n'\n",
      "                    '品画像の収集が困難である．ここで，外観検査に CNN を利用する場合，学習用\\n'\n",
      "                    'の不良品画像をデータ拡充（Data Augmentation）す\\n'\n",
      "                    'る手法がある．不良品画像をデータ拡充する一般的なアプローチとして，手元にある不良品画像に対し，コントラスト変換，ガンマ変換，ガウシアンノイズ付与，鮮鋭化することによって，新たな不良品画像を生成\\n'\n",
      "                    'する手法がある[5]．しかし，これらのデータ拡充手法\\n'\n",
      "                    'によって生成した画像は，現実に起こりうるかどうかの情報がない．したがって，生成した画像に正しいラベルを付与することができず，データ拡充によって識別率 '\n",
      "                    'が 低 下 す る 可 能 性 が あ る ． ま た ， 近 年 で は ，\\n'\n",
      "                    'Generative Adversarial  Networks  (GAN)によって高\\n'\n",
      "                    '精度な学習用不良品画像を生成する手法が提案さ\\n'\n",
      "                    'れている[6]．しかし，GAN  の学習には多量の学習\\n'\n",
      "                    '用データが必要となるため，不良品画像の収集が困\\n'\n",
      "                    '難である生産ラインにおいて，GAN  を学習させるこ\\n'\n",
      "                    'とは困難である．そこで，2 つの画像をアルファブレン\\n'\n",
      "                    'ディング[9]するデータ拡充手法を用いて，新たに不\\n'\n",
      "                    '良品画像を生成する．そして，新たに生成した不良Copyright © SSII 2019. All Rights '\n",
      "                    'Reserved.- IS1-27 -品画像を用いた CNN の識別率を調査することによっ\\n'\n",
      "                    'て，正しいラベルが付与される α の値を実験的に調\\n'\n",
      "                    '査する．本調査のアルファブレンディングでは，良品画像と不良品画像に対し，内挿をおこない，新たな不良品画像を生成する．そして，アルファブレンディ\\n'\n",
      "                    'ングの α の値を変更し，CNN の識別率が最も向上す\\n'\n",
      "                    'るパラメータを調査する．なお，本稿では，調査に加え，明らかになった実験結果に基づいて新たなデータ拡充手法の提案も\\n'\n",
      "                    'おこなう．本稿では，2 章に従来手法，3 章に最適な α の値\\n'\n",
      "                    'の調査目的と調査方法，4 章に最適な α の値の調査\\n'\n",
      "                    '結果，5 章に新たなデータ拡充手法の提案，6 章に\\n'\n",
      "                    '新たなデータ拡充手法の実験結果と考察，7 章にま\\n'\n",
      "                    'とめと今後の方針を示す．',\n",
      "              '結論': '本調査では，アルファブレンディングによるデータ拡充として，良品画像と不良品画像を合成し，内挿をおこない，新たな不良品画像を生成した．そして，ア\\n'\n",
      "                    'ルファブレンディングの α の値を変更し，CNN の識\\n'\n",
      "                    '別率が最も向上するパラメータを調査した．また，調査に加え，良品画像と不良品画像の画像間距離を考慮したデータ拡充手法を提案した．提案手法によ\\n'\n",
      "                    'って，CNN の識別率が 1.7%向上した．今後は，良品画像と不良品画像の画像間距離に\\n'\n",
      "                    '応じた α のパラメータの決定方法と，特徴空間にお\\n'\n",
      "                    'けるデータ拡充手法に取り組む．表 6  '\n",
      "                    '基画像と生成された不良品画像との画像間距離画像間距離の画像間距離のデータ拡充手法平均分散提案手法1.020.003コントラスト変換3.312.261ガンマ変換5.2326.458ガウシアンノイズ5.3532.941付与鮮鋭化67.201505.264Copyright '\n",
      "                    '© SSII 2019. All Rights Reserved.- IS1-27 -'},\n",
      "  'date': '2000-01-01 00:00:00',\n",
      "  'paper_title': '',\n",
      "  'pdf_name': 'IS1-27'},\n",
      " {'_id': ObjectId('5eb6bd1360e378f730f5b633'),\n",
      "  'conf_name': 'SSII2019',\n",
      "  'content': {'序論': 'コンピュータビジョン技術は，Convolutional  Neural \\n'\n",
      "                    'Network  (CNN)の提案によって大きく発展したと言え\\n'\n",
      "                    'る．CNN の成功には，ImageNet  [1]や Places  [2]のよ\\n'\n",
      "                    'うな大規模かつ注釈が付与されたデータベースが大きく貢献したと言える．しかし，大規模なデータベース\\n'\n",
      "                    'の構築には，膨大な量の画像 1 枚 1 枚に注釈を付与\\n'\n",
      "                    'する必要があり，多大な時間・労力を要する．この問\\n'\n",
      "                    '題に対し，2019 年現在では，クラウドソーシングによ\\n'\n",
      "                    'って協力者を募ることで対処する事例が増加傾向に\\n'\n",
      "                    'あるが，多くの費用がかかる．本稿では，ファッション分野のデータベースを例に，アノテーション作業の軽減について検討する．ファッ\\n'\n",
      "                    'ションに関連するデータベースには，HipsterWars  [3]\\n'\n",
      "                    'や FashionStyle14  [4]のように，人物画像にファッショ\\n'\n",
      "                    'ンスタイル名の注釈が付与されたものが多く提案されている．しかし，ファッションスタイルはファッションを専門に扱うスタイリスト間でも定義が異なることがあり，不特定多数のアノテータによって定義を一貫させ，正確に注釈を付与することは困難である．そのため，少数精鋭のアノテータによって，効率的に注釈を付\\n'\n",
      "                    '与するためのフレームワークが必要であると考える． \\n'\n",
      "                    '本稿の目的は，効率的に注釈付け作業を行うために，識別器による自動注釈と人間による修正作業を\\n'\n",
      "                    '繰り返し行う半自動注釈付け手法の提案と定める(図\\n'\n",
      "                    '1)．注釈付けは，Abe らによって提案された大規模フ\\n'\n",
      "                    'ァ ッ シ ョ ン デ ー タ ベ ー ス で あ る Fashion  Culture \\n'\n",
      "                    'DataBase  (FCDB)[5]に行う．また，CNN の自動注釈\\n'\n",
      "                    '付け部分の有効性について評価し，半自動注釈付\\n'\n",
      "                    'け手法の有効性について考究を加える．以降，次章ではアノテーション作業の効率化に関する従来研究を列挙し，注釈付けを行うデータベー\\n'\n",
      "                    'スである FCDB について概説する．続いて，半自動\\n'\n",
      "                    '注釈付け手法について詳述し，評価実験では提案\\n'\n",
      "                    '手法の CNN の識別精度の推移について検証する．\\n'\n",
      "                    '以上を踏まえたうえ，最終章で本稿を総括する．Copyright © SSII 2019. All Rights '\n",
      "                    'Reserved.- IS1-28 -中村  明生†図 1  提案手法の概略図',\n",
      "              '結論': '本稿では，ファッション分野のデータベースを用いて，アノテーション作業を軽減する半自動注釈付け\\n'\n",
      "                    '手法を提案した．CNN の識別によって自動で注釈を\\n'\n",
      "                    '付与し，人手で誤分類した注釈を修正する．この過程で注釈を付与したデータを用いて再び学習するこCopyright © '\n",
      "                    'SSII 2019. All Rights Reserved.- IS1-28 -図 3  ResNet '\n",
      "                    'における精度推移図 4  VGG，AlexNet '\n",
      "                    'における精度推移とで識別器の精度を向上し，人手による作業量を軽減する．実験では，提案手法のサイクルを繰り返すこ\\n'\n",
      "                    'とで，識別器の精度が向上する推移を確認し，約 5\\n'\n",
      "                    '割の精度で，自動で注釈付けを行うことができた．今後の展望としては，本提案手法によって，人間による注釈付け作業をどれほど軽減できているかを評価す\\n'\n",
      "                    'る．参考文献[1]  Jia  Deng,  et  al.  “Imagenet:  A  '\n",
      "                    'large-scalehierarchical image database,” CVPR 2009.[2]  '\n",
      "                    'Bolei Zhou, et al. “Places: An image '\n",
      "                    'databasefordeepsceneunderstanding,”arXiv:1610.02055, '\n",
      "                    '2016.[3]  M.  Hadi  Kiapour,  et  al.  “Hipster  '\n",
      "                    'Wars:Discovering  Elements  of  Fashion  Styles,”ECCV '\n",
      "                    '2014.[4]  Moeko  Takagi,  et  al.  “What  Makes  a  '\n",
      "                    'Style:IS1-28\\n'\n",
      "                    'SO1-28第25回画像センシングシンポジウムExperimental Analysis of Fashion '\n",
      "                    'Prediction,”ICCVW 2017.[5]  Kaori Abe, et al. “Dynamic '\n",
      "                    'Fashion Cultures”,MIRU 2017.[6]  Lluís  Castrejón,  et  '\n",
      "                    'al.  “Annotating  ObjectInstances with a Polygon-RNN,” '\n",
      "                    'CVPR 2017.[7]  '\n",
      "                    'ZongweiZhou,etal.“Fine-TuningConvolutionalNeuralNetworksforBiomedical  '\n",
      "                    'Image  Analysis:  Actively  andIncrementally,” CVPR '\n",
      "                    '2017.[8]  Bart  Thomee,  et  al.  “YFCC100M:  The  '\n",
      "                    'NewData in Multime-dia Research,” ACM 2016.[9]  Edgar  '\n",
      "                    'Simo-Serra,  et  al.  “Fashion  Styinle  in128 Floats,” '\n",
      "                    'CVPR 2016.[10] Edgar  Simo-Serra,  et  al.  '\n",
      "                    '“Neuroaesthetics  infashion:  '\n",
      "                    'Modelingtheperceptionoffashionability,” CVPR 2015.[11] '\n",
      "                    'Ziwei  Liu,  et  al.  “DeepFashion:  PoweringRobust  '\n",
      "                    'Clothes  Recognition  and  Re-trievalwith Rich '\n",
      "                    'Annotations,” CVPR, 2016.[12] Kaiming He, et al. “Deep '\n",
      "                    'residual learning, forimage recognition,” CVPR, 2016.[13] '\n",
      "                    'Karen  Simonyan,etal.“Very  DeepConvolutional  '\n",
      "                    'Networksfor  Large-ScaleImage Recognition,” ICLR, '\n",
      "                    '2015.[14] Alex  Krizhevsky,etal.“ImageNetClassification '\n",
      "                    'with Deep Convolutional NeuralNetworks,” NIPS, '\n",
      "                    '2012.Copyright © SSII 2019. All Rights Reserved.- IS1-28 '\n",
      "                    '-'},\n",
      "  'date': '2000-01-01 00:00:00',\n",
      "  'paper_title': '',\n",
      "  'pdf_name': 'IS1-28'},\n",
      " {'_id': ObjectId('5eb6bd1360e378f730f5b636'),\n",
      "  'conf_name': 'SSII2019',\n",
      "  'content': {'序論': '出射した光が物体に反射して戻ってくるまでの時\\n'\n",
      "                    '間で対象までの距離を計測する TOF（time  of  flight）\\n'\n",
      "                    'カメラの中でも，LiDAR  (light  detection  and  ranging)\\n'\n",
      "                    'は 200m 程度の遠方まで計測が可能であり，車の自\\n'\n",
      "                    '動 運 転 な ど で の 利 用 が 期 待 さ れ て い る [1][2] ．\\n'\n",
      "                    'LiDAR から出射したレーザーパルスは物体表面で\\n'\n",
      "                    '拡散反射し，強度が弱まって戻ってくる．このパルスの出射から入射までの時間を用いて物体までの距離を算出するが，入射光には，物体表面で反射した太陽などの環境光が混在しており，距離が遠くなってパルスが弱まるほど，パルスの入射時刻の検出精度が低下する．そこで，受信波に含まれる環境光をノイズとして除去し，パルスだけを残すデノイズ処理を検討\\n'\n",
      "                    'した．画像のデノイズにおいては，畳み込みニューラル\\n'\n",
      "                    'ネットワーク(CNN: convolutional neural network)\\n'\n",
      "                    'を用いる深層学習方式により，旧来のハンドクラフト\\n'\n",
      "                    '方式よりも高い復元精度が得られており[3][4][5]，今\\n'\n",
      "                    '回の 1 次元信号のデノイズにおいても CNN を用いた．\\n'\n",
      "                    'その際，入射パルスの到着時間による差異に適応するために，処理対象の受信波に加えて，その時間軸\\n'\n",
      "                    'の情報を参照波として CNN に入力する手法を提案\\n'\n",
      "                    'する．シミュレーションデータを用いた実験において，距離検出の精度が提案手法により向上することを確\\n'\n",
      "                    '認した．図１  実験で用いた畳み込みニューラルネットワークCopyright © SSII 2019. All '\n",
      "                    'Rights Reserved.- IS1-31 -2.  深層学習型デノイズと提案手法2.1  デノイズ用 '\n",
      "                    'CNN図１に今回の深層学習型デノイズに用いた CNN\\n'\n",
      "                    'を示す[6]．畳み込みレイヤと活性化関数を単純に接\\n'\n",
      "                    '続し，学習を促進するために２つの畳み込みレイヤを\\n'\n",
      "                    'またぐショートカットを設けた[7]．ショートカットの部分\\n'\n",
      "                    'を一つずつ res-block とよぶ．',\n",
      "              '結論': ''},\n",
      "  'date': '2000-01-01 00:00:00',\n",
      "  'paper_title': '',\n",
      "  'pdf_name': 'IS1-31'},\n",
      " {'_id': ObjectId('5eb6bd7260e378f730f5b637'),\n",
      "  'conf_name': 'SSII2019',\n",
      "  'content': {'序論': '近年，自動車の自律走行や，拡張現実（Augmented\\n'\n",
      "                    'Reality) が注目を浴びている．これらの基礎となる手\\n'\n",
      "                    '法の一つに Simultaneous Localization and Mapping\\n'\n",
      "                    '(SLAM) という技術が存在する．SLAM とは自己位置推\\n'\n",
      "                    '定と環境マップの復元を同時に実現するものであり，特\\n'\n",
      "                    'に，入力としてカメラで撮影された動画を用いた SLAM\\n'\n",
      "                    'が Visual SLAM である．自己位置推定と環境マップの\\n'\n",
      "                    '復元には，カメラと被写体の距離を表す深度マップと，\\n'\n",
      "                    'カメラ移動量の推定を行う必要があり，特に車載カメ\\n'\n",
      "                    'ラに対し推定を行う需要が高まっている．これを実現する従来手法として，上記 2 つの推定を\\n'\n",
      "                    'Convolutional Neural Network (CNN) を用いて行うも\\n'\n",
      "                    'のがある．そして，カメラ映像を用いた CNN モデルに\\n'\n",
      "                    'よる深度またはカメラ移動量の推定手法には大きく分\\n'\n",
      "                    'けて 2 つのアプローチがある．1 つ目は，特殊なセンサ等を用いて取得したカメラ映\\n'\n",
      "                    '像の各フレームにおける深度データまたはフレーム間\\n'\n",
      "                    'のカメラモーションの真値を利用した教師あり学習モ\\n'\n",
      "                    'デルによるもの [1, 2, 3, 4] である．このアプローチは，\\n'\n",
      "                    '真値を用いるため，高精度な推定を行うことができる\\n'\n",
      "                    'という利点がある．しかし，大規模な深度とカメラモー\\n'\n",
      "                    'ションの真値データを必要とするため，学習のための\\n'\n",
      "                    'データの数が大きく限られるという欠点がある．2 つ目は，カメラ映像のみから深度とカメラ移動量の\\n'\n",
      "                    '推定を行う教師なし学習モデルによるもの [5, 6, 7] で\\n'\n",
      "                    'ある．このアプローチは真値データを特に必要としな\\n'\n",
      "                    'いため，データセットの制限が少なく，近年では，教\\n'\n",
      "                    '師なし学習モデルによる推定手法が注目を浴びている．\\n'\n",
      "                    '特に，SfmLearner という従来手法 [5] の学習モデルは，\\n'\n",
      "                    '多くの教師なし学習による深度とカメラ移動量の推定\\n'\n",
      "                    'のベースとなっている．この従来手法では，複数フレー\\n'\n",
      "                    'ムから CNN を通じて推定された深度マップとカメラ移\\n'\n",
      "                    '動量からそれらの中間フレームを再構築し，これを入\\n'\n",
      "                    '力中間フレームに近づけることで，深度マップとカメ\\n'\n",
      "                    'ラ移動量の推定精度を高めている．このアプローチに\\n'\n",
      "                    'よりこの手法は，教師なし学習モデルでありながら，教\\n'\n",
      "                    '師あり学習モデルによる手法と近い推定精度を実現し\\n'\n",
      "                    'ている．しかし特に推定深度マップに着目すると，遠Copyright © SSII 2019. All Rights '\n",
      "                    'Reserved.- IS1-32 -方や密集した被写体の正確な形状を捉えた深度推定が\\n'\n",
      "                    '難しいという傾向が見られる．これは，従来手法の目\\n'\n",
      "                    '的関数が主に，入力中間フレームと再構築した中間フ\\n'\n",
      "                    'レームとの L1 ロスで構成されているため，被写体の位\\n'\n",
      "                    '置関係や境界等，全体構造の差異に関して，目的関数\\n'\n",
      "                    'が十分に表現できていないという問題に起因すると考\\n'\n",
      "                    'えられる．そこで本研究では，特に深度推定に着目し，\\n'\n",
      "                    'この問題の回避に取り組む．上述の問題を解決するため，本研究では，Generative\\n'\n",
      "                    'Adversarial Network (GAN)[8] における識別機能のア\\n'\n",
      "                    'プローチを従来手法 [5] に導入し，再構築されたフーレ\\n'\n",
      "                    'ムの尤もらしさを判定することで，深度マップの推定\\n'\n",
      "                    '精度を向上させることを考える．具体的には，主に画\\n'\n",
      "                    '像変換に使われる Conditional GAN (cGAN)[9] を応用\\n'\n",
      "                    'し，推定精度向上を目指す．本稿では，再構築された\\n'\n",
      "                    'フレームの復元精度と深度マップの推定精度の関係性\\n'\n",
      "                    'について検討する．2 関連手法カメラ映像から，深度推定及び，カメラ移動量の推\\n'\n",
      "                    '定を行うことは，ロボティクス分野において重要な問\\n'\n",
      "                    '題である．伝統的な手法として，これらの推定をステレ\\n'\n",
      "                    'オ画像や多視点の複数画像から数値ベースで行うもの\\n'\n",
      "                    'が数多く存在する [10, 11]．一方，近年は，CNN を用\\n'\n",
      "                    'いたニューラルネットワークベースの推定手法が台頭\\n'\n",
      "                    'している．CNN を用いた深度推定の先駆けとなった手\\n'\n",
      "                    '法の一つに Eigen らの手法 [1] がある．この手法では，\\n'\n",
      "                    '深度データの真値を教師データとし，1 枚のカメラ画像\\n'\n",
      "                    'の入力から，ピクセル単位で深度推定した画像を出力\\n'\n",
      "                    'するような CNN の学習を行う．別の手法として Liu ら\\n'\n",
      "                    '[2] は，入力のカメラ画像と深度データの真値の相関関\\n'\n",
      "                    '係を Markov Random Field（MRF）で表し，エネル\\n'\n",
      "                    'ギー関数の設計に必要な Unary term と Pairwise term\\n'\n",
      "                    'を CNN を用いて推定する学習を行うモデルとなってい\\n'\n",
      "                    'る．また，深度データの真値を用いず，ステレオカメ\\n'\n",
      "                    'ラ画像のみを用いて，深度推定を行う手法 [3] も存在す\\n'\n",
      "                    'る．手法 [3] は，ステレオカメラで撮影された 2 枚のフ\\n'\n",
      "                    'レームを CNN への入力データとして学習を行う．左右\\n'\n",
      "                    'のレンズのベースライン間距離が既知である前提で学IS1-32\\n'\n",
      "                    'SO1-32第25回画像センシングシンポジウム図 1 従来手法 [5] '\n",
      "                    'の概観習を行うため，カメラ姿勢を教師データとする学習モ\\n'\n",
      "                    'デルとして扱われている．ただし，教師ありモデルの\\n'\n",
      "                    'これらの手法 [1, 2, 3] は大規模な深度やカメラ姿勢の\\n'\n",
      "                    '真値データを必要とするため，学習のためのデータの\\n'\n",
      "                    '数が大きく限られるという欠点がある．一方，本研究でのベースとなる手法である [5] は，カ\\n'\n",
      "                    'メラ映像のみから深度とカメラ移動量の推定を行う教\\n'\n",
      "                    '師なし学習モデルである．このアプローチは真値デー\\n'\n",
      "                    'タを特に必要としないため有用性が高く，この手法 [5]\\n'\n",
      "                    'をベースとした新たな手法が提案されている．特に，手\\n'\n",
      "                    '法 [7] は，フレーム内の移動物体のモーションを考慮し\\n'\n",
      "                    'た学習モデルであるため，教師なし学習モデルであり\\n'\n",
      "                    'ながら，高い推定精度を実現している．本研究の提案\\n'\n",
      "                    '手法では，[5] の手法をベースとした別のアプローチを\\n'\n",
      "                    '用いることで，高精度な深度推定を行うことを目標と\\n'\n",
      "                    'する．具体的には，cGAN 特有の識別器の導入に伴い，\\n'\n",
      "                    '新たな目的関数を設計する．',\n",
      "              '結論': '本研究では cGAN のアプローチを用いてカメラ映像\\n'\n",
      "                    'から各フレームの深度マップを推定する手法を提案し\\n'\n",
      "                    'た．実験より，車載カメラで撮影した動画に対して，高\\n'\n",
      "                    '精度に深度推定できることを示した．これより，Depth\\n'\n",
      "                    'CNN の性能の向上を確認することができた．また，本\\n'\n",
      "                    '研究の学習モデルは，Depth CNN と Pose CNN の重\\n'\n",
      "                    'みを同時に更新し，深度マップとカメラ移動量の推定\\n'\n",
      "                    '精度を相互に高め合うように学習を行うモデルとなっ\\n'\n",
      "                    'ている．そのため，Depth CNN の性能の向上は，Pose\\n'\n",
      "                    'CNN の向上に関係していることが期待できる．今後は，\\n'\n",
      "                    'これを確かめるため，カメラ移動量の定量評価を行い，\\n'\n",
      "                    'Pose CNN の性能について検証する必要がある．さらに，\\n'\n",
      "                    '車載カメラ映像だけではなく，他のデータセットのカメIS1-32\\n'\n",
      "                    'SO1-32第25回画像センシングシンポジウムラ映像に対しても，推定精度を高められるかどうかを\\n'\n",
      "                    '確かめる実験を行う必要があると考えている．最後に，\\n'\n",
      "                    '今回の提案手法は，関連手法の一つである GeoNet[7]\\n'\n",
      "                    'の深度推定精度を上回ることができなかったことが分\\n'\n",
      "                    'かった．これは，GeoNet が従来手法 [5] をベースに，フ\\n'\n",
      "                    'レーム内の移動物体のモーションも考慮した学習モデ\\n'\n",
      "                    'ルとなっているため，移動物体を含んだフレームの深\\n'\n",
      "                    '度推定をより正確に行うことができていたからだと考\\n'\n",
      "                    'えられる．今後の課題として，cGAN のアプローチを\\n'\n",
      "                    '用いた上で，さらに移動物体のモーションを考慮した\\n'\n",
      "                    '学習モデルの設計等，改善の余地があると考えている．'},\n",
      "  'date': '2000-01-01 00:00:00',\n",
      "  'paper_title': 'カメラ映像を用いた cGAN に基づく深度推定に関する検討',\n",
      "  'pdf_name': 'IS1-32'},\n",
      " {'_id': ObjectId('5eb6bd7260e378f730f5b639'),\n",
      "  'conf_name': 'SSII2019',\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  'content': {'序論': 'Copyright © SSII 2019. All Rights Reserved.- IS1-34 '\n",
      "                    '-レードオフを考える必要がある．\\n'\n",
      "                    '本研究では，3D CNNによる動画認識をするうえで，\\n'\n",
      "                    '認識性能とコストのトレードオフを考えるための基礎\\n'\n",
      "                    '実験に取り組む．本研究では，特に認識精度と計算コス\\n'\n",
      "                    'ト (FLOPs)の関係についての実験をすることで，この\\n'\n",
      "                    'トレードオフについて議論する．これにより，3D CNN\\n'\n",
      "                    'を実際に用いる際の入力サイズの設定方法を明らかに\\n'\n",
      "                    'する．\\n'\n",
      "                    '2 実験設定\\n'\n",
      "                    '本研究では，著者らの先行研究で提案した 3D CNN\\n'\n",
      "                    'のネットワーク構造である 3D ResNet-18 [2]を利用し，\\n'\n",
      "                    '入力サイズが認識性能と計算コストに与える影響につ\\n'\n",
      "                    'いて実験的に議論する．\\n'\n",
      "                    '以下では，本実験の具体的な設定について述べる．\\n'\n",
      "                    '2.1 学習\\n'\n",
      "                    'ネットワークの学習は確率的勾配降下法 (SGD) に\\n'\n",
      "                    'よって行う．学習サンプルの生成は，学習データの動画\\n'\n",
      "                    '中からランダムに生成する形で Data Augmentationしな\\n'\n",
      "                    'がら行う．具体的には，まず各サンプルの時間位置を動\\n'\n",
      "                    '画中から一様分布に従ってランダムに決定する．そし\\n'\n",
      "                    'て，決定した時間位置を中心に T フレームを抽出する．\\n'\n",
      "                    'ここで，動画が T フレームより短かった場合は，T フ\\n'\n",
      "                    'レームになるまで動画を繰り返すことで対応する．な\\n'\n",
      "                    'お，T は入力の時間方向のサイズである．次に，フレー\\n'\n",
      "                    'ム中から空間位置とスケールをランダムで選択して切\\n'\n",
      "                    'り出す．このようにして生成したサンプルを 50%の確\\n'\n",
      "                    '率で水平方向に反転する．以上の手順に従って，学習\\n'\n",
      "                    'サンプルを生成しながら SGDによりネットワークのパ\\n'\n",
      "                    'ラメータを最適化していく．\\n'\n",
      "                    '評価実験では，バッチサイズを 128として，150エポッ\\n'\n",
      "                    'ク学習した．Weight Decayは 0.0001であり，momentum\\n'\n",
      "                    'は 0.9とした．学習率は 0.03から始め，50, 100エポッ\\n'\n",
      "                    'クで学習率を 0.1倍した．\\n'\n",
      "                    '2.2 認識\\n'\n",
      "                    '学習したモデルを用いて動画中の行動を認識する．動\\n'\n",
      "                    '画中から Sliding Windowの形式で T フレームの入力をIS1-34\\n'\n",
      "                    'SO1-34第25回画像センシングシンポジウム生成していき，各入力に対するスコアを推定する．な\\n'\n",
      "                    'お，各 Windowは 16フレームずつ移動する形で重なり\\n'\n",
      "                    'なく動画中から生成しており，各フレームでは中央か\\n'\n",
      "                    'ら最大スケールで切り出しを行っている．そして，動\\n'\n",
      "                    '画中の全入力から推定したスコアの平均を計算し，最\\n'\n",
      "                    '大平均スコアとなるクラスを動画の行動ラベルとして\\n'\n",
      "                    '認識する．\\n'\n",
      "                    '2.3 データセット\\n'\n",
      "                    '評価実験では，Kineticsデータセット [3]を利用した．\\n'\n",
      "                    'Kineticsデータセットは 400種類の行動クラスを含み，\\n'\n",
      "                    '各クラスの動画数は 400以上，全動画数は 30万以上の\\n'\n",
      "                    '大規模動画データセットになっている．学習データ，検\\n'\n",
      "                    '証データ，テストデータはそれぞれ約 24万，2万，4万\\n'\n",
      "                    '動画である．各動画は一つの行動を含むように時間的\\n'\n",
      "                    'に切り出されており，それぞれ約 10秒である．実験時\\n'\n",
      "                    'には各動画を高さが 240画素になるようにリサイズし\\n'\n",
      "                    'て保存してから利用した．\\n'\n",
      "                    '2.4 入力サイズの変化\\n'\n",
      "                    '本実験では，空間方向のサイズと時間方向のサイズ，\\n'\n",
      "                    'それぞれを変化させ，両者が認識性能に与える影響に\\n'\n",
      "                    'ついて議論する．なお，空間方向・時間方向それぞれ\\n'\n",
      "                    'のサイズが計算コストに与える影響は同じである．こ\\n'\n",
      "                    'こで検討すべき点として，それぞれの方向のサイズが\\n'\n",
      "                    '認識性能に与える影響であるため，本実験では別々に\\n'\n",
      "                    '変化させたときの性能を実験的に確認する．\\n'\n",
      "                    '空間方向のサイズとして，C3D [4]などで用いられて\\n'\n",
      "                    '2倍ず\\n'\n",
      "                    'いる 112画素を最小のサイズとし，そこから p\\n'\n",
      "                    'つ 224まで変化させた．\\n'\n",
      "                    '時間方向のサイズとして，C3D [4]などで用いられて\\n'\n",
      "                    'いる 16フレームを最小のサイズとし，そこから 2倍ず\\n'\n",
      "                    'つ 64まで変化させた．なお，16フレームの際には 3D\\n'\n",
      "                    'ResNet-18の conv1の時間方向のストライドを 1とし，\\n'\n",
      "                    'ネットワークの途中で特徴マップの時間方向のサイズ\\n'\n",
      "                    'が 1にならないようにした．\\n'\n",
      "                    '3 実験結果・考察\\n'\n",
      "                    '表 1に，入力の空間方向のサイズを変化させたときの\\n'\n",
      "                    '認識率と計算量 (GFLOPs)を示す．入力サイズのベース\\n'\n",
      "                    'は 32(cid:2)112(cid:2)112であり，空間方向のサイズのみp\\n'\n",
      "                    '2倍ず\\n'\n",
      "                    'つ変化させている．入力サイズを大きくするにつれて，\\n'\n",
      "                    '認識率が向上していることがわかる．また，112 (cid:2) 112\\n'\n",
      "                    'から 158 (cid:2) 158に変化させたときの方が，158 (cid:2) 158か\\n'\n",
      "                    'ら 224 (cid:2) 224に変化させたときよりも変化の差が大き\\n'\n",
      "                    'い．このことから，入力サイズを大きくすることによ\\n'\n",
      "                    'る認識性能の向上は今後頭打ちすることが推測される．\\n'\n",
      "                    'また，この実験では入力サイズを p\\n'\n",
      "                    '2倍ずつ変化させて\\n'\n",
      "                    'いるため，それぞれの計算量は 2倍（縦 (cid:2)横）ずつ変\\n'\n",
      "                    '化している．必要な認識率に応じて適切な入力サイズCopyright © SSII 2019. All Rights '\n",
      "                    'Reserved.- IS1-34 -表 1 空間方向のサイズの変化\\n'\n",
      "                    '入力サイズ 112\\n'\n",
      "                    '認識率15822456.5\\n'\n",
      "                    '6.459.3\\n'\n",
      "                    '12.961.1\\n'\n",
      "                    '25.4GFLOPs表 2 時間方向のサイズの変化\\n'\n",
      "                    '入力サイズ 16\\n'\n",
      "                    '認識率643256.5\\n'\n",
      "                    '6.456.5\\n'\n",
      "                    '6.459.1\\n'\n",
      "                    '12.9GFLOPsを選ぶことが望ましい．\\n'\n",
      "                    '次に，表 2に，入力の時間方向のサイズを変化させた\\n'\n",
      "                    'ときの認識率と計算量を示す．入力サイズのベースは\\n'\n",
      "                    '16 (cid:2) 112 (cid:2) 112であり，時間方向のサイズのみ 2倍ず\\n'\n",
      "                    'つ変化させている．この実験では，入力サイズを 16フ\\n'\n",
      "                    'レームにする場合，ResNet-18の畳み込みをしていく中\\n'\n",
      "                    'で特徴マップの時間方向のサイズが 1になってしまう\\n'\n",
      "                    'ため，conv1の時間方向の strideを 2から 1に変更する\\n'\n",
      "                    'ことでそれを防いでいる．そのため，計算量について\\n'\n",
      "                    'は 16フレームと 32フレームでの変化はない．認識率\\n'\n",
      "                    'の変化を確認すると，16フレームから 32フレームに上\\n'\n",
      "                    'げたときでは認識率は向上せず，64フレームまで大き\\n'\n",
      "                    'くしたときは認識率が向上していることがわかる．従\\n'\n",
      "                    '来研究では，入力の時間方向のサイズを大きくするこ\\n'\n",
      "                    'とで，より広範囲の時間変化を捉えることができるた\\n'\n",
      "                    'め認識性能が向上すると報告されていた [5]．しかし，\\n'\n",
      "                    '本実験の結果では 16フレームと 32フレームの差が見\\n'\n",
      "                    'られなかったことから，入力の時間サイズを大きくす\\n'\n",
      "                    'ることの利点は広範囲の観測を可能にすることではな\\n'\n",
      "                    'く，単に畳み込み時の特徴マップのサイズが大きくな\\n'\n",
      "                    'ることにあるのではないかと考えられる．\\n'\n",
      "                    '空間方向と時間方向，それぞれのサイズを変化させた\\n'\n",
      "                    'ときの結果を比較すると，入力サイズが 16(cid:2) 158(cid:2) 158\\n'\n",
      "                    'のときと 64 (cid:2) 112 (cid:2) 112のときでは計算量は同じであ\\n'\n",
      "                    'り，認識率にも大きな差が見られない．そのため，認\\n'\n",
      "                    '識性能と計算量のバランスを考えるうえでは，入力サ\\n'\n",
      "                    'イズを空間方向，時間方向どちらに変化させても同様\\n'\n",
      "                    'に考えて良いことになる．\\n'\n",
      "                    '4 おわりに\\n'\n",
      "                    '本研究では，3D CNNによる動画認識を行う上で入\\n'\n",
      "                    '力サイズが認識性能に与える影響を実験的に評価する\\n'\n",
      "                    '上で，実利用時に適切な入力サイズを選択する手がか\\n'\n",
      "                    'りを提供した．今後は，空間方向，時間方向どちらもIS1-34\\n'\n",
      "                    'SO1-34第25回画像センシングシンポジウム大きくした際の認識性能の変化を確認することなどを\\n'\n",
      "                    '検討している．\\n'\n",
      "                    '参考文献[1] J. Carreira and A. Zisserman. Quo vadis, action '\n",
      "                    'recog-\\n'\n",
      "                    'nition? A new model and the Kinetics dataset. In Pro-\\n'\n",
      "                    'ceedings of the IEEE Conference on Computer Vision\\n'\n",
      "                    'and Pattern Recognition (CVPR), pages 4724–4733,\\n'\n",
      "                    '2017.[2] K. Hara, H. Kataoka, and Y. Satoh. Can '\n",
      "                    'spatiotem-\\n'\n",
      "                    'poral 3D CNNs retrace the history of 2D CNNs and\\n'\n",
      "                    'In Proceedings of the IEEE Conference\\n'\n",
      "                    'imageNet?\\n'\n",
      "                    'on Computer Vision and Pattern Recognition (CVPR),\\n'\n",
      "                    'pages 6546–6555, 2018.[3] W. Kay, J. Carreira, K. '\n",
      "                    'Simonyan, B. Zhang, C. Hillier,\\n'\n",
      "                    'S. Vijayanarasimhan, F. Viola, T. Green, T. Back,\\n'\n",
      "                    'P. Natsev, M. Suleyman, and A. Zisserman. The Ki-\\n'\n",
      "                    'netics human action video dataset. arXiv preprint,\\n'\n",
      "                    'arXiv:1705.06950, 2017.[4] D. Tran, L. Bourdev, R. '\n",
      "                    'Fergus, L. Torresani, and\\n'\n",
      "                    'M. Paluri. Learning spatiotemporal features with 3D\\n'\n",
      "                    'convolutional networks. In Proceedings of the Interna-\\n'\n",
      "                    'tional Conference on Computer Vision (ICCV), pages\\n'\n",
      "                    '4489–4497, 2015.[5] G. Varol, I. Laptev, and C. Schmid. '\n",
      "                    'Long-term tempo-\\n'\n",
      "                    'ral convolutions for action recognition. IEEE Transac-\\n'\n",
      "                    'tions on Pattern Analysis Machine Intelligence, 40(6),\\n'\n",
      "                    '2018.[6] X. Wang, R. Girshick, A. Gupta, and K. He. '\n",
      "                    'Non-local\\n'\n",
      "                    'In Proceedings of the IEEE Con-\\n'\n",
      "                    'neural networks.\\n'\n",
      "                    'ference on Computer Vision and Pattern Recognition\\n'\n",
      "                    '(CVPR), pages 7794–7803, 2018.Copyright © SSII 2019. All '\n",
      "                    'Rights Reserved.- IS1-34 -',\n",
      "              '結論': 'IS1-34\\nSO1-34第25回画像センシングシンポジウム'},\n",
      "  'date': '2000-01-01 00:00:00',\n",
      "  'paper_title': '動画認識のための時空間 3次元畳み込みニューラルネットワークにおける',\n",
      "  'pdf_name': 'IS1-34'},\n",
      " {'_id': ObjectId('5eb6bdba60e378f730f5b640'),\n",
      "  'conf_name': 'SSII2019',\n",
      "  'content': {'序論': '社会インフラ事業において，長期使用する設備の保全は重要な項目である．電力インフラにおける送電鉄塔は，1970年前後の高度成長期に数多く建設されており，今後高経年化に伴う錆劣化が増加するこ\\n'\n",
      "                    'とが見込まれている[1]．そこで，電力会社は，鉄塔\\n'\n",
      "                    'の錆劣化を毎年点検し，錆量の割合が多いものから順番に補修することで電力インフラの長寿命化を図っている．錆劣化の判定は，山奥などの現場に専門家を派遣して実地調査を行うため，コスト増加が課題となっていた．そこで近年，ヘリなどから空撮した画像を用いて判定することで省力化を図るアプローチが\\n'\n",
      "                    '検討されている[2]．しかし，従来の画像処理に基づ\\n'\n",
      "                    'く手法では，錆と同色の背景が含まれる場合に誤って錆と判定してしまう課題があった．また，画像から鉄塔を抽出してから錆劣化を判定するアプローチが考えられるが，画像のみでは背景と鉄塔を精度良く分離することは難しく，抽出精度が低いという課題があった．そこで，本稿ではカラー画像と距離情報を同時に取得可能なカラー開口撮像技術を用い、これらの信号から鉄塔を高精度に抽出するニューラルネットワークを，深層学習を用いて学習する手法を提案する．提案する錆劣化の判定処理は以下の2ステップで構成される．1.  '\n",
      "                    'カラー画像と距離情報から対象領域を抽出 \\n'\n",
      "                    '2.  '\n",
      "                    '抽出領域に対して錆劣化を判定この手法では，前段処理での過抽出が後段処理に影響するため，過抽出の低減に対する優先度を高く考えなければならない．また，過抽出低減のためにはより高精度に抽出する必要があり，画像の解像度\\n'\n",
      "                    'も重要視しなければならない．以降，2 章で本稿の距離情報取得のために用いた\\n'\n",
      "                    'カラー開口撮像技術について述べ，3 章では物体抽\\n'\n",
      "                    '出の従来手法について，4 章では，提案手法につい\\n'\n",
      "                    'て説明する．5 章で実験結果について説明し，提案Copyright © SSII 2019. All Rights '\n",
      "                    'Reserved.- IS1-41 -(a)  カラー開口  (b)  カラー画像  (c)  距離情報図  1  '\n",
      "                    'カラー開口撮像技術手法の有効性を示し，6 章でまとめを述べる．2  '\n",
      "                    'カラー開口撮像技術画像を用いた距離計測手段として，ステレオカメラがよく知られているが，高精度に距離計測するには\\n'\n",
      "                    '測定対象までの距離に応じて，最適な 2 台のカメラ\\n'\n",
      "                    'の間隔（眼間）を設定する必要があり，原理的に小型化と高精度化の両立が難しい．そこで東芝は，小型化に適した単眼カメラを用いて，カラー画像と距離情報を同時に取得できるカラー開口撮像技術を開発し\\n'\n",
      "                    'ている[3]．カラー開口は，シアンとイエローの 2 色の\\n'\n",
      "                    'カラーフィルタで開口を 2 分割する構造開口である．\\n'\n",
      "                    'このカラー開口を用いることで撮像画像のぼけを色ごとに変化させ，その形状違いを利用して，被写体までの距離を推定する．同一光学系の単眼カメラで撮影\\n'\n",
      "                    'した 1 枚の画像を元に距離を推定するため，位置ず\\n'\n",
      "                    'れや時間変化を補正する必要がなく，高い距離分解\\n'\n",
      "                    '能が得られる．図  1 にカラー開口，カラー開口撮像\\n'\n",
      "                    '技術によって得られるカラー画像と距離情報の一例\\n'\n",
      "                    'を示す．本稿では，100～200m の距離でも距離情\\n'\n",
      "                    '報が取得でき，なおかつヘリコプターが動いていても撮影が可能なセンシングデバイスとしてカラー開口撮\\n'\n",
      "                    '像技術を用いる．',\n",
      "              '結論': '鉄 塔 保 守 に 向 け た セ グ メ ン テ ー シ ョ ン と し て ，\\n'\n",
      "                    'FuseNet に U-Net のネットワーク構造を付加した U-\\n'\n",
      "                    'FuseNet 方式を提案し，実験により構造が複雑な領\\n'\n",
      "                    '域において背景との分離ができることを示した．また，抽出結果に対して錆量判定を行ったところ，過抽出\\n'\n",
      "                    'を低減できる U-FuseNet を用いることで，より高精度\\n'\n",
      "                    'な錆量判定を行うことができることを示した．参考文献[1]  '\n",
      "                    '電力中央研究所，”送電用鉄塔の腐食等に対する健全性評価技術に関する研究”，https://criepi.denken.or.jp/jp/kenkikaku/report/\\n'\n",
      "                    'download/1GmZ6etQGT2HSEF5lBKu6L5O7\\n'\n",
      "                    'JhNv7hX/N24.pdf[2]  電力中央研究所報告，”塗装すべき経年鉄塔\\n'\n",
      "                    'の選定に役立つ画像処理技術の開発-空撮\\n'\n",
      "                    '画像を使った簡易劣化判定のための支援ツ\\n'\n",
      "                    'ールのプロトタイプの開発-”，June.2018[3]  森内優介，三島直，”カラー開口を用いたIS1-41\\n'\n",
      "                    'SO1-41第25回画像センシングシンポジウムDFD技術”, SSII2016[4]  Vijay  '\n",
      "                    'Badrinarayanan,  Ankur  Handa  and \\n'\n",
      "                    'Roberto  Cipolla, \\n'\n",
      "                    '\"SegNet:  A  Deep \\n'\n",
      "                    'Convolutional  Encoder-Decoder  Architecture \\n'\n",
      "                    'for  Robust  Semantic  Pixel-Wise  Labelling.\", \\n'\n",
      "                    'arXiv preprint arXiv:1505.07293, 2015.[5]  Olaf '\n",
      "                    'Ronneberger, Philipp Fischer, and Thomas \\n'\n",
      "                    'Brox,  “U-Net:  Convolutional  Networks  for \\n'\n",
      "                    'Biomedical  Image  Segmentation”,  Computer \\n'\n",
      "                    'Vision and Pattern Recognition, May.2015[6]  Caner  '\n",
      "                    'Hazirbas,  Lingni  Ma,  Csaba  Domokos \\n'\n",
      "                    'and  Daniel  Cremers,  \"FuseNet:  Incorporating \\n'\n",
      "                    'Depth into Semantic Segmentation via Fusion-\\n'\n",
      "                    'based CNN Architecture\", in proceedings of the \\n'\n",
      "                    '13th  Asian  Conference  on  Computer  Vision, \\n'\n",
      "                    '2016.[7]  Caffe, https://github.com/BVLC/caffe \\n'\n",
      "                    '[8]  SegNet,https://github.com/TimoSaemann/caffe-segnet-\\n'\n",
      "                    'cudnn5[9]  架空送電線の電線劣化 （腐食）現象調査専\\n'\n",
      "                    '門委員会，”架空送電設備の鋼材腐食・摩耗\\n'\n",
      "                    '現象”，電気学会技術報告，2009，1163 号Copyright © SSII 2019. All Rights '\n",
      "                    'Reserved.- IS1-41 -'},\n",
      "  'date': '2000-01-01 00:00:00',\n",
      "  'paper_title': 'カラー画像と距離情報を統合した深層学習による高精度な鉄塔抽出技術',\n",
      "  'pdf_name': 'IS1-41'}]\n"
     ]
    }
   ],
   "source": [
    "dict_list = next(database_loader_generator)\n",
    "pprint.pprint(dict_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dictのリストのパース"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "メモリを削減するためdataloaderからfindしたリストそれぞれをPaperオブジェクトに変換する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DocumentのリストからPaperのリストを取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-10T08:04:32.481267Z",
     "start_time": "2020-05-10T08:04:32.457330Z"
    }
   },
   "outputs": [],
   "source": [
    "class DictListPdfParser():\n",
    "    def __init__(self, pdf_parser):\n",
    "        self.pdf_parser = pdf_parser\n",
    "\n",
    "    def parse_from_dict_list(self, dict_list):\n",
    "        paper_list = [self.pdf_parser.parse_by_dict(one_dict) for one_dict in dict_list]\n",
    "        return paper_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-10T08:04:33.217502Z",
     "start_time": "2020-05-10T08:04:33.198553Z"
    }
   },
   "outputs": [],
   "source": [
    "list_paper_parser = PdfParserCount(count_patterns=find_patterns)\n",
    "dict_list_pdf_parser = DictListPdfParser(pdf_parser=list_paper_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-10T08:05:58.934145Z",
     "start_time": "2020-05-10T08:05:58.899240Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IS1-24\n",
      "車載カメラ映像からの二輪車との交通事故リスク推定\n",
      "OrderedDict([(re.compile('ディープラーニング|深層学習'), 6), (re.compile('CNN|ニューラルネットワーク'), 24), (re.compile('VAE|変分オートエンコーダ'), 0), (re.compile('GAN'), 0)])\n",
      "2000-01-01 00:00:00, IS1-31\n",
      "\n",
      "OrderedDict([(re.compile('ディープラーニング|深層学習'), 3), (re.compile('CNN|ニューラルネットワーク'), 6), (re.compile('VAE|変分オートエンコーダ'), 0), (re.compile('GAN'), 0)])\n",
      "2000-01-01 00:00:00, IS1-26\n",
      "深層学習による水中映像からのマグロ成魚の尾数推定\n",
      "OrderedDict([(re.compile('ディープラーニング|深層学習'), 2), (re.compile('CNN|ニューラルネットワーク'), 2), (re.compile('VAE|変分オートエンコーダ'), 0), (re.compile('GAN'), 0)])\n",
      "2000-01-01 00:00:00, IS1-41\n",
      "カラー画像と距離情報を統合した深層学習による高精度な鉄塔抽出技術\n",
      "OrderedDict([(re.compile('ディープラーニング|深層学習'), 1), (re.compile('CNN|ニューラルネットワーク'), 2), (re.compile('VAE|変分オートエンコーダ'), 0), (re.compile('GAN'), 0)])\n",
      "2000-01-01 00:00:00, IS1-32\n",
      "カメラ映像を用いた cGAN に基づく深度推定に関する検討\n",
      "OrderedDict([(re.compile('ディープラーニング|深層学習'), 0), (re.compile('CNN|ニューラルネットワーク'), 15), (re.compile('VAE|変分オートエンコーダ'), 0), (re.compile('GAN'), 6)])\n",
      "2000-01-01 00:00:00, IS1-27\n",
      "\n",
      "OrderedDict([(re.compile('ディープラーニング|深層学習'), 0), (re.compile('CNN|ニューラルネットワーク'), 7), (re.compile('VAE|変分オートエンコーダ'), 0), (re.compile('GAN'), 3)])\n",
      "2000-01-01 00:00:00, IS1-34\n",
      "動画認識のための時空間 3次元畳み込みニューラルネットワークにおける\n",
      "OrderedDict([(re.compile('ディープラーニング|深層学習'), 0), (re.compile('CNN|ニューラルネットワーク'), 6), (re.compile('VAE|変分オートエンコーダ'), 0), (re.compile('GAN'), 0)])\n",
      "2000-01-01 00:00:00, IS1-28\n",
      "\n",
      "OrderedDict([(re.compile('ディープラーニング|深層学習'), 0), (re.compile('CNN|ニューラルネットワーク'), 5), (re.compile('VAE|変分オートエンコーダ'), 0), (re.compile('GAN'), 0)])\n",
      "2000-01-01 00:00:00, IS1-25\n",
      "画像変換に基づく高齢者の運転時注視マップの推定\n",
      "OrderedDict([(re.compile('ディープラーニング|深層学習'), 0), (re.compile('CNN|ニューラルネットワーク'), 2), (re.compile('VAE|変分オートエンコーダ'), 0), (re.compile('GAN'), 0)])\n",
      "2000-01-01 00:00:00]\n"
     ]
    }
   ],
   "source": [
    "paper_list = dict_list_pdf_parser.parse_from_dict_list(dict_list)\n",
    "paper_list.sort(key=lambda paper_counter: tuple(paper_counter.counters.values()),reverse=True)\n",
    "print(paper_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdf_py37",
   "language": "python",
   "name": "pdf_py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
