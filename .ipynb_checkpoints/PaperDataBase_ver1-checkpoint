{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T16:16:12.420231Z",
     "start_time": "2020-05-09T16:16:11.408232Z"
    }
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import datetime\n",
    "import pprint\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T16:16:14.525807Z",
     "start_time": "2020-05-09T16:16:12.441180Z"
    }
   },
   "outputs": [],
   "source": [
    "from pdf_parser_ver2 import PdfParser, DirectoryPdfParser, PdfParserCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データベースの設定 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T16:16:14.685369Z",
     "start_time": "2020-05-09T16:16:14.566686Z"
    }
   },
   "outputs": [],
   "source": [
    "client = MongoClient('localhost', 27017)\n",
    "db = client[\"db\"]\n",
    "collection = db[\"papers\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データベースから正規表現で検索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T16:16:14.721273Z",
     "start_time": "2020-05-09T16:16:14.698335Z"
    }
   },
   "outputs": [],
   "source": [
    "count_patterns = [re.compile(\"ディープラーニング|深層学習\"),\n",
    "                  re.compile(\"CNN|ニューラルネットワーク\"),\n",
    "                  re.compile(\"VAE|変分オートエンコーダ\"),\n",
    "                  re.compile(\"GAN\")\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 複数の正規表現で検索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T16:16:15.720302Z",
     "start_time": "2020-05-09T16:16:14.779116Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "find_result = collection.find({\"$or\":[{\"content.序論\":re.compile(\"VAE|変分オートエンコーダ\")},\n",
    "                                      {\"content.序論\":re.compile(\"GAN\")}]})\n",
    "counter = 0\n",
    "for i in find_result:\n",
    "    counter += 1\n",
    "    #pprint.pprint(i)\n",
    "    \n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### パターンのリストで指定 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T16:16:17.002553Z",
     "start_time": "2020-05-09T16:16:15.737249Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n"
     ]
    }
   ],
   "source": [
    "content_name = \"序論\"\n",
    "find_result = collection.find(filter={\"$or\":[{\"content.\"+content_name:i} for i in count_patterns]}).sort(\"date\")  # dateはないけど\n",
    "counter = 0\n",
    "for i in find_result:\n",
    "    counter += 1\n",
    "    #pprint.pprint(i)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データベースから取得し，検索するクラス "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T16:16:17.060028Z",
     "start_time": "2020-05-09T16:16:17.012527Z"
    }
   },
   "outputs": [],
   "source": [
    "class PaperDataBaseLoadPattern():\n",
    "    def __init__(self, collection, find_patterns, content_name, find_max=100):\n",
    "        self.collection = collection\n",
    "        self.find_patterns = find_patterns\n",
    "        self.content_name = content_name\n",
    "        self.find_max = find_max\n",
    "        \n",
    "    def load(self):\n",
    "        \"\"\"\n",
    "        検索結果からfind_max分だけ検索．呼んだ回数だけ検索結果が進む\n",
    "        \"\"\"\n",
    "        result = collection.find(filter={\"$or\":[{\"content.\"+self.content_name:i} for i in self.find_patterns]}).sort(\"date\")  # dateはないけど\n",
    "        out_list = []\n",
    "        iter_counter = 0\n",
    "        for paper in result:\n",
    "            out_list.append(paper)\n",
    "            iter_counter += 1\n",
    "            if iter_counter >= self.find_max-1:\n",
    "                yield out_list\n",
    "                out_list = []  # リストの初期化\n",
    "                iter_counter = 0\n",
    "        \n",
    "        #yield out_list #全て終わったときに返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T16:16:17.128519Z",
     "start_time": "2020-05-09T16:16:17.076503Z"
    }
   },
   "outputs": [],
   "source": [
    "find_patterns = [re.compile(\"ディープラーニング|深層学習\"),\n",
    "                 re.compile(\"CNN|ニューラルネットワーク\"),\n",
    "                 re.compile(\"VAE|変分オートエンコーダ\"),\n",
    "                 re.compile(\"GAN\")\n",
    "                ]\n",
    "\n",
    "database_loader = PaperDataBaseLoadPattern(collection=collection,\n",
    "                                           content_name=\"序論\",\n",
    "                                           find_patterns=find_patterns,\n",
    "                                           find_max=10\n",
    "                                           )\n",
    "database_loader_generator = database_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T16:16:19.086577Z",
     "start_time": "2020-05-09T16:16:17.143478Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'_id': ObjectId('5eb6b97560e378f730f5b619'),\n",
      "  'conf_name': 'SSII2019',\n",
      "  'content': {'序論': '近年，画像を活用した異常検出手法は数多く提案され\\n'\n",
      "                    'ており，不良品検出や，監視カメラからの危険人物の検\\n'\n",
      "                    '出，医療の発病検知などの領域に応用されている [1{4]．\\n'\n",
      "                    'その中でも，生成モデルで通常クラスの画像を十分\\n'\n",
      "                    '学習させた後，生成モデルに画像を入力し生成された\\n'\n",
      "                    '再構成画像と，入力画像との誤差画像を基に異常を検\\n'\n",
      "                    '出する手法が代表的である．生成モデルとして Auto-\\n'\n",
      "                    'Encoder (AE) を応用した異常検出手法 [5] や，Vari-\\n'\n",
      "                    'ational Auto-Encoder (VAE) [6] を応用した手法 [7]，\\n'\n",
      "                    'Generative Adversarial Networks (GAN) [8] を活用し\\n'\n",
      "                    'た手法 [9] が既に提案されている．その中でも，GAN\\n'\n",
      "                    'を活用した手法 [9] は，精度が良いものの，識別時に再\\n'\n",
      "                    '構成画像と入力画像との誤差の最小化計算のコストが\\n'\n",
      "                    'かかる問題 [10] や，最小化中に初期値によっては局所\\n'\n",
      "                    '解に陥ってしまう問題がある．そこで，VAE を用いて\\n'\n",
      "                    'GAN の初期値を決めることで，常に入力画像に近い生\\n'\n",
      "                    '成画像を出力する VAEGAN [11] の活用が必要である\\n'\n",
      "                    'と考える．一方で，実世界での利用を想定した場合，入力画像\\n'\n",
      "                    'にはノイズが多く含まれる．生成モデルでは平均的な\\n'\n",
      "                    '画像を出力するため，入力画像にノイズが混入してい\\n'\n",
      "                    'る場合，モデルはノイズの少ない画像を生成する．そ\\n'\n",
      "                    'のため，ノイズ部には再構成誤差が生じる．前述した\\n'\n",
      "                    '手法では，画像全体の再構成誤差の大きさにより判定\\n'\n",
      "                    'を行っているため，異常に起因する誤差と，ノイズに\\n'\n",
      "                    '起因する誤差を分ける必要がある．しかし，これらの\\n'\n",
      "                    '手法では，その機構がない．すなわち，ノイズ部の誤\\n'\n",
      "                    '差と本来の異常領域の誤差の区別ができなく，ノイズ\\n'\n",
      "                    'への頑健性が懸念される．そこで我々は以前，異常ク\\n'\n",
      "                    'ラスの一部を教示し，識別に必要となる領域に焦点を\\n'\n",
      "                    '当てることで，ノイズに頑健な手法を提案した [12,13]．\\n'\n",
      "                    'ところが，異常画像の一部を事前に知り得る必要があ\\n'\n",
      "                    'り，それらが未知の場合は根本的な問題となる．そこで本稿では，まず，先の局所解の問題を解決する\\n'\n",
      "                    'ために，VAEGAN を用いた異常値判検出手法を提案す\\n'\n",
      "                    'る．次に，ノイズへの頑健性を向上させるために，VAE-\\n'\n",
      "                    'GAN の Discriminator から Region-of-interest (ROI)\\n'\n",
      "                    'を算出し，それを再構成誤差の重み付けに活用する手Copyright © SSII 2019. All Rights '\n",
      "                    'Reserved.- IS1-02 -図 1: 提案手法の概要図．‘0’ が正常クラスであり，‘4’\\n'\n",
      "                    'の画像が入力された時の手法による異常度合いの違い．\\n'\n",
      "                    'Na(cid:127)(cid:16)ve に VAEGAN を用いた手法では，ピンクの領域が\\n'\n",
      "                    'ノイズの有無に起因しているため，判別に悪影響を及\\n'\n",
      "                    'ぼしてしまう．法を提案する．そして，VAE，GAN を活用した従来手\\n'\n",
      "                    '法と比較実験を実施し，評価する．提案手法の概略図\\n'\n",
      "                    'を図 1 に示す．',\n",
      "              '結論': '本稿では，VAEGAN を活用した異常検出手法と，更\\n'\n",
      "                    'に Discriminator から算出された ROI も活用するノイ\\n'\n",
      "                    'ズに頑健な異常検出手法を提案した．そして，これら\\n'\n",
      "                    'の手法は，複数のデータセットでの評価により，従来\\n'\n",
      "                    '手法よりも高精度に異常検出が可能であることを確認\\n'\n",
      "                    'した．今後の課題として，他の異常検出の画像データセッ\\n'\n",
      "                    'トを活用した評価や，VAEGAN の学習中に ROI 情報\\n'\n",
      "                    'を活用する手法の提案などが挙げられる．IS1-02\\n'\n",
      "                    'SO1-02第25回画像センシングシンポジウムnormal digitMNIST w/ noiseaverage\\n'\n",
      "                    '.63(cid:6).2 .83(cid:6).1 .54(cid:6).1 .57(cid:6).1 '\n",
      "                    '.62(cid:6).1 .67(cid:6).0 .61(cid:6).1 .69(cid:6).1 '\n",
      "                    '.55(cid:6).1 .70(cid:6).0 .64(cid:6).1\\n'\n",
      "                    '.45(cid:6).0 .75(cid:6).0 .48(cid:6).0 .49(cid:6).0 '\n",
      "                    '.58(cid:6).0 .51(cid:6).0 .51(cid:6).0 .59(cid:6).0 '\n",
      "                    '.47(cid:6).0 .56(cid:6).0 .54(cid:6).1\\n'\n",
      "                    'Na(cid:127)(cid:16)ve VAEGAN .71(cid:6).0 .86(cid:6).0 '\n",
      "                    '.60(cid:6).0 .65(cid:6).0 .68(cid:6).0 .66(cid:6).0 '\n",
      "                    '.70(cid:6).0 .73(cid:6).0 .64(cid:6).0 .70(cid:6).0 '\n",
      "                    '.69(cid:6).1\\n'\n",
      "                    'VAEGAN+ROI .74(cid:6).0 .92(cid:6).0 .62(cid:6).0 '\n",
      "                    '.68(cid:6).0 .69(cid:6).0 .69(cid:6).0 .73(cid:6).0 '\n",
      "                    '.78(cid:6).0 .66(cid:6).0 .74(cid:6).0 '\n",
      "                    '.73(cid:6).12103VAE [7]GAN [9]表 1: ノイズを付与した MNIST '\n",
      "                    'データセットに対する異常検出における ROC 曲線の AUC 値．正常クラスを\\n'\n",
      "                    '‘0’(cid:24)‘9’ に変更し，それぞれの値及びその平均値を記載．例えば，正常クラスが ‘0’ '\n",
      "                    'の場合，‘1’(cid:24)‘9’ が異常クラス．Pigeon [16].64(cid:6).0\\n'\n",
      "                    '.49(cid:6).0\\n'\n",
      "                    '.68(cid:6).0\\n'\n",
      "                    '.77(cid:6).1VAE [7]GAN [9]Na(cid:127)(cid:16)ve '\n",
      "                    'VAEGANVAEGAN+ROI表 2: 鳩のポーズのデータセットに対する異常検出にお\\n'\n",
      "                    'ける ROC 曲線の AUC 値'},\n",
      "  'date': '2000-01-01 00:00:00',\n",
      "  'paper_title': 'VAEGAN の再構成誤差と Discriminator の ROI を活用した異常検出',\n",
      "  'pdf_name': 'IS1-02'},\n",
      " {'_id': ObjectId('5eb6b97560e378f730f5b61a'),\n",
      "  'conf_name': 'SSII2019',\n",
      "  'content': {'序論': '橋梁や道路，トンネルなどの社会インフラは年々増加している．これらの設備は建設後，使用とともに徐々に老朽化するため，定期的な検査が必要である．一般的に，構造物の検査には非破壊検査が用いられており，特に内部欠陥や厚さの測定には超音波厚さ測定が用いられる．超音波厚さ測定はソーナーの原理と同じで，超音波が往復する時間差によって所定の位置の厚さ（長さ）を求める検査方法である．測定器具の取扱いや計測データの解釈には専門的知識及び時間を必要とするため，大規模な測定は困難\\n'\n",
      "                    'である．このような課題に対し，本研究ではCNN によ\\n'\n",
      "                    'り測定データから超音波エコーを検出し，厚さを求める手法を提案する．実験の結果，専門的知識を必要\\n'\n",
      "                    'としなくても 95%のデータの厚みを測定でき，本手法\\n'\n",
      "                    'の有効性が確認できた．1  '\n",
      "                    'はじめに金属やコンクリートなどの厚さを測定する場合，ノギスやマイクロメータ等の物体を挟み込んで測定する厚さ測定器が使用される．測定対象物がタンクや配管など，物理的にこれらの測定器を使用できない場合は，非破壊検査手法のひとつである，超音波厚さ\\n'\n",
      "                    '測定（UTM：Ultrasonic Thickness Measurement）が用\\n'\n",
      "                    'いられることが多い．超音波厚さ測定とは，超音波探触子で発生した超音波を測定対象物に入射し，その反射波を計測することによって対象物の厚さを測定する検査方法である．超音波が測定対象物に垂直もしくは設定した任意の角度で入射し，所定の位置で反射することを前提とするため，測定器具の取扱いや計測データの読み取りに専門的技量を必要とする．そのため，超音波厚さ測定は他の非破壊試験と同様\\n'\n",
      "                    'に，日本工業規格（JIS）で測定方法が規定されてい\\n'\n",
      "                    'る[1]．また，各種法定検査では認定技術者による検\\n'\n",
      "                    '査が求められている．一方，国土交通省の予測によCopyright © SSII 2019. All Rights '\n",
      "                    'Reserved.- IS1-03 '\n",
      "                    '-ると，高度経済成長以降に整備されたインフラ設備が今後一斉に老朽化するとされており，効率的な維\\n'\n",
      "                    '持管理が喫緊の課題となっている．超音波厚さ測定や類似検査である超音波探傷試験では，探触子を自動で移動走査することによる自\\n'\n",
      "                    '動計測システム[2]は開発されているものの，計測デ\\n'\n",
      "                    'ータから測定値を算出する部分には言及されていない．一般に，超音波厚さ測定では計測データ中の超音波エコーから測定対象物の厚さを求めているが，超音波エコーの妥当性は目視で確認している．その確認に多大な時間を要するため省力化が課題となっている．本研究では，超音波厚さ測定における計測\\n'\n",
      "                    'データから Convolutional  Neural  Network  (CNN)を\\n'\n",
      "                    '用いて超音波エコーを検出し，厚さを算出する手法\\n'\n",
      "                    'を提案する．2  '\n",
      "                    '超音波厚さ測定（UTM）超音波厚さ測定は，超音波探触子から発生した超音波パルスを測定対象物に入射し，その伝搬時間をもとに対象物の厚さを求める方法である．探触子で発生した超音波を効率よく測定対象物に入射させるために，対象物を水などの媒質内に沈めて測定されることが多い．ここでは，超音波パルスが対象物中を図 '\n",
      "                    '1 に示すように，厚さ(cid:1830)の測定対象物中に超音往復する時間を計測することで厚さを求める，パルス\\n'\n",
      "                    '反射法について説明する．波探触子から超音波パルス T を送信したとき，T の一\\n'\n",
      "                    '部は対象物の表面で反射し，探触子で受信される．図 1.  超音波厚さ測定の概略図IS1-03\\n'\n",
      "                    'SO1-03第25回画像センシングシンポジウムこれを表面エコーS と呼ぶ．探触子から送信した超音\\n'\n",
      "                    '波パルス T の一部は，測定対象物の内部に入射し，\\n'\n",
      "                    '対象物の底面で反射する．底面で反射した超音波の一部が測定対象物表面を透過し，探触子で受信され\\n'\n",
      "                    'る．これを第 1 底面エコーB1 と呼ぶ．また，底面で反\\n'\n",
      "                    '射した超音波の一部は対象物表面で反射し，再び\\n'\n",
      "                    '底面で反射した後に表面を透過し，探触子で第 2 底\\n'\n",
      "                    '面エコーB2 として受信される．以降，同様の経路が\\n'\n",
      "                    '繰り返され，探触子では複数の底面エコーを受信す\\n'\n",
      "                    'る．これを多重エコーと呼ぶ．S と B1 との受信時間差（伝搬時間差）(cid:1872)，測定対象\\n'\n",
      "                    '物の音速(cid:1829)のとき，(1)式のように示される．(cid:1830)(cid:3404)(cid:1829)(cid:1872)2(1)すなわち，反射パルス（エコー）の受信時間差を計測\\n'\n",
      "                    'することで，対象物の厚さを求められる．実際の測定においては周辺ノイズに起因する電気信号によって受信波形が乱れることがある．超音波\\n'\n",
      "                    '厚さ測定では，ゲート（閾値）を設定し，表面エコーS\\n'\n",
      "                    'および底面エコーB1 を検出する，ゲート方式と呼ば\\n'\n",
      "                    'れる方法が採用されることが多い．ゲート設定値は，測定の度にエコーを確認しながら検査員が調整しているため，定量的な設定が困難である．また，ノイズ\\n'\n",
      "                    'や多重エコーが含まれるデータから S と B1 を自動的\\n'\n",
      "                    'に検出することは容易ではない．図 2 に示すようにノ\\n'\n",
      "                    'イズや多重エコーを S もしくは B1 として検出してしま\\n'\n",
      "                    'うと，実際とは異なる厚さが求まってしまう．そのため，検査員が測定波形を目視で確認し，ゲート方式で検出したエコー位置を修正している．一回の検査での測定点は数万点以上のこともあり，目視確認には多大な時間を要するため，作業工数の低減が課題とな\\n'\n",
      "                    'っている．図 2.  計測エコー例Copyright © SSII 2019. All Rights '\n",
      "                    'Reserved.- IS1-03 -',\n",
      "              '結論': '超音波サンプリングデータから波形画像を生成し，\\n'\n",
      "                    'CNN により表面エコーS および第 1 底面エコーB1\\n'\n",
      "                    'のピーク位置を検出する手法を提案した．人工的な欠陥部を有する鋼鉄材料の測定データを用いた評\\n'\n",
      "                    '価実験の結果，専門的知識がなくとも 95%のデータ\\n'\n",
      "                    'に対しては厚さを算出できることが分かった．また，残\\n'\n",
      "                    'りの 5%に対しては，測定データの測定対象物上で\\n'\n",
      "                    'の物理的配置を考慮することで精度向上が可能であると考えられる．今後の課題として，実際の検査デー\\n'\n",
      "                    'タを用いた性能評価や，第 2 底面エコーB2 を考慮し\\n'\n",
      "                    'た手法が挙げられる．参考文献[1]  JIS  Z  2355-1:2016.  非破壊試験－超音波厚さ測定－第 '\n",
      "                    '1 部：測定方法[2]  石田仁志,  “ステンレス鋳鋼超音波探傷試験\\n'\n",
      "                    'シ ス テ ム の 開 発 と 実 機 適 用 , ”   INSS \\n'\n",
      "                    'JOURNAL, Vol.25, pp. 209-213, 2018[3]  K.  He  et  al.  '\n",
      "                    '“Deep Residual Learning for \\n'\n",
      "                    'Image  Recognition,”  Proc.  CVPR,  pp. \\n'\n",
      "                    '770-778, 2016.'},\n",
      "  'date': '2000-01-01 00:00:00',\n",
      "  'paper_title': '超音波厚さ測定の効率化のためのエコー検出手法',\n",
      "  'pdf_name': 'IS1-03'},\n",
      " {'_id': ObjectId('5eb6b97560e378f730f5b61d'),\n",
      "  'conf_name': 'SSII2019',\n",
      "  'content': {'序論': '放牧における牛の監視作業は，労働の負担が大きい\\n'\n",
      "                    '一方で，個体の状態を管理し病気や怪我の牛を早期発見\\n'\n",
      "                    'するために重要である．本稿ではドローンを用いて撮影\\n'\n",
      "                    'した空撮画像から乳牛の模様によって個体識別を行い，\\n'\n",
      "                    'それによる個体ごとの監視を目指す．空撮画像では牛\\n'\n",
      "                    'の向きは不揃いであるが，牛の識別を行う際には，体の\\n'\n",
      "                    '向きが揃っていた方が識別精度がよいと予測される．こ\\n'\n",
      "                    'のため，畳込みニューラルネットワーク (CNN) である\\n'\n",
      "                    'YOLOv3 を用いてまず牛の体と頭部を検出し，画像に\\n'\n",
      "                    'おける牛の体軸整合を行う．その後 Siamese Network\\n'\n",
      "                    'を用いて乳牛の模様の類似度による個体識別を行う．従\\n'\n",
      "                    '来手法であるテンプレートマッチングと比較し，提案\\n'\n",
      "                    '手法の有効性を確認した．1 はじめに畜産農家の作業時間の中で，大きな割合を占めるの\\n'\n",
      "                    'は監視作業である．その中でも病気や怪我をした牛の\\n'\n",
      "                    '早期発見が重要である．近年，一戸あたりの牛飼養頭\\n'\n",
      "                    '数は増加する一方，労働人口は高齢化しており，農家\\n'\n",
      "                    'の管理負担が増している．放牧の監視作業の負担を軽\\n'\n",
      "                    '減するため，長期的なコストが少ない，保守運用が楽\\n'\n",
      "                    'などのメリットを持つコンピュータビジョンを用いた\\n'\n",
      "                    '監視が注目されている．特に，広い範囲にわたる放牧\\n'\n",
      "                    '地をカバーできるドローンによる空撮画像の使用が有\\n'\n",
      "                    '望である．本稿は，ドローンによる空撮画像から乳牛の個体識\\n'\n",
      "                    '別を目指す．これにより，遠隔地から牛を個体ごとに観\\n'\n",
      "                    '察することが可能となり，農家の支援につながる．空\\n'\n",
      "                    '撮画像では対象物が様々な向きで写されるが，個体識\\n'\n",
      "                    '別のためには，体の向きが揃っている方が精度が良い\\n'\n",
      "                    'ことが期待される．そこで，YOLOv3 [1] を用いた牛の\\n'\n",
      "                    '体と頭部の検出を行い，これに基づいて個体の向きを\\n'\n",
      "                    '揃える体軸整合を行う．また，模様による個体識別に\\n'\n",
      "                    'ついては類似度学習を行う Siamese Network [2] を用い\\n'\n",
      "                    'る．実験では，Siamese Network を用いる場合とテン\\n'\n",
      "                    'プレートマッチングを用いる場合の比較を行い，また\\n'\n",
      "                    'それぞれでは体軸整合の処理の有無による比較も行う．Copyright © SSII 2019. All '\n",
      "                    'Rights Reserved.- IS1-06 -図 1: 手法の概要',\n",
      "              '結論': '本研究では牛の個体ごとの監視のため，ドローンを用\\n'\n",
      "                    'いて撮影した空撮画像から牛の個体識別を行った．牛の\\n'\n",
      "                    '識別精度向上のため，YOLOv3 を用いて牛の位置とそ\\n'\n",
      "                    'の頭部位置を検出し，牛の体軸整合を行った．その後，\\n'\n",
      "                    '牛の模様の情報から Siamese Network による類似度学\\n'\n",
      "                    '習を行い，二枚の画像の牛が同一であるかどうかの判\\n'\n",
      "                    '定を行った．実験により，Siamese Network による個体\\n'\n",
      "                    '識別は従来の Template Matching の手法に比べ，高い\\n'\n",
      "                    '性能が得らることを確認した．また，Siamese Network\\n'\n",
      "                    'を使用する場合，体軸整合を行う方が，回転を行わず\\n'\n",
      "                    'に個体識別を行う場合よりも高い性能が得られた．し\\n'\n",
      "                    'たがって，体軸整合が個体識別に有用であることが確\\n'\n",
      "                    '認できた．'},\n",
      "  'date': '2000-01-01 00:00:00',\n",
      "  'paper_title': 'ドローンによる牧場空撮画像における乳牛の個体識別',\n",
      "  'pdf_name': 'IS1-06'},\n",
      " {'_id': ObjectId('5eb6b9fd60e378f730f5b620'),\n",
      "  'conf_name': 'SSII2019',\n",
      "  'content': {'序論': '畳み込みニューラルネットワーク  (CNN)  を用いた\\n'\n",
      "                    'セマンティックセグメンテーションはコンピュータビジョンにおける基本的な問題の一つである．本稿では，\\n'\n",
      "                    'Encoder-Decoder 構 造 の ネ ッ ト ワ ー ク に 導 入 す る\\n'\n",
      "                    'Class の観点からの Attention 機構を提案する．  Class\\n'\n",
      "                    'の観点からの Attention 機構では，クラス領域を学習\\n'\n",
      "                    'しながら復元された特徴情報に注目する．これにより各クラスのセグメンテーションに重要な特徴情報に焦点を当てるようになるため，より正確に結果を生成す\\n'\n",
      "                    'ることが可能となる．本稿では CamVid データセットを\\n'\n",
      "                    '用いて評価し，従来手法よりも高い精度が得られた．1  はじめに画像内の各画素に対してクラス推定を行うセマン\\n'\n",
      "                    'ティックセグメンテーション[2,  3,  5]は，コンピュータビ\\n'\n",
      "                    'ジョンにおける基本的な問題の一つであり，自動運転や医学など様々な分野に応用され始めている．近年，画像認識問題において非常に高い認識精度を\\n'\n",
      "                    '実現した畳み込みニューラルネットワーク(CNN)[1]に\\n'\n",
      "                    '基づいた Encoder-Decoder 構造を採用した手法が数\\n'\n",
      "                    '多く提案されている．従来のセマンティックセグメンテーション手法では，あるクラスに属する物体の一部を異なるクラスであると誤って識別してしまうことがある．これは画像内に存在する各物体を画素単位で学習していたため，クラス間の区別が上手く出来ていないことが原因であると考えられる．この問題を対処する\\n'\n",
      "                    'ため，Encoder-Decoder 構造のネットワークに導入す\\n'\n",
      "                    'る Class の観点からの Attention 機構を提案する．Class の観点からの Attention '\n",
      "                    '機構は，セグメンテー\\n'\n",
      "                    'ションのために各クラス領域を学習しながら，それらを用いて特徴情報に注目する工程である．これにより重要な特徴情報のみ逐次焦点を当てるようになるた\\n'\n",
      "                    'め，より正確に結果を生成することが可能となる．実験では，車載カメラにより撮影された画像を 11 '\n",
      "                    'クCopyright © SSII 2019. All Rights Reserved.- IS1-09 -堀田  '\n",
      "                    '一弘ラスでラベル付けされた CamVid データセット[4]を使\\n'\n",
      "                    '用し，評価指標には IoU(Intersection over Union) [5]\\n'\n",
      "                    'を用いた．その結果，提案手法は従来のセグメンテ\\n'\n",
      "                    'ーション手法よりも高い精度を得ることができた．本論文は以下のように構成される．2 節では関連\\n'\n",
      "                    '研究，3 節では提案手法について述べる．また，4 節\\n'\n",
      "                    'で評価実験の結果を示し，5 節にまとめを述べる．',\n",
      "              '結論': '本稿では Encoder-Decoder 構造のネットワークに導入する Class の観点からの '\n",
      "                    'Attention 機構を提案した．\\n'\n",
      "                    'Class の観点からの Attention 機構を導入したネットワ\\n'\n",
      "                    'ークは CamVid データセットにおいて，従来手法より\\n'\n",
      "                    'も高い識別精度を達成した．IS1-09\\n'\n",
      "                    'SO1-09第25回画像センシングシンポジウム'},\n",
      "  'date': '2000-01-01 00:00:00',\n",
      "  'paper_title': '',\n",
      "  'pdf_name': 'IS1-09'},\n",
      " {'_id': ObjectId('5eb6b9fd60e378f730f5b622'),\n",
      "  'conf_name': 'SSII2019',\n",
      "  'content': {'序論': '固有画像分解は，入力画像を照明成分と反射率成分\\n'\n",
      "                    'に分解する手法である．反射率成分を正確に推定する\\n'\n",
      "                    'ことは，環境光による外乱を最小限に抑えることに相当\\n'\n",
      "                    'する．画像を照明成分と反射率成分の積として表現する\\n'\n",
      "                    'Retinex 理論を基礎として，近年では畳み込みニューラ\\n'\n",
      "                    'ルネットワーク (CNN) を用いた手法が提案されている．\\n'\n",
      "                    '正解画像を大量に用意することが著しく困難なこの問題\\n'\n",
      "                    '設定において，人工的に合成した画像を用いる従来手法\\n'\n",
      "                    'では，その結果に色ムラやにじみなどの不自然さが見ら\\n'\n",
      "                    'れた．本研究では，Encoder-Decoder 構造の CNN と，\\n'\n",
      "                    'その入力に Generative Adversarial Networks(GAN) に\\n'\n",
      "                    'よって生成した画像群を用いる枠組みを提案する．CNN\\n'\n",
      "                    '構造の最適化に加え，実世界における環境光のデータ\\n'\n",
      "                    '分布に基づき生成した画像群を入力とすることで，低\\n'\n",
      "                    '光量のテスト画像に対して，従来手法と比較してより自\\n'\n",
      "                    '然で視認性の高い反射率画像を得た．セマンティック・\\n'\n",
      "                    'セグメンテーションによる評価実験では，本手法で得\\n'\n",
      "                    'た反射率画像を用いて学習およびテストを行うことで，\\n'\n",
      "                    '環境光が画素値に与える悪影響が緩和され，識別精度\\n'\n",
      "                    'が向上することを確認した．2 はじめに一般物体認識やセマンティック・セグメンテーション\\n'\n",
      "                    'をはじめとするコンピュータビジョンのタスク全般に\\n'\n",
      "                    'おいて，その精度に悪影響をもたらす外乱として，シー\\n'\n",
      "                    'ンの照明状況が挙げられる．画像を通して観測される\\n'\n",
      "                    '物体の色情報は照明状況に強く依存し，環境変化が激\\n'\n",
      "                    'しい実空間において問題となる．特に，環境光の乏し\\n'\n",
      "                    'い暗部あるいは逆光条件下において物体の色情報は大\\n'\n",
      "                    'きく失われ，有効な画像特徴を抽出するにあたって障\\n'\n",
      "                    '害となる．そのため，画像の視認性を改善させる画像\\n'\n",
      "                    '処理技術への需要は高く，古くから活発に研究されて\\n'\n",
      "                    'いる分野である．\\n'\n",
      "                    '\\u3000一方，人間の視覚にはこのような外乱に適応する能\\n'\n",
      "                    '力が備わっており，異なる照明状況下においても同様に\\n'\n",
      "                    '物体を知覚できることが知られている．例えば，赤い\\n'\n",
      "                    'リンゴを明るい部屋と暗い部屋で撮影した場合を考えCopyright © SSII 2019. All Rights '\n",
      "                    'Reserved.- IS1-11 -(a) 入力画像 S(b) 反射率画像 R(c) 照明画像 I図 1: '\n",
      "                    '固有画像分解ると，リンゴの領域の RGB 画素値は部屋の明暗によっ\\n'\n",
      "                    'て大きく異なるはずだが，人間はどちらの環境におい\\n'\n",
      "                    'てもリンゴは同じく赤いものであるとして知覚できる．\\n'\n",
      "                    'この視覚特性は色恒常性と呼ばれ，物体の色や明るさ\\n'\n",
      "                    'をその周辺との分光反射率の比のみで知覚することで，\\n'\n",
      "                    '変化する照明状況に対して見え方が自動的に補正され\\n'\n",
      "                    'るという性質をあらわしている．\\n'\n",
      "                    '\\u3000この人間が持つ色恒常性をコンピュータビジョンに\\n'\n",
      "                    '応用する手法として，固有画像分解 (図 1) が挙げられ\\n'\n",
      "                    'る．シーンで撮影された入力画像 (a) を，物体の反射\\n'\n",
      "                    '率を RGB 値に変換した反射率画像 (b) と，物体の色成\\n'\n",
      "                    '分を一切持たず環境光のみに依存する照明画像 (c) に分\\n'\n",
      "                    '解する．反射率画像を正確に推定することは，環境光\\n'\n",
      "                    'による外乱の影響を最小限に抑えることと同義であり，\\n'\n",
      "                    'コンピュータビジョンにおいては，固有画像分解によっ\\n'\n",
      "                    'て人間の色恒常性のようなロバスト性を実現できると\\n'\n",
      "                    '考えられる．\\n'\n",
      "                    '\\u3000多くの有用な固有画像分解手法は Retinex 理論 [1] [2]\\n'\n",
      "                    'を基礎にしている．Retinex は Retina (網膜) と Cor-\\n'\n",
      "                    'tex(大脳皮質) を合わせた造語で，人間の脳による光や\\n'\n",
      "                    '色の知覚をモデル化する．Retinex 理論では，入力画像\\n'\n",
      "                    'S と反射率画像 R，照明画像 I の関係を次の式であら\\n'\n",
      "                    'わす．S = R・I(1)式 (1) は 2 つの未知変数を持つ不良設定問題であるた\\n'\n",
      "                    'め，1 枚の入力画像から Retinex 理論による固有画像分\\n'\n",
      "                    '解を行うためには，何らかの仮定を設ける必要がある．\\n'\n",
      "                    '実用上は，照明成分の勾配に滑らかさを仮定して，入\\n'\n",
      "                    '力画像に平滑化フィルタを畳み込んだ結果を照明画像IS1-11\\n'\n",
      "                    'SO1-11第25回画像センシングシンポジウムI として，そこから反射率画像 R を順次推定する場合\\n'\n",
      "                    'が多い．このような手法はヒューリスティックではある\\n'\n",
      "                    'が，仮定に沿った照明状況のシーンでは優れた結果が\\n'\n",
      "                    '得られることが知られている．しかし，逆光などの極\\n'\n",
      "                    '端な低光量環境や，偏った環境光が照射している状況\\n'\n",
      "                    'においては，性能の限界があるだけでなく，照明状況\\n'\n",
      "                    'に合わせたパラメータの調整が必要であるという点で\\n'\n",
      "                    '問題がある．\\n'\n",
      "                    '\\u3000本研究では，固有画像分解を Retinex 理論および Con-\\n'\n",
      "                    'volutional Neural Networks(CNN) を用いてデータ・ド\\n'\n",
      "                    'リブンに行うことで，高品質な反射率画像と照明画像を\\n'\n",
      "                    '取得するだけではなく，従来手法では対応できなかった\\n'\n",
      "                    '複雑な照明状況の画像に対しても視認性を改善させる\\n'\n",
      "                    '手法を提案する．反射率画像と照明画像の正解データを\\n'\n",
      "                    '用意することは著しく困難であるため，異なる照明状況\\n'\n",
      "                    'の画像ペアを入力とし，Retinex 理論に基づく損失関数\\n'\n",
      "                    'を設定した Encoder-Decoder 構造の CNN を用いるこ\\n'\n",
      "                    'とで，正解データ無しで反射率画像と照明画像に分解す\\n'\n",
      "                    'る．異なる照明状況の画像ペアを大量に用意することも\\n'\n",
      "                    'また困難であることから，比較的少数の低光量画像を用\\n'\n",
      "                    'いて学習した Generative Adversarial Networks(GAN)\\n'\n",
      "                    'によって実世界における環境光のデータ分布に基づき\\n'\n",
      "                    '生成した画像ペアを入力として，CNN を学習する．3 関連手法低光量画像の視認性改善に関する研究はこれまで数\\n'\n",
      "                    '多く行われてきた．以下では関連手法を大別して，画素\\n'\n",
      "                    '値・ヒストグラムベースの手法，Retinex 理論ベースの\\n'\n",
      "                    '手法，Deep Learning ベースの手法に分けて説明する．\\n'\n",
      "                    '3.1 画素値・ヒストグラムベースの手法Histogram Equalization(HE) [3] '\n",
      "                    'は，画像全体の輝\\n'\n",
      "                    '度ヒストグラムの偏りを減らすことで視認性を改善さ\\n'\n",
      "                    'せる手法で，実用的に広く利用されている．Contrast\\n'\n",
      "                    'Limited Adaptive Histgram Equalization(CLAHE) [4]\\n'\n",
      "                    'は，画像をブロックと呼ばれる局所領域で分割し，ブ\\n'\n",
      "                    'ロックごとにヒストグラム平坦化の処理をすることで，\\n'\n",
      "                    'HE で問題となっていた過剰なコントラスト増幅を改善\\n'\n",
      "                    'した．\\n'\n",
      "                    '\\u3000 He らの Dark Channel Prior [5] はヘイズ除去 (De-\\n'\n",
      "                    'haze) のアルゴリズムであるが，自然画像にみられる特\\n'\n",
      "                    '性を上手く利用した視認性改善手法であるという点で\\n'\n",
      "                    '重要である．自然画像のうちヘイズを含んでいない通\\n'\n",
      "                    '常領域においては，RGB のうち 1 つのチャンネルに著\\n'\n",
      "                    'しく小さい値が入っている，という特性を利用し，パッ\\n'\n",
      "                    'チ単位で RGB 成分の最小値を集めることでヘイズを推\\n'\n",
      "                    '定した．ヘイズは太陽光が大気中のもやに拡散される\\n'\n",
      "                    'ことで発生するため，Dehaze と照明成分および反射率\\n'\n",
      "                    '成分を推定することの間には密接な関係があると考えCopyright © SSII 2019. All Rights '\n",
      "                    'Reserved.- IS1-11 -られる．実際，Dong らは [6] の研究で，階調反転した\\n'\n",
      "                    '低光量画像に He の Dehaze 処理をかけ，その結果を再\\n'\n",
      "                    'び階調反転するというアプローチが視認性改善に有効\\n'\n",
      "                    'であることを示した．この Dehaze 処理と低光量画像の\\n'\n",
      "                    '視認性改善の関係については [7] で詳細に議論されてお\\n'\n",
      "                    'り，両者は次のようなシンプルな線形関係にあること\\n'\n",
      "                    'が証明された．Dehaze(I) = 1 (cid:0) Retinex(1 (cid:0) '\n",
      "                    'I)(2)この式の意味するところは，Retinex 処理の対象である\\n'\n",
      "                    '低光量画像には，ヘイズと同様に照明成分の情報が含\\n'\n",
      "                    'まれているということである．本研究でもこの関係性\\n'\n",
      "                    'を利用し，低光量画像に着目して固有画像分解を行う．3.2 Retinex 理論ベースの手法Retinex '\n",
      "                    '理論に基づく視認性改善手法のうち代表\\n'\n",
      "                    '的なものとして，Jobson らによって提案された Cen-\\n'\n",
      "                    'ter/Surround Retinex [8] [9] が挙げられる．照明成分\\n'\n",
      "                    'を推定する際に，入力画像に畳み込む平滑化フィルタと\\n'\n",
      "                    'してガウシアンフィルタを用いるモデルは Single-Scale\\n'\n",
      "                    'Retinex(SSR)，複数のカーネルサイズのガウシアンフィ\\n'\n",
      "                    'ルタを用いるモデルは Multi-Scale Retinex(MSR) と呼\\n'\n",
      "                    'ばれる．[10] は Retinex ベースのアルゴリズムに明る\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    'さを調整するフィルタを導入することでより自然な結\\n'\n",
      "                    '果を得た．[11] は照明画像をシグモイド関数と CLAHE\\n'\n",
      "                    'で複数に分解し，それぞれをコントラスト強調した後\\n'\n",
      "                    'に再合成した．Retinex 理論の式 (1) では右辺が 2 つの\\n'\n",
      "                    '画像の積の形になっており取り扱いが難しいため，実用\\n'\n",
      "                    '上は両辺の対数をとって画像の線形和で扱うことが多\\n'\n",
      "                    'いが，[12] では weighted variational model を提案し，\\n'\n",
      "                    '対数を用いた手法よりも頑健な結果が得られると主張\\n'\n",
      "                    'した．\\n'\n",
      "                    '3.3 Deep Learning ベースの手法近年では，CNN を用いた固有画像分解手法が提案さ\\n'\n",
      "                    'れはじめている．反射率画像および照明画像の Ground\\n'\n",
      "                    'Truth は，緻密な撮影環境設定の上で分光測色計を用い\\n'\n",
      "                    'るなどの手段で得ることができるが，CNN の学習に足\\n'\n",
      "                    'るほどの枚数を用意することはあらゆるコスト面で現実\\n'\n",
      "                    '的ではない．[13] は，人工的に合成した Ground Truth\\n'\n",
      "                    'で CNN を学習した．[14] は AutoEncoder ベースのネッ\\n'\n",
      "                    'トワークで，学習には人工的に輝度を低くした上でノ\\n'\n",
      "                    'イズを加えた画像を用いた．これらの手法は合成画像\\n'\n",
      "                    'に対しては優良な結果が得られているが，複雑な自然\\n'\n",
      "                    '画像に対しての評価は十分に行われていない．\\n'\n",
      "                    '\\u3000自然画像への適応を試みたアプローチとして，異な\\n'\n",
      "                    'る照明状況で撮られた複数枚の同一シーン画像群を入\\n'\n",
      "                    '力として，Retinex 理論に基づく損失関数を設けること\\n'\n",
      "                    'で，反射率画像および照明画像の Ground Truth を用\\n'\n",
      "                    '意することなく固有画像分解を行う手法が提案されてIS1-11\\n'\n",
      "                    'SO1-11第25回画像センシングシンポジウム図 2: 提案手法の全体構成いる．Chen らの '\n",
      "                    'RetinexNet [15] は，通常画像と低光\\n'\n",
      "                    '量画像のペアを CNN の入力とした．そのような画像ペ\\n'\n",
      "                    'アを用意することもまた緻密なカメラセッティングが\\n'\n",
      "                    '必要で手間がかかるため，大半は単純な輝度値操作を\\n'\n",
      "                    '施した画像を低光量画像として学習した．[16] では入\\n'\n",
      "                    '力画像として定点カメラで撮影されたタイムラプス画\\n'\n",
      "                    '像群を用いており，時間経過に伴う照明変動から照明\\n'\n",
      "                    '成分の情報を得た．これらの複数枚を入力とする手法\\n'\n",
      "                    'では，照明状況が異なる入力画像群において反射率画\\n'\n",
      "                    '像は不変であるという制約を損失関数に導入している．\\n'\n",
      "                    '機械学習の枠組みで大量のデータから分布を学習する\\n'\n",
      "                    'アプローチは，従来のヒューリスティックな仮定を置い\\n'\n",
      "                    'た手法と比べて，幅広いシーンにおいて優れた結果を\\n'\n",
      "                    '示している．しかし，単純な画像処理で輝度を下げた\\n'\n",
      "                    '画像や定点カメラによる画像は，過学習や，自然画像\\n'\n",
      "                    'に対する結果に色ムラやにじみが見られるなどの課題\\n'\n",
      "                    'があり，CNN の構造と併せて，照明状況が異なる画像\\n'\n",
      "                    '群をいかに高品質かつ大量に用意するかが焦点となっ\\n'\n",
      "                    'ている．',\n",
      "              '結論': '入力画像を反射率成分と照明成分に分解する固有画\\n'\n",
      "                    '像分解のタスクに関して，CNN を用いたデータ・ドリ\\n'\n",
      "                    'ブンなアプローチを提案した．Retinex 理論を損失関数\\n'\n",
      "                    'に組み込み，Encoder-Decoder 構造の CNN によって大\\n'\n",
      "                    '域的および局所的特徴を捉えることを試みた．低光量\\n'\n",
      "                    '画像に含まれる照明情報を活用するため，低光量画像\\n'\n",
      "                    'と通常画像から成るデータセットで学習した GAN に\\n'\n",
      "                    'より，実空間の照明状況のデータ分布に基づき生成さ\\n'\n",
      "                    'れた低光量画像を CNN の学習に用いることで，従来手\\n'\n",
      "                    '法と比較して自然な分解結果を得た．自然画像に関す\\n'\n",
      "                    'る反射率画像および照明画像の Groud Truth を用意す\\n'\n",
      "                    'ることは困難であったため，その代替案として，セマ\\n'\n",
      "                    'ンティック・セグメンテーションによる評価実験を行っ\\n'\n",
      "                    'た．CamVid データセットの画像を提案手法で固有画\\n'\n",
      "                    '像分解し，その反射率画像を用いて SegNet の学習およ\\n'\n",
      "                    'びテストを行った．提案手法による反射率画像では，環\\n'\n",
      "                    '境光が画素値に与える悪影響によって生じる逆光領域\\n'\n",
      "                    'や影領域が緩和され，セグメンテーションの識別精度\\n'\n",
      "                    'が向上した．本研究の固有画像分解手法によって，視\\n'\n",
      "                    '認性の改善されたより高品質な反射率画像が得られる\\n'\n",
      "                    'ことを確認した．\\n'\n",
      "                    '\\u3000今後の課題としては，より大規模な学習によるロバス\\n'\n",
      "                    'ト性の向上が挙げられる．また，今回は GAN を CNN\\n'\n",
      "                    'への入力画像を生成するためのみに使用したが，全体\\n'\n",
      "                    'の構成は end-to-end であることが実用上好ましいため，\\n'\n",
      "                    'GAN の枠組み内で完結して固有画像分解を行う手法も\\n'\n",
      "                    '考えられる．'},\n",
      "  'date': '2000-01-01 00:00:00',\n",
      "  'paper_title': 'CNN を用いた固有画像分解による低光量画像の視認性改善',\n",
      "  'pdf_name': 'IS1-11'},\n",
      " {'_id': ObjectId('5eb6ba8a60e378f730f5b624'),\n",
      "  'conf_name': 'SSII2019',\n",
      "  'content': {'序論': 'カメラと 3D LiDAR のセンサフュージョンにより，セ\\n'\n",
      "                    'ンサ単体の歩行者認識より性能が高く，かつ車載マイコンによる実用的なシステムを可能とすべく，本開\\n'\n",
      "                    '発を行った[1]．一般的に，歩行者は奥行き方向，横向き方向それぞれ類似した動作であるが，我々はより複雑な動作\\n'\n",
      "                    '(荷物を運ぶ，しゃがみなど)の場合の人物も認識でき\\n'\n",
      "                    'るシステムの開発を目指している。このため，識別処理により性能が高いものが必要になり，かつ処理時間の課題が発生することが容易に想定できたため，識別処理にディープラーニングを選定し，さらにこれ\\n'\n",
      "                    'を  FPGA へ搭載することで，これらの課題を解決した．',\n",
      "              '結論': ''},\n",
      "  'date': '2000-01-01 00:00:00',\n",
      "  'paper_title': '車載向け FPGA 搭載に向けたコンパクトな Residual Network による人物検知',\n",
      "  'pdf_name': 'IS1-13'},\n",
      " {'_id': ObjectId('5eb6ba8a60e378f730f5b627'),\n",
      "  'conf_name': 'SSII2019',\n",
      "  'content': {'序論': 'Toward safe and secure society, almost every build-ing or '\n",
      "                    'facility has installed surveillance cameras andhires many '\n",
      "                    'security guards to monitor the camerastreams and ﬁnd '\n",
      "                    'persons of interest such as crimi-nals, lost children, or '\n",
      "                    'VIPs. Human-based monitoring\\n'\n",
      "                    'is not practical in terms of cost because hundreds '\n",
      "                    'ofguards could be needed in the near future to '\n",
      "                    'monitorthousands of camera streams without failure. '\n",
      "                    'Inter-est is now shifting to systems that are based on '\n",
      "                    'deeplearning.When we apply deep learning techniques to '\n",
      "                    'ﬁnd per-sons of interest, we often perform any one or all '\n",
      "                    'thefollowing three tasks. The ﬁrst one is to detect '\n",
      "                    'bodyCopyright © SSII 2019. All Rights Reserved.- IS1-16 '\n",
      "                    '-shots from camera streams. The second is to deter-mine '\n",
      "                    'whether two given body shots belong to the sameidentity, '\n",
      "                    'i.e., person re-identiﬁcation (reid). We cancompare a '\n",
      "                    'body shot of a criminal with those detectedfrom camera '\n",
      "                    'streams, then determine the current loca-tion of the '\n",
      "                    'criminal. The third task is attribute recog-nition in '\n",
      "                    'which we estimate what attributes a givenbody shot has. A '\n",
      "                    'lost ﬁve-year-old girl wearing a redshirt and blue skirt '\n",
      "                    'carrying a backpack can be easilyfound if attribute '\n",
      "                    'recognition can suggest body shotsthat have the '\n",
      "                    'attributes “ﬁve years old”, “female”,“red shirt”, and '\n",
      "                    '“backpack”.To handle detection tasks, generic object '\n",
      "                    'recogni-tion models such as You Look Only Once (YOLO)[8, '\n",
      "                    '9] and Single Shot MultiBox Detector (SSD) [7]are '\n",
      "                    'suitable.Re-identiﬁcation is done with models that '\n",
      "                    'areroughly divided into two groups:identiﬁcation '\n",
      "                    'andveriﬁcation.Identiﬁcation models train '\n",
      "                    'themselvesthough a supervised classiﬁcation task in which '\n",
      "                    'weestimate to which identity a body shot belongs, '\n",
      "                    'thentheir intermediate outputs can be used as featuremaps '\n",
      "                    'for reid. This group includes ResNet50 [2],Attention '\n",
      "                    'Framework of Person Body (AFPB) [12],and Harmonious '\n",
      "                    'Attention Convolutional Neural Net-work (HA-CNN) [5]. '\n",
      "                    'Veriﬁcation models train them-selves through a supervised '\n",
      "                    'binary classiﬁcation taskin which we estimate whether a '\n",
      "                    'pair of body shots be-long to the same identity. This '\n",
      "                    'group includes, e.g.,Ahmednet [1].Attribute recognition '\n",
      "                    'is sometimes handled alongwith another task to improve '\n",
      "                    'accuracy;for exam-ples, YOLO detects body shots and '\n",
      "                    'estimates their at-tributes at the same time and '\n",
      "                    'Attribute-Person Recog-nition (APR) [6] adopts multi-task '\n",
      "                    'learning with reid.In contrast, Multi-Label Convolutional '\n",
      "                    'Neural Net-works (ML-CNN) [11] only handles attribute '\n",
      "                    'recogni-\\n'\n",
      "                    'tion task but is characterized by its way of processing\\n'\n",
      "                    'body shots; that is, ML-CNN divides a body shot '\n",
      "                    'intopieces scheduled beforehand to independently '\n",
      "                    'evalu-IS1-16\\n'\n",
      "                    'SO1-16第25回画像センシングシンポジウムate each body part to improve '\n",
      "                    'accuracy, especially forthose attributes that are related '\n",
      "                    'to a particular bodypart such as “shirt color” and '\n",
      "                    '“carrying backpack”rather than “age” or “gender”.However, '\n",
      "                    'the three tasks become diﬃcult when theentire body of the '\n",
      "                    'person is not in the body shot. Sucha scenario is '\n",
      "                    'frequent. For instance, when a surveil-\\n'\n",
      "                    'lance camera is mounted on the ceiling and looks downover '\n",
      "                    'an escalator, only the upper portions of peopleappear in '\n",
      "                    'the camera stream.In such cases, detec-tors produce '\n",
      "                    'half-body shots. As a result, accuracytends to decrease '\n",
      "                    'in the consequent tasks of reid andattribute recognition. '\n",
      "                    'The reason performance deteri-orates would be because we '\n",
      "                    'cannot appropriately lookat each part of body. With '\n",
      "                    'current models includ-ing ML-CNN, it is assumed that the '\n",
      "                    'entire body of aperson is provided. So they simply break '\n",
      "                    'body shotsinto pieces in a predetermined manner, where a '\n",
      "                    'piececan be designated for the face in a full-body shot '\n",
      "                    'buthip in a lower-half-body shot can be in the piece. '\n",
      "                    'Al-though attention-based models[5] including HA-CNNare '\n",
      "                    'promising for assimilating body parts, HA-CNNstill cannot '\n",
      "                    'appropriately determine if clothing withstripe patterns '\n",
      "                    'is being worn on the upper, lower, orentire body, as '\n",
      "                    'shown in Fig. 1(a).We aim to maintain reid accuracy even '\n",
      "                    'with half-body shots and attribute recognition accuracy '\n",
      "                    'as wellbecause we believe that we inevitably cannot '\n",
      "                    'produce\\n'\n",
      "                    'a full-body shot in the detection task if only half the\\n'\n",
      "                    'body appears in the camera stream. To the best of '\n",
      "                    'ourknowledge, this paper is the ﬁrst attempt to focus '\n",
      "                    'onhalf-body shots. Although recently proposed '\n",
      "                    'models,including AFPB, take into account occlusion in '\n",
      "                    'bodyshots, an occluded shot for them has a height '\n",
      "                    'equiva-lent to the length from head to foot and is a '\n",
      "                    'full-bodyshot in our deﬁnition. To this end, we focus on '\n",
      "                    'the fol-lowing two problems in this paper. A) Assuming '\n",
      "                    'thata detector can produce lower- or upper-body shots '\n",
      "                    'butwe have full-body shots for persons of interest, we '\n",
      "                    'per-form reid between the person-of-interest and '\n",
      "                    'detectedbody shots. B) Assuming that a detector can '\n",
      "                    'producefull-body shots but we only have upper- or '\n",
      "                    'lower-bodyshots for persons of interest, we perform reid '\n",
      "                    'betweenthem. Note that attribute recognition is omitted '\n",
      "                    'in B)because state-of-the-art attribute recognition '\n",
      "                    'modelsare already accurate with regard to full-body '\n",
      "                    'shots.To address these two problems, we introduce '\n",
      "                    'twoadditional components to HA-CNN: spatial supervi-sion '\n",
      "                    'and multi-task learning. We call this modi-Copyright © '\n",
      "                    'SSII 2019. All Rights Reserved.- IS1-16 -ﬁed model '\n",
      "                    'Spatially-Supervised Harmonious At-\\n'\n",
      "                    'tention Convolutional Neural Network (SS-HA-CNN). We '\n",
      "                    'choose HA-CNN as a baseline because itis a '\n",
      "                    'state-of-the-art reid model as long as body shotsare '\n",
      "                    'full. As shown in Fig. 1(b), SS-HA-CNN can dis-tinguish a '\n",
      "                    'stripe pattern worn on the lower body fromthat worn on '\n",
      "                    'the upper body or on the entire body.Our contributions '\n",
      "                    'are following:\\n'\n",
      "                    '• We formulated reid and attribute recognitionproblems '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    'with half-body shots.• We proposed SS-HA-CNN, which '\n",
      "                    'improves reid\\n'\n",
      "                    'accuracy with half-body shots and enables at-tribute '\n",
      "                    'recognition.2 Problem Formulation\\n'\n",
      "                    'Suppose that there are a training dataset DT, query\\n'\n",
      "                    'dataset DQ, and gallery dataset DG, and that all three\\n'\n",
      "                    'consist of body shots x ∈ {0,··· , 255}c×w×h and the\\n'\n",
      "                    'corresponding identity labels y ∈ {1,··· , I} (I is the\\n'\n",
      "                    'number of identities), attribute labels z ∈ {±1}M (M\\n'\n",
      "                    'is the number of attributes), and appearance labels\\n'\n",
      "                    'a ∈ {1,··· , A} (A is the number of appearances). By\\n'\n",
      "                    'omitting the subscripts of T, Q and G, we can obtain}{the '\n",
      "                    'following equation:D =(x(i), y(i), z(i), '\n",
      "                    'a(i)).(1)iDatasets DQ and DG share every identity label '\n",
      "                    'y,\\n'\n",
      "                    'but such identity label never appears in DT, and '\n",
      "                    'viceversa. Some attribute labels may be related to '\n",
      "                    'par-ticular body parts (e.g.clothing color, hair '\n",
      "                    'style,whether wearing eyeglasses), while others may '\n",
      "                    'corre-spond to the entire body (e.g. gender, age) but '\n",
      "                    'allattributes must be labels of either true or false. '\n",
      "                    'Thejth component of z becomes +1 if x meets the jth\\n'\n",
      "                    'attribute for j ∈ {1,··· , M}; otherwise, −1.\\n'\n",
      "                    'Our purpose is, provided that x(i)\\n'\n",
      "                    'T , y(i)\\n'\n",
      "                    'T , and\\n'\n",
      "                    'a(i)\\n'\n",
      "                    'T are all available for training, to determine which\\n'\n",
      "                    '′\\n'\n",
      "                    'pair of x(i)\\n'\n",
      "                    'G (regard as de-\\n'\n",
      "                    'tected shot) belong to the same identities and whichT , '\n",
      "                    'z(i)Q (person of interest) and x(i)ones do not (reid), as '\n",
      "                    'well as estimating attribute la-\\n'\n",
      "                    'bels z(i)G (attribute recognition).With traditional reid '\n",
      "                    'models, a(i) represents only\\n'\n",
      "                    'full body (A = 1). On the contrary, we consider full-, '\n",
      "                    'upper- and lower-body shots (A = 3), and simplynotate a '\n",
      "                    'as 0, 1 and 2, respectively.In this paper, we focus on '\n",
      "                    'the following two speciﬁcproblems.IS1-16\\n'\n",
      "                    'SO1-16第25回画像センシングシンポジウムA) Full-body Query but Half-body '\n",
      "                    'GalleryIn this problem, body shots in DT appear as '\n",
      "                    'full-,\\n'\n",
      "                    'upper-, or lower-body (aT ∈ {0, 1, 2}) but those in DQ\\n'\n",
      "                    'must be full-body (aQ = 0) and those in DG must be\\n'\n",
      "                    'upper- or lower-body (aG ∈ {1, 2}). For this problem,we '\n",
      "                    'perform reid as well as attribute recognition.B) '\n",
      "                    'Half-body Query but Full-body GalleryIt is assumed with '\n",
      "                    'this problem that aT ∈ {0, 1, 2}\\n'\n",
      "                    'as well; however, aQ ∈ {1, 2} and aG = 1. For '\n",
      "                    'thisproblem, we perform reid only.3 Baseline and '\n",
      "                    'DrawbacksHA-CNN has one global branch and R local\\n'\n",
      "                    'branches, where R ∈ Z + is a hyper parameter and\\n'\n",
      "                    'R = 2 is used in this paper. The global branch learnsthe '\n",
      "                    'global level feature from the entire body while '\n",
      "                    'localbranches focus on R particular local regions (e.g. '\n",
      "                    'face,shirt, trousers) of the body shot. HA-CNN '\n",
      "                    'maintainsaccuracy even when the body shots are misaligned '\n",
      "                    'dueto clutter or occlusion thanks to the harmony amongthe '\n",
      "                    'two diﬀerent types of attention branches.HA-CNN is an '\n",
      "                    'identiﬁcation model, which receivesa body shot x to '\n",
      "                    'provide the estimated identity labelˆ\\n'\n",
      "                    'y. Before calculating ˆy, global branch and local branch\\n'\n",
      "                    'extract global feature XG ∈ Rd and local feature XL ∈\\n'\n",
      "                    'Rd, respectively, through the following equations:(xG, '\n",
      "                    'ˆθr) = f (x; W0),(2)xr = f (S(x; ˆθr); Wr),\\n'\n",
      "                    'XG = f (xG; WG),\\n'\n",
      "                    'XL = f ({xr}r; WL),(3)(4)(5)\\n'\n",
      "                    'for r ∈ {1,··· , R}, where f (·; W) denotes a neural\\n'\n",
      "                    'network (usually MLP or CNN) with a weight pa-\\n'\n",
      "                    'rameter W, and S(x, ˆθ) is a spatial transformer[3],\\n'\n",
      "                    'which ﬁrst crops a rectangular region deﬁned by ˆθ ∈\\n'\n",
      "                    '[−1, +1]4 that contains the four vertex coordinates\\n'\n",
      "                    '(left, right, top, bottom) in x then resizes the '\n",
      "                    'croppedregion to the required width and height in a '\n",
      "                    'bilinear\\n'\n",
      "                    'manner. The neural network with W0 and those with\\n'\n",
      "                    'Wr are repeatedly applied L times (L ∈ Z + is a hyper\\n'\n",
      "                    'parameter and L = 3 is used in this paper) but omit-\\n'\n",
      "                    'ted in the equations for simplicity. When we denote\\n'\n",
      "                    'ˆθ = ( ˆξ0, ˆξ1, ˆη0, ˆη1), they must meet −1 ≤ ˆξ0 ≤ ˆξ1 '\n",
      "                    '≤ 1\\n'\n",
      "                    'and −1 ≤ ˆη0 ≤ ˆη1 ≤ 1. When ˆθ = (−1, 1,−1, 1),Copyright '\n",
      "                    '© SSII 2019. All Rights Reserved.- IS1-16 -the spatial '\n",
      "                    'transformer S(x; ˆθ) does not crop any re-\\n'\n",
      "                    'gion but resizes x to the size that f (·; Wr) accepts.\\n'\n",
      "                    'Finally, HA-CNN generates the estimation of '\n",
      "                    'identitylabel:y = f (XG, XL; WC).\\n'\n",
      "                    'ˆ(6)When updating the weight parameters, HA-CNNevaluates '\n",
      "                    'gradients at W for the following function:L = gENT(ˆy, '\n",
      "                    'y),(7)where gENT represents cross entropy.For reid, '\n",
      "                    'features extracted with HA-CNN are suit-able. We consider '\n",
      "                    'the Euclidean distance between thequery and gallery '\n",
      "                    'features:d(XQ, XG) = dG(XGQ, XGG) + dL(XLQ, '\n",
      "                    'XLG),(8)whereG) = ∥XG\\n'\n",
      "                    'G) = ∥XL∥2\\n'\n",
      "                    '2,\\n'\n",
      "                    '∥2\\n'\n",
      "                    '2,dG(XGQ, XG\\n'\n",
      "                    'G\\n'\n",
      "                    'Q, XL\\n'\n",
      "                    'GQ, XG\\n'\n",
      "                    'Q, XL(9)dL(XL(10)and we sometimes concatenate the two '\n",
      "                    'features XG\\n'\n",
      "                    'and XL and denote this as X. The distance d will be\\n'\n",
      "                    'small when xQ and xG belong to the same identity\\n'\n",
      "                    'but large otherwise. By sorting xG in the ascending\\n'\n",
      "                    'order of d from xQ, we obtain the most similar gallery\\n'\n",
      "                    'body shots to the query one.However, HA-CNN loses '\n",
      "                    'accuracy in reid and at-\\n'\n",
      "                    'tribute recognition when a ̸= 0 or half the body ap-\\n'\n",
      "                    'pears in the shot. Originally, every spatial transformer\\n'\n",
      "                    'S(·; ˆθr) for r ∈ {1,··· , R} is designed to be respon-\\n'\n",
      "                    'sible for every particular distinguishable part of '\n",
      "                    'bodythat is not deﬁned by users but determined in the '\n",
      "                    'prin-cipal of reinforcement learning. Actually, any '\n",
      "                    'eﬀectivepolicy cannot be acquired and so each spatial '\n",
      "                    'trans-former crops inconsistent parts of the body '\n",
      "                    'between\\n'\n",
      "                    'x with a = 0 and x with a ̸= 0. In Figure 1(a), red\\n'\n",
      "                    'boxes (spatial transformers of r = 1) crop the shortsin '\n",
      "                    'the query shot but they crop the upper bodies inthe '\n",
      "                    'gallery shots, suggesting that feature extraction isno '\n",
      "                    'longer adequate, especially in local branches.In '\n",
      "                    'addition, HA-CNN is a state-of-the-art reidmodel but '\n",
      "                    'cannot be applied to attribute recognitionas is.4 '\n",
      "                    'Proposed ModelWe propose SS-HA-CNN to address the two '\n",
      "                    'issuesmentioned above. We introduce two components ofthis '\n",
      "                    'model: spatial supervision (Section 4.1) andmulti-task '\n",
      "                    'learning (Section 4.2), and modify theloss function '\n",
      "                    '(Section 4.3).IS1-16\\n'\n",
      "                    'SO1-16第25回画像センシングシンポジウム4.1 Spatial SupervisionSpatial '\n",
      "                    'supervision aims to escape from awkward re-inforcement '\n",
      "                    'learning and explicitly assigns each bodypart to each '\n",
      "                    'local branch. In other words, we deﬁneground truth vertex '\n",
      "                    'coordinates of the region for eachspatial transformer to '\n",
      "                    'crop. We argue that each localbranch can share the '\n",
      "                    'consistent body part among body\\n'\n",
      "                    'shots {x} even if they have diﬀerent appearances {a}\\n'\n",
      "                    'as long as the body shots capture the body part.To this '\n",
      "                    'end, we design the ground truth θ of givena and denote it '\n",
      "                    'as θ(a). We found that θ(a) can beimplemented in a very '\n",
      "                    'instinctive fashion, as Figure2 illustrates. When a '\n",
      "                    'person appears as a full-body\\n'\n",
      "                    'shot (a = 0), we vertically spread R supervisory '\n",
      "                    'rect-angles over the body shot. When the person appearsas '\n",
      "                    'an upper-body shot (a = 1), we spread the ﬁrst '\n",
      "                    'R/2rectangles over the body shot but place the other '\n",
      "                    'R/2rectangles at the bottom with heights of zero. Whenthe '\n",
      "                    'person appears as a lower-body shot (a = 2), wespread the '\n",
      "                    'last R/2 but place the other R/2 at the topwithout '\n",
      "                    'heights. We can simply formulate this idea:0(a) = −1,\\n'\n",
      "                    '\\uf8f1\\n'\n",
      "                    'ξr\\n'\n",
      "                    '\\uf8f4\\n'\n",
      "                    '\\uf8f4\\n'\n",
      "                    '\\uf8f4\\n'\n",
      "                    '\\uf8f4\\n'\n",
      "                    'ξr\\n'\n",
      "                    '1(a) = +1,\\n'\n",
      "                    '\\uf8f4\\n'\n",
      "                    '\\uf8f4\\n'\n",
      "                    '\\uf8f2\\n'\n",
      "                    '\\uf8f4\\n'\n",
      "                    '\\uf8f4\\n'\n",
      "                    '\\uf8f4\\n'\n",
      "                    '\\uf8f4\\n'\n",
      "                    '\\uf8f4\\n'\n",
      "                    '\\uf8f4\\n'\n",
      "                    '\\uf8f3\\n'\n",
      "                    '\\uf8f1\\n'\n",
      "                    '\\uf8f4\\n'\n",
      "                    '\\uf8f4\\n'\n",
      "                    '\\uf8f4\\n'\n",
      "                    '\\uf8f4\\n'\n",
      "                    '\\uf8f4\\n'\n",
      "                    '\\uf8f4\\n'\n",
      "                    '\\uf8f2\\n'\n",
      "                    '\\uf8f4\\n'\n",
      "                    '\\uf8f4\\n'\n",
      "                    '\\uf8f4\\n'\n",
      "                    '\\uf8f4\\n'\n",
      "                    '\\uf8f4\\n'\n",
      "                    '\\uf8f4\\n'\n",
      "                    '\\uf8f3−1 + 2(r−1)\\n'\n",
      "                    '−1 + 4(r−1)(a = 0)(a = 1, r ≤ R\\n'\n",
      "                    '2 )\\n'\n",
      "                    '(a = 1, r > R\\n'\n",
      "                    '2 )\\n'\n",
      "                    '(a = 2, r ≤ R\\n'\n",
      "                    '2 )\\n'\n",
      "                    '(a = 2, r > R\\n'\n",
      "                    '2 ),RRηr\\n'\n",
      "                    '0(a) =+1\\n'\n",
      "                    '−1−1)−1 + 4(r− R\\n'\n",
      "                    '0(a) + 2\\n'\n",
      "                    'ηr\\n'\n",
      "                    'R\\n'\n",
      "                    '0(a) + 4\\n'\n",
      "                    'ηr\\n'\n",
      "                    'R2\\n'\n",
      "                    'R(a = 0)(a = 1, r ≤ R\\n'\n",
      "                    '2 )\\n'\n",
      "                    '(a = 1, r > R\\n'\n",
      "                    '2 )\\n'\n",
      "                    '(a = 2, r ≤ R\\n'\n",
      "                    '2 )\\n'\n",
      "                    'R (a = 2, r > R\\n'\n",
      "                    '2 ),ηr\\n'\n",
      "                    '1(a) =+1\\n'\n",
      "                    '−10(a) + 4\\n'\n",
      "                    'ηr1(a)) for r ∈\\n'\n",
      "                    'where θr(a) = (ξr\\n'\n",
      "                    '{1,··· , R}. Note that we do not use odd numbers\\n'\n",
      "                    'for R, forcing R/2 to be integers.1(a), ηr0(a), ηr0(a), '\n",
      "                    'ξr4.2 Multi-task LearningTo apply SS-HA-CNN to attribute '\n",
      "                    'recognition tasks,we introduce multi-task learning '\n",
      "                    'between the identity\\n'\n",
      "                    'and attribute classiﬁcations, i.e., we replace Eq. '\n",
      "                    '(6)with′\\n'\n",
      "                    '(ˆy, ˆz) = f (XG, XL; WC),(11)where ˆz is the estimation '\n",
      "                    'of z. We argue that multi-\\n'\n",
      "                    'task learning not only enables attribute recognition '\n",
      "                    'butalso improves reid accuracy.Copyright © SSII 2019. All '\n",
      "                    'Rights Reserved.- IS1-16 -図 2\\n'\n",
      "                    '(left:Spatial supervisory signal of full-bodyθ(0)), '\n",
      "                    'upper-body (center:θ(1)) andlower-body (right: θ(2)) '\n",
      "                    'where R = 2: red boxesand green ones are supervisory '\n",
      "                    'signals for spatialtransformers of r = 1 or r = 2, '\n",
      "                    'respectively.4.3 Modiﬁed Loss FunctionWe introduce loss '\n",
      "                    'terms regarding attributes andspatial supervision to '\n",
      "                    'Eq.(7) to deﬁne a new loss∑ rfunction:RL = gENT(ˆy, '\n",
      "                    'y)+gLOG(ˆz, z)+∥ˆθr−θr(a)∥22, (12)=1where gLOG represents '\n",
      "                    'the logistics loss function.5 Experiments5.1 Reid '\n",
      "                    'AccuracyTo conﬁrm that SS-HA-CNN outperforms HA-CNNin '\n",
      "                    'term of reid with half-body shots, we conducted '\n",
      "                    'anexperiment on the Market1501 dataset[10, 6], '\n",
      "                    'whichcontains |DT| = 12936, |DQ| = 3368, |DG| = 13115\\n'\n",
      "                    'and M = 30 attributes 1. As a side note, there are\\n'\n",
      "                    '751 identities (y = 1,··· , 751) contained in DT and\\n'\n",
      "                    'another 750 identities (y = 752,··· , 1501) in DQ and\\n'\n",
      "                    'DG. To obtain half-body shots from the original Mar-\\n'\n",
      "                    'ket1501 that by nature has a size of 64 × 128, we\\n'\n",
      "                    'ﬁrst crop the upper or lower 64 × 64 then resize it\\n'\n",
      "                    'to 64× 128 when we want upper- or lower-body shots,\\n'\n",
      "                    'respectively.We compare the following four versions of '\n",
      "                    'HA-CNN:1) HA-CNN (baseline), 2) HA-CNN with spatial '\n",
      "                    'su-pervision (SS), 3) HA-CNN with multi-task '\n",
      "                    'learning(ML), and 4) SS-HA-CNN (proposed model). We '\n",
      "                    'userank-k accuracies and mean Average Precision (mAP)as '\n",
      "                    'indicators. Both rank-k and mAP take values be-tween 0 '\n",
      "                    'and 1. They become larger with better accu-racies but '\n",
      "                    'smaller with poor ones.For each HA-CNN version, we ﬁrst '\n",
      "                    'initialize theweight parameters randomly and train them '\n",
      "                    'with DT1Market1501 originally has M = 27 attributes '\n",
      "                    'including age,\\n'\n",
      "                    'which takes four values (young, teenager, adult, old). We '\n",
      "                    'con-\\n'\n",
      "                    'sider them as four independent binary attributes (young '\n",
      "                    'or not,\\n'\n",
      "                    'teenager or not, and so on) then obtain M = 30.IS1-16\\n'\n",
      "                    'SO1-16第25回画像センシングシンポジウム表 1 mAP and rank-k (avg.±std. of '\n",
      "                    'ﬁve trials)\\n'\n",
      "                    'at the 300th epoch (best in bold)A) full-body query but '\n",
      "                    'half-body gallery problem\\n'\n",
      "                    'rank-5\\n'\n",
      "                    '.504±.018\\n'\n",
      "                    '.539±.015\\n'\n",
      "                    '.518±.008\\n'\n",
      "                    '.562±.010typemAPrank-1\\n'\n",
      "                    '.261±.015\\n'\n",
      "                    '.280±.017\\n'\n",
      "                    '.273±.010\\n'\n",
      "                    '.316±.005.112±.006\\n'\n",
      "                    '.119±.005\\n'\n",
      "                    '.116±.004\\n'\n",
      "                    '.134±.0051) baseline2) SS3) ML4) propB) half-body query '\n",
      "                    'but full-body gallery problem\\n'\n",
      "                    'rank-5\\n'\n",
      "                    '.529±.007\\n'\n",
      "                    '.556±.005\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    '.518±.009\\n'\n",
      "                    '.566±.003typemAPrank-1\\n'\n",
      "                    '.307±.006\\n'\n",
      "                    '.330±.007\\n'\n",
      "                    '.295±.008\\n'\n",
      "                    '.348±.003.177±.004\\n'\n",
      "                    '.191±.007\\n'\n",
      "                    '.172±.005\\n'\n",
      "                    '.202±.0021) baseline2) SS3) ML4) propthrough three '\n",
      "                    'hundred epochs by using ADAM opti-mizer [4] with a '\n",
      "                    'learning rate of 0.0005, weight decayof 0.0005, and '\n",
      "                    'gradient clipping of 5. Note that we\\n'\n",
      "                    'sample {a(i)\\n'\n",
      "                    'tion for each epoch to augment DT:}i from the following '\n",
      "                    'probability distribu-\\uf8f1\\n'\n",
      "                    '\\uf8f4\\n'\n",
      "                    '\\uf8f2\\n'\n",
      "                    '\\uf8f4\\n'\n",
      "                    '\\uf8f3 0.5T(aT = 0)\\n'\n",
      "                    '0.35 (aT = 1)\\n'\n",
      "                    '0.15 (aT = 2).(13)P (aT) =We then evaluate their '\n",
      "                    'performance with thetrained weight parameters in A) the '\n",
      "                    'full-body querybut half-body gallery problem and B) the '\n",
      "                    'half-bodyquery but full-body gallery problem. For problem '\n",
      "                    'A),\\n'\n",
      "                    'a(i)\\n'\n",
      "                    'Q = 1, 2 with probabilities of 0.7 and 0.3, respec-\\n'\n",
      "                    'tively, and a(i)\\n'\n",
      "                    'Q = 0, and\\n'\n",
      "                    'a(i)\\n'\n",
      "                    'G = 1, 2 with 0.7 and 0.3. This comprises a trial,\\n'\n",
      "                    'which is repeated ﬁve times to obtain more reliableG = 0. '\n",
      "                    'For problem B), a(i)mAP and rank-k accuracies.Table 1 '\n",
      "                    'lists the results. We can see that the pro-posed model '\n",
      "                    'has the best accuracy among the four andthat SS and ML '\n",
      "                    'outperforms baseline in most cases.The results indicate '\n",
      "                    'that both spatial supervision andmulti-task learning are '\n",
      "                    'eﬀective.5.2 Attribute-Recognition AccuracyWe conduct '\n",
      "                    'another experiment on Market1501 toverify that SS-HA-CNN '\n",
      "                    'is eﬀective even with half-body shots in terms of '\n",
      "                    'attribute recognition. We com-pare the accuracy of '\n",
      "                    'SS-HA-CNN for problem A) and\\n'\n",
      "                    'on all-full-body dataset where aT = aQ = aG = 0.The '\n",
      "                    'results are listed in Table 2. We can see that '\n",
      "                    'theaccuracy of SS-HA-CNN is slightly lower with half-body '\n",
      "                    'shots than with full-body shots but we arguethat it has '\n",
      "                    'no problem in practical use.Copyright © SSII 2019. All '\n",
      "                    'Rights Reserved.- IS1-16 -表 2\\n'\n",
      "                    'HA-CNN at the 300th epoch: mean, min, '\n",
      "                    'me-Attributes-recognition accuracy of SS-dian and max '\n",
      "                    'among M = 30 attributes (aver-age of ﬁve trials)appr. '\n",
      "                    'mean min med maxfull.914.734.9351.00half.889.682.9191.006 '\n",
      "                    'DiscussionIn this section, we discuss how spatial '\n",
      "                    'supervisionand multi-task learning improve SS-HA-CNN in '\n",
      "                    'termsof accuracy then argue that SS-HA-CNN performs '\n",
      "                    'aswell as HA-CNN even when all the body shots in '\n",
      "                    'thedatasets have full-body appearances.6.1 Beneﬁts from '\n",
      "                    'Spatial SupervisionFigure 3 illustrates the ﬁve most '\n",
      "                    'similar galleriesto a particular query predicted with '\n",
      "                    'each of the HA-CNN versions accompanied by global feature '\n",
      "                    'distance\\n'\n",
      "                    'dG and local feature distance dL in problem B).We can see '\n",
      "                    'that those versions without spatial su-pervision '\n",
      "                    '(baseline and ML) are not accurate in '\n",
      "                    'spatialtransformation. As a result, they mark notably '\n",
      "                    'small\\n'\n",
      "                    'local distance dL with gallery shots wearing striped\\n'\n",
      "                    'shirts in the upper body or over the entire body. '\n",
      "                    'Thiscould be the reason why baseline and ML '\n",
      "                    'mistakenlydetermine that those gallery shots are similar '\n",
      "                    'to thequery.On the contrary, SS-HA-CNN (prop) overcomes '\n",
      "                    'thedrawback.SS-HA-CNN correctly determines thatthose '\n",
      "                    'gallery shots wearing black shirts and stripedshorts are '\n",
      "                    'similar to the query shot even if the queryshot is a '\n",
      "                    'lower-body shot without the black shirt ap-pearing in.6.2 '\n",
      "                    'Beneﬁts from Multi-task LearningIn Figure 3, the greatest '\n",
      "                    'diﬀerence between SS andprop would be the boundary '\n",
      "                    'between the red box andthe green one in the query shot. '\n",
      "                    'We argue that propcould determine that the query shot is '\n",
      "                    'a lower-bodyshot and extend the green box upward thanks '\n",
      "                    'to thehelp of multi-task learning along with attribute '\n",
      "                    'recog-nition, some of which are related to particular '\n",
      "                    'partsof body. This could be why prop do not pick up '\n",
      "                    'thosegallery shots wearing striped clothing in the '\n",
      "                    'upperbody or over the entire body.However, the '\n",
      "                    'improvement was not as much as wehad expected, as table 1 '\n",
      "                    'shows that the proposedIS1-16\\n'\n",
      "                    'SO1-16第25回画像センシングシンポジウムdG\\n'\n",
      "                    'dL196189203205219116127127130118(a) baselinedG\\n'\n",
      "                    'dL241239252251260226237230246237(b) SSdG\\n'\n",
      "                    'dL245248268271283102120121131122(c) MLdG\\n'\n",
      "                    'dL185185179243228211223237190223(d) prop図 3 Query and ﬁve '\n",
      "                    'most similar galleries pre-\\n'\n",
      "                    'dicted with each of HA-CNN versions with ad-\\n'\n",
      "                    'dition of dG and dL: red and green boxes are\\n'\n",
      "                    'regions that spatial transformers of r = 1, '\n",
      "                    '2cropped.model is superior to SS for both problems A) and '\n",
      "                    'B)but ML underperforms baseline for problem B). Thereason '\n",
      "                    'could be because attribute annotation in Mar-ket1501 is '\n",
      "                    'not in instance level but in identity level.\\n'\n",
      "                    'We mean that some body shots are given positive la-bels '\n",
      "                    'such as red shirt or carrying bag even unless thebody '\n",
      "                    'shot actually includes a shirt or bag due to theCopyright '\n",
      "                    '© SSII 2019. All Rights Reserved.- IS1-16 -図 4gLOG curves '\n",
      "                    'for ML on problem A)表 3 mAP and rank-k on all-full-body '\n",
      "                    'dataset\\n'\n",
      "                    '(avg.±std. of ﬁve trials) at the 300th epoch (best\\n'\n",
      "                    'in bold)typemAPrank-1\\n'\n",
      "                    '.712±.033\\n'\n",
      "                    '.725±.026\\n'\n",
      "                    '.745±.009\\n'\n",
      "                    '.737±.045rank-5\\n'\n",
      "                    '.866±.018\\n'\n",
      "                    '.870±.017\\n'\n",
      "                    '.890±.005\\n'\n",
      "                    '.877±.031.467±.038\\n'\n",
      "                    '.486±.025\\n'\n",
      "                    '.511±.010\\n'\n",
      "                    '.503±.0531) baseline2) SS3) ML\\n'\n",
      "                    '4) propcamera angle or the generating process of '\n",
      "                    'half-bodyshots. Figure 4 shows how the '\n",
      "                    'attribute-recognition\\n'\n",
      "                    'loss or gLOG in Eq. (12) changes among epochs but the\\n'\n",
      "                    'ﬁgure suggests overﬁtting. We believe that '\n",
      "                    'multi-tasklearning could further improve accuracy when '\n",
      "                    'appliedto datasets with instance-level annotation.6.3 On '\n",
      "                    'All Full-body DatasetWe conduct one more experiment on '\n",
      "                    'the Mar-\\n'\n",
      "                    'ket1501, but this time aT = aQ = aG = 0. The\\n'\n",
      "                    'results from the reid perspective are listed in Table3; '\n",
      "                    'multi-task learning signiﬁcantly improves the accu-racy '\n",
      "                    'of SS-HA-CNN and spatial supervision does notsigniﬁcantly '\n",
      "                    'degrade its accuracy.7 ConclusionWe modiﬁed HA-CNN and '\n",
      "                    'proposed SS-HA-CNNfor person re-identiﬁcation and '\n",
      "                    'attribute recognitionin half-body shots. The experiments '\n",
      "                    'showed that SS-HA-CNN outperforms HA-CNN from the '\n",
      "                    'perspectiveof reid in the full-body query but half-body '\n",
      "                    'galleryproblem and half-body query but full-body '\n",
      "                    'galleryproblem as well as almost maintaining '\n",
      "                    'attribute-recognition accuracy compared to when applied '\n",
      "                    'to anall-full-body dataset.Future work includes more '\n",
      "                    'general veriﬁcation andloosening regulation for '\n",
      "                    'appearance. For the former,we have to conﬁrm that '\n",
      "                    'SS-HA-CNN outperforms HA-CNN and others on various '\n",
      "                    'datasets, especially withinstance-level annotation. For '\n",
      "                    'the latter, we will in-\\n'\n",
      "                    'stall new components so that SS-HA-CNN can be ap-IS1-16\\n'\n",
      "                    'SO1-16第25回画像センシングシンポジウムplicable to body shots that '\n",
      "                    'contain full-, upper-, lower-body and other possible new '\n",
      "                    'appearance such as right-body, left-body, face, and '\n",
      "                    'shoulder.参考文献[1] Ejaz Ahmed, Michael Jones, and Tim K. '\n",
      "                    'Marks.An improved deep learning architecture for per-son '\n",
      "                    're-identiﬁcation.In The IEEE Conferenceon Computer Vision '\n",
      "                    'and Pattern Recognition(CVPR), June 2015.[2] Kaiming He, '\n",
      "                    'Xiangyu Zhang, Shaoqing Ren, andJian Sun. Deep residual '\n",
      "                    'learning for image recog-nition.2016 IEEE Conference on '\n",
      "                    'ComputerVision and Pattern Recognition (CVPR), '\n",
      "                    'pages770–778, 2016.[3] Max Jaderberg, Karen Simonyan, '\n",
      "                    'Andrew Zis-serman, and koray kavukcuoglu. Spatial '\n",
      "                    'trans-former networks.In Advances in Neural Infor-mation '\n",
      "                    'Processing Systems 28.[4] Diederik P. Kingma and Jimmy '\n",
      "                    'Ba. Adam:A method for stochastic optimization. '\n",
      "                    'CoRR,abs/1412.6980, 2014.[5] Wei Li, Xiatian Zhu, and '\n",
      "                    'Shaogang Gong.Harmonious attention network for person '\n",
      "                    're-identiﬁcation. CoRR, abs/1802.08122, 2018.[6] Yutian '\n",
      "                    'Lin, Liang Zheng, Zhedong Zheng,Yu Wu, and Yi Yang.\\n'\n",
      "                    'Improving person re-\\n'\n",
      "                    'identiﬁcation by attribute and identity learning.CoRR, '\n",
      "                    'abs/1703.07220, 2017.[7] Wei Liu, Dragomir Anguelov, '\n",
      "                    'Dumitru Erhan,Christian Szegedy, Scott E. Reed, '\n",
      "                    'Cheng-YangFu, and Alexander C. Berg. SSD: single '\n",
      "                    'shotmultibox detector. CoRR, abs/1512.02325, 2015.[8] '\n",
      "                    'Joseph Redmon,Santosh Kumar Divvala,Ross B. Girshick, and '\n",
      "                    'Ali Farhadi. You only lookonce: Uniﬁed, real-time object '\n",
      "                    'detection. CoRR,abs/1506.02640, 2015.[9] Joseph Redmon '\n",
      "                    'and Ali Farhadi. YOLO9000:better, faster, stronger. CoRR, '\n",
      "                    'abs/1612.08242,2016.[10] Liang Zheng, Liyue Shen, Lu '\n",
      "                    'Tian, ShengjinWang, Jingdong Wang, and Qi Tian. '\n",
      "                    'Scalableperson re-identiﬁcation: A benchmark. In '\n",
      "                    'Com-puter Vision, IEEE International Conference '\n",
      "                    'on,2015.[11] Jianqing Zhu, Shengcai Liao, Zhen Lei, '\n",
      "                    'andStan Z. Li. Multi-label convolutional neural net-work '\n",
      "                    'based pedestrian attributeclassiﬁcation. Im-Copyright © '\n",
      "                    'SSII 2019. All Rights Reserved.- IS1-16 -age Vision '\n",
      "                    'Comput., 58(C):224–229, February\\n'\n",
      "                    '2017.[12] Jiaxuan Zhuo, Zeyu Chen, Jianhuang Lai,and '\n",
      "                    'Guangcong Wang. Occluded person re-identiﬁcation. CoRR, '\n",
      "                    'abs/1804.02792, 2018.',\n",
      "              '結論': 'We modiﬁed HA-CNN and proposed SS-HA-CNNfor person '\n",
      "                    're-identiﬁcation and attribute recognitionin half-body '\n",
      "                    'shots. The experiments showed that SS-HA-CNN outperforms '\n",
      "                    'HA-CNN from the perspectiveof reid in the full-body query '\n",
      "                    'but half-body galleryproblem and half-body query but '\n",
      "                    'full-body galleryproblem as well as almost maintaining '\n",
      "                    'attribute-recognition accuracy compared to when applied '\n",
      "                    'to anall-full-body dataset.Future work includes more '\n",
      "                    'general veriﬁcation andloosening regulation for '\n",
      "                    'appearance. For the former,we have to conﬁrm that '\n",
      "                    'SS-HA-CNN outperforms HA-CNN and others on various '\n",
      "                    'datasets, especially withinstance-level annotation. For '\n",
      "                    'the latter, we will in-\\n'\n",
      "                    'stall new components so that SS-HA-CNN can be ap-IS1-16\\n'\n",
      "                    'SO1-16第25回画像センシングシンポジウムplicable to body shots that '\n",
      "                    'contain full-, upper-, lower-body and other possible new '\n",
      "                    'appearance such as right-body, left-body, face, and '\n",
      "                    'shoulder.'},\n",
      "  'date': '2000-01-01 00:00:00',\n",
      "  'paper_title': 'Toward Person Re-identiﬁcation in Half-body Shots',\n",
      "  'pdf_name': 'IS1-16'},\n",
      " {'_id': ObjectId('5eb6bc5c60e378f730f5b628'),\n",
      "  'conf_name': 'SSII2019',\n",
      "  'content': {'序論': '近 年 ， RICOH 社 の THETA[1] や Kodak 社 の\\n'\n",
      "                    '4KVR360[2]などの全天球カメラの登場により，手軽\\n'\n",
      "                    'に広視野動画像を撮影できるようになった．また， \\n'\n",
      "                    'YouTube などのソーシャルネットワーキングサービス\\n'\n",
      "                    'が 360°動画などの広視野動画像に対応し，広視野\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    '動画像コンテンツの需要は高まりつつある．広視野\\n'\n",
      "                    '動画像は，ユーザが自由に視点をコントロールするこ\\n'\n",
      "                    'とが可能であるほか，ヘッドマウントディスプレイなど\\n'\n",
      "                    'を併用することで，あたかもその場にいるような没入\\n'\n",
      "                    '感を得ることが可能である．そのため，観光地のプロ\\n'\n",
      "                    'モーションビデオや賃貸物件の紹介など，場所の臨\\n'\n",
      "                    '場感や雰囲気を伝える必要がある業界のマーケティ\\n'\n",
      "                    'ングツールとしても期待されている.しかし一方で，広視野動画像を撮影するためには\\n'\n",
      "                    '専用の機器 が必要と なる問題 が ある． もし も単眼\\n'\n",
      "                    'RGB カメラで撮影した動画像から広視野動画像コン\\n'\n",
      "                    'テンツを生成することが可能になれば，製作のハード\\n'\n",
      "                    'ルおよびコストを大きく下げることができる．また，単\\n'\n",
      "                    '眼 RGB カメラで撮影された過去の膨大な動画像コン\\n'\n",
      "                    'テンツも広視野動画像化して活用できることになる． \\n'\n",
      "                    'そこで本研究では，Structure  from  Motion  (SfM)\\n'\n",
      "                    'および深層学習を組み合わせ，単眼 RGB カメラで撮\\n'\n",
      "                    '影した動画から，より視野の広い動画を生成する手\\n'\n",
      "                    '法を提案する．単眼 RGB カメラで撮影した動画を入\\n'\n",
      "                    '力として，  SfM により中間的に 3 次元モデルを復元\\n'\n",
      "                    'する．撮影対象となる環境の 3 次元モデルが復元さ\\n'\n",
      "                    'れれば，仮想カメラにより自由な視点・アングル・画角\\n'\n",
      "                    'での 2 次元画像が得られるようになる．ここで広視野\\n'\n",
      "                    '化の際に問題になるのは，観測方向以外の方向を撮\\n'\n",
      "                    '影した空間における空間の解像度が比較的低く，疎\\n'\n",
      "                    'な 3 次元点群として復元され，従って 2 次元画像に投\\n'\n",
      "                    '影した場合に疎になってしまうという点である．この問\\n'\n",
      "                    '題を解決するために，本研究では深層学習に基づく\\n'\n",
      "                    '画像補間処理を加え，さらに訓練データの拡張を行\\n'\n",
      "                    'う．これにより視野の自由度が高く，かつ違和感の少\\n'\n",
      "                    'ない動画像を生成する．今回は交通シーンの動画像\\n'\n",
      "                    'を対象としてその性能を確認した．Copyright © SSII 2019. All Rights '\n",
      "                    'Reserved.- IS1-17 -図 1  提案手法の概要図',\n",
      "              '結論': '本研究では，専用機器を使用せずに撮影された\\n'\n",
      "                    '映像コンテンツから広視野動画を生成することを目 \\n'\n",
      "                    '的として，SfM と深層学習による画像補間を用いた\\n'\n",
      "                    '手法を提案し，実験より単眼 RGB カメラで撮影した\\n'\n",
      "                    '動画像の視野を広げた画像を生成できることを示した．今後はさらにデータセットを増やして実験，また\\n'\n",
      "                    '詳細な考察を行う予定である．参考文献[1]  RICOH, '\n",
      "                    'THETA,https://theta360.com/ja/, Feb. 2019  参照.[2]  Kodak, '\n",
      "                    '4KVR360,https://kodakpixpro.com/AsiaOceania/jp/cameras/vrcamera/4kvr360/, '\n",
      "                    'Feb. 2019 \\n'\n",
      "                    '参照．[3]  mapillary, OpenSfM,点群投影画像（入力）補間画像（出力） \\n'\n",
      "                    '（a）  pix2pix[5]点群投影画像（入力）補間画像（出力） \\n'\n",
      "                    '（b）  Inpainting[6]点群投影画像（入力）補間画像（出力）（c）  Inpainting + '\n",
      "                    'pix2pix図 3  深層学習による補間結果Copyright © SSII 2019. All Rights '\n",
      "                    'Reserved.- IS1-17 '\n",
      "                    '-https://github.com/mapillary/OpenSfM, \\n'\n",
      "                    'Feb. 2019  参照．[4]  A.  Geiger,  P.  Lenz,  C.  Stiller  '\n",
      "                    'and  R.Urtasun,  “Vision  meets  Robotics:  TheKITTI '\n",
      "                    'Dataset,” International Journal ofRobotics Research '\n",
      "                    '(IJRR), 2013.[5]  P.  Isola,  J.-Y.  Zhu,  T.  Zhou,  '\n",
      "                    'and  A.  A.Efros, “Image-to- mage Translation '\n",
      "                    'withConditional  Adversarial  Networks,”Conference  on  '\n",
      "                    'Computer  Vision  andPattern Recognition (CVPR), July '\n",
      "                    '2017.[6]  G. Liu, F. A. Reda, K. J. Shih, T. C. Wang,A.  '\n",
      "                    'Tao  and  B.  Catanzaro,  “ImageInpainting  for  '\n",
      "                    'Irregular  Holes  UsingPartial  Convolutions,”  The  '\n",
      "                    'EuropeanConference on Computer Vision (ECCV),pp.85-100, '\n",
      "                    'Sept. 2018.Ground TruthGround TruthGround TruthIS1-17\\n'\n",
      "                    'SO1-17第25回画像センシングシンポジウム(a)  原画像のあるフレーム(b)  3 '\n",
      "                    '次元モデルを複数視線方向で撮影した２次元投影画像（数字は正面を 0°としたヨー角量）(c)  2 '\n",
      "                    '次元投影画像の補間結果図 4    3 '\n",
      "                    '次元モデルを複数視線方向で撮影した２次元投影画像の補間結果と合成結果（a）  '\n",
      "                    'データ拡張なし                                                        '\n",
      "                    '（b）データ拡張あり図 5    データ拡張を行った時の補間結果の比較Copyright © SSII 2019. '\n",
      "                    'All Rights Reserved.- IS1-17 -'},\n",
      "  'date': '2000-01-01 00:00:00',\n",
      "  'paper_title': '',\n",
      "  'pdf_name': 'IS1-17'},\n",
      " {'_id': ObjectId('5eb6bc5c60e378f730f5b62a'),\n",
      "  'conf_name': 'SSII2019',\n",
      "  'content': {'序論': '機械学習や深層学習の仕組みを用いて semantic seg-\\n'\n",
      "                    'mentation を行う際, 一般的に, 学習させたデータとは\\n'\n",
      "                    '別の環境で入手したデータをテストデータとして用い\\n'\n",
      "                    'ると, データの入手環境 (domain) に違いによって精度\\n'\n",
      "                    'が落ちることが予想される. この問題は, 新たなドメイ\\n'\n",
      "                    'ンにおける学習モデルを作成することで解決できるが\\n'\n",
      "                    'semantic segmentation においては教師データを用意す\\n'\n",
      "                    'ることに多大なコストがかかる. 更に生体医用画像の教\\n'\n",
      "                    '師データ作成においては有識者の関与が必要なケース\\n'\n",
      "                    'も多い. このような問題により, 新たなドメインにおい\\n'\n",
      "                    'て教師データを用意せずに 既存の学習モデルを利用し,\\n'\n",
      "                    '適応的にモ デルを作成していく技術が重要である. そ\\n'\n",
      "                    'のような技術分野はドメイン適応 (domain adaptation)\\n'\n",
      "                    'と呼ばれ近年活発に研究されている. 本論文では生体\\n'\n",
      "                    '医用画像特に病理画像の semantic segmentation にお\\n'\n",
      "                    'けるドメイン適応の効果を癌種というドメインに焦点\\n'\n",
      "                    'を当てて検証する.2 semantic segmentation とドメイン適応ドメイン適応の実施例として, '\n",
      "                    '例えば車載映像に対す\\n'\n",
      "                    'る研究が挙げられる. ドメインの対象として, 既存の環境\\n'\n",
      "                    'のデータ (source data) としては SYNTHIA[1],GTA5[2]\\n'\n",
      "                    'といったコンピュータグラフィックスによる合成デー\\n'\n",
      "                    'タ, 新たな環境のデータ (target data) として撮影デー\\n'\n",
      "                    'タ (cityscapes)[3] が用いられている. 先行研究例で\\n'\n",
      "                    'は,Cycle-GAN を用いた手法 [4], 敵対的学習 を用いた\\n'\n",
      "                    '手法 [5] などが取られている.\\n'\n",
      "                    '医用画像においても semantic segmentation の研究が\\n'\n",
      "                    '広く行われている [6],[7] ものの運用上は. 様々なドメイ\\n'\n",
      "                    'ン差による精度低下が懸念される. 例えば, 撮影機器や,\\n'\n",
      "                    '染色方法の違い, 識別対象の形状や構造の違いなどが要\\n'\n",
      "                    '因として考えられる [8],[9]. そこで本研究では病理画像\\n'\n",
      "                    'における癌種の違いに焦点を当てドメイン適応の有効\\n'\n",
      "                    '性を検証した.1Copyright © SSII 2019. All Rights Reserved.- '\n",
      "                    'IS1-19 -3 敵対的学習を用いたドメイン適応近年のドメイン適応は敵対的学習に基づいた研究\\n'\n",
      "                    '[10],[11] が多く, その有効性も実験的に示されている. こ\\n'\n",
      "                    'のような手法においては, 一般的に入力データが source\\n'\n",
      "                    'data と target data のどちらに由来するかを識別する\\n'\n",
      "                    'discriminator をモデルに組み込むことで, 双方のドメ\\n'\n",
      "                    'インの共通表現を学習する．semantic segmentation の\\n'\n",
      "                    'ドメイン適応においては物体の位置関係の情報を保持\\n'\n",
      "                    'することを狙い segmentor の出力自体をその共通表現\\n'\n",
      "                    'として用いる手法も提案されている [5]. 本研究ではの\\n'\n",
      "                    '[5] 報告に倣い, 図 1 に示す深層学習のモデルを構築し\\n'\n",
      "                    'た. 識別モデル D は生成モデル G による結果 P が\\n'\n",
      "                    'source data Is 由来か,target data It 由来か判断する.\\n'\n",
      "                    '一方で, 生成モデル G は source data に対する semantic\\n'\n",
      "                    'segmentation の精度向上とドメインに依存しない出力\\n'\n",
      "                    '生成の双方を目的として学習させる.maxminL(Is; It)(1)DGL(Is; It) = L(Is) + '\n",
      "                    '(cid:13)LD(P )(1) を解くことにより, 両ドメインに適応した結果が生成\\n'\n",
      "                    'されるように学習できる. また, 生体医用画像の seman-\\n'\n",
      "                    'tic segmentation では U-net[12] が用いられることが多\\n'\n",
      "                    'いが, モデルの軽量化と精度の観点から DRN-C-26[13]\\n'\n",
      "                    'を用いている.図 1: ネットワーク図4 実験概要今回使用するデータは独自のデータセットを用いてい\\n'\n",
      "                    'る. 分類すべきクラスは 3 クラスあり, 癌細胞核, その他IS1-19\\n'\n",
      "                    'SO1-19第25回画像センシングシンポジウムの細胞核, 背景である. 訓練データとして source data '\n",
      "                    'は\\n'\n",
      "                    '膵臓癌と肺癌画像を合計 50 枚,target data は大腸癌の\\n'\n",
      "                    '画像を 55 枚用意した. また, テストデータとして source\\n'\n",
      "                    'data は膵臓癌と肺癌画像を合計 29 枚,target data は大\\n'\n",
      "                    '腸癌の画像を 8 枚用意した. 表 1 にそれぞれの枚数を\\n'\n",
      "                    '記載した. data augmentation としてサイズ 512 × 512\\n'\n",
      "                    'の random crop, 上下左右反転の random rotation を採\\n'\n",
      "                    '用している. また, データの normalization に関しては\\n'\n",
      "                    '実験的に精度が向上したため平均 0, 分散 1 になるよう\\n'\n",
      "                    '入力画像毎に個別に行なった. 学習ステップとしては,\\n'\n",
      "                    'まず教師データの与えられている source data のみで生\\n'\n",
      "                    '成モデルを事前学習させる. その後, 教師データの与え\\n'\n",
      "                    'られている source data と教師データの与えられていな\\n'\n",
      "                    'い target data の両方を用いて図 1 に従い生成モデルと\\n'\n",
      "                    '識別モデルを交互に更新した.表 1: データセットの概要 (枚数)traintestSource(膵臓, '\n",
      "                    '肺)5029Target(大腸)5585 実験結果ドメイン適応による mIoU を表 2 に示す. また, それ\\n'\n",
      "                    'ぞれ source data,target data におけるクラス別の IoU\\n'\n",
      "                    'を表 3, 表 4 に示す. 表 2 は左から source data のみで\\n'\n",
      "                    '学習した際のモデルの場合,target data のみで学習し\\n'\n",
      "                    'た際のモデルの場合, ドメイン適応をした際の場合の結\\n'\n",
      "                    '果にそれぞれ対応する. ドメイン適応によって, 教師な\\n'\n",
      "                    'し学習にも関わらず target data の精度を向上しつつ\\n'\n",
      "                    'source data 自体の精度も向上するような結果となった.\\n'\n",
      "                    'target data である大腸癌の結果の例を図 2 に示す. 図\\n'\n",
      "                    '2 より，ドメイン適応により target data の細胞核の形\\n'\n",
      "                    '状が改善されている事が確認できる. また, 特筆すべき\\n'\n",
      "                    'なのは,target data に対する精度に関して target data\\n'\n",
      "                    'のみで学習したモデルよりも source data をもとにドメ\\n'\n",
      "                    'イン適用したモデルの方が高い精度を示したことであ\\n'\n",
      "                    'る. これは, 今回のドメイン差においては少数の target\\n'\n",
      "                    'data を用いた教師あり学習よりも,source data と target\\n'\n",
      "                    'data を併用したドメイン適応の手法の方が結果として\\n'\n",
      "                    '高い精度となり得る可能性を示唆している．更に, 表 3,\\n'\n",
      "                    '表 4 より, その他の細胞核の識別精度が大幅に向上して\\n'\n",
      "                    'いることがわかる. 病理画像においては, 病気の希少性\\n'\n",
      "                    'や, また, アノテーションに有識者が必要であることか\\n'\n",
      "                    'ら訓練に十分な教師データの入手が困難な場合が多い.\\n'\n",
      "                    'そのような問題に対し, 既存の教師ありデータと新規の\\n'\n",
      "                    '教師なしデータを組み合わせて学習を行える本手法はCopyright © SSII 2019. All Rights '\n",
      "                    'Reserved.- IS1-19 -有効である. また, 出力結果図 2 を見てみると適応前で\\n'\n",
      "                    'は target data の細胞の形状に適さない結果となってい\\n'\n",
      "                    'るが, ドメイン適応によって改善されている.表 2: 実験結果 (mIoU)target\\n'\n",
      "                    'modelaftersource\\n'\n",
      "                    'modeladaptationSource0.612-0.659(膵臓, 肺)Target\\n'\n",
      "                    '(大腸)0.5850.5870.612表 3: source data クラス別の結果 '\n",
      "                    '(IoU)cancernormalback\\n'\n",
      "                    'groundcellcellsource\\n'\n",
      "                    'model\\n'\n",
      "                    'after0.6790.2640.8940.7310.3350.911adaptation表 4: target '\n",
      "                    'data クラス別の結果 (IoU)cancernormalback\\n'\n",
      "                    'groundcellcellsource\\n'\n",
      "                    'model\\n'\n",
      "                    'target\\n'\n",
      "                    'model\\n'\n",
      "                    'after0.6580.2530.8430.6460.2670.8460.6640.3260.846adaptation6 '\n",
      "                    '結論本論文では, 生体医用画像, 特に病理画像の semantic\\n'\n",
      "                    'segmentation において癌種の違いをドメインの違いと\\n'\n",
      "                    'した時にドメイン適応がどのような効果をもたらすの\\n'\n",
      "                    'かを検証した. 実験結果の通り,source data と target\\n'\n",
      "                    'data 双方の精度が向上し, 出力結果にもその効果が現\\n'\n",
      "                    'れた. これにより, 特定の癌種に教師データを付与し, ド\\n'\n",
      "                    'メイン適応を実施することで他の癌種に対してもその\\n'\n",
      "                    '学習モデルが応用できる可能性を示した. 今後は他の\\n'\n",
      "                    '癌種や, 撮影機器, 染色方法の違いに対して本手法の有\\n'\n",
      "                    '効性を検証する.参考文献[1] German Ros, Laura Sellart, Joanna '\n",
      "                    'Materzynska,David Vazquez, and Antonio M. Lopez. The '\n",
      "                    'syn-IS1-19\\n'\n",
      "                    'SO1-19第25回画像センシングシンポジウム[1] 元画像[2] 正解[3] ドメイン適応前 [4] '\n",
      "                    'ドメイン適応後青:癌細胞, 白：正常細胞, 黒:背景図 2: 大腸癌におけるドメイン適応結果例thia '\n",
      "                    'dataset: A large collection of synthetic im-ages for '\n",
      "                    'semantic segmentation of urban scenes.In The IEEE '\n",
      "                    'Conference on Computer Visionand Pattern Recognition '\n",
      "                    '(CVPR), June 2016.[2] Stephan R. Richter, Vibhav Vineet, '\n",
      "                    'Stefan Roth,and Vladlen Koltun. Playing for data: '\n",
      "                    'Groundtruth from computer games.In Bastian Leibe,Jiri '\n",
      "                    'Matas, Nicu Sebe, and Max Welling, edi-tors, European '\n",
      "                    'Conference on Computer Vision(ECCV), volume 9906 of LNCS, '\n",
      "                    'pages 102{118.Springer International Publishing, 2016.[3] '\n",
      "                    'Marius Cordts, Mohamed Omran, SebastianRamos, Timo '\n",
      "                    'Rehfeld, Markus Enzweiler, Ro-drigo Benenson, Uwe Franke, '\n",
      "                    'Stefan Roth, andBernt Schiele. The cityscapes dataset for '\n",
      "                    'seman-tic urban scene understanding.In Proc. of theIEEE '\n",
      "                    'Conference on Computer Vision and Pat-tern Recognition '\n",
      "                    '(CVPR), 2016.[4] J. Hoﬀman, E. Tzeng, T. Park, J.-Y. '\n",
      "                    'Zhu,P. Isola, K. Saenko, A. Efros, and T. Darrell.CyCADA: '\n",
      "                    'Cycle-consistent adversarial domainadaptation. In '\n",
      "                    'Jennifer Dy and Andreas Krause,\\n'\n",
      "                    'editors, Proceedings of the 35th International\\n'\n",
      "                    'Conference on Machine Learning, volume 80 ofProceedings '\n",
      "                    'of Machine Learning Research, pages1989{1998, '\n",
      "                    'Stockholmsm(cid:127)assan, Stockholm Swe-den, 10{15 Jul '\n",
      "                    '2018. PMLR.[5] Y.-H. Tsai, W.-C. Hung, S. Schulter, K. '\n",
      "                    'Sohn,M.-H. Yang, and M. Chandraker. Learning toadapt '\n",
      "                    'structured output space for semantic seg-mentation. In '\n",
      "                    'IEEE Conference on Computer Vi-sion and Pattern '\n",
      "                    'Recognition (CVPR), 2018.[6] S. S. Mohseni Salehi, D. '\n",
      "                    'Erdogmus,andCopyright © SSII 2019. All Rights Reserved.- '\n",
      "                    'IS1-19 -A. Gholipour. Auto-context convolutional '\n",
      "                    'neuralnetwork (auto-net) for brain extraction in '\n",
      "                    'mag-netic resonance imaging. IEEE Transactions onMedical '\n",
      "                    'Imaging, 36(11):2319{2330, Nov 2017.[7] A. Fakhry, T. '\n",
      "                    'Zeng, and S. Ji. Residual decon-volutional networks for '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    'brain electron microscopyimage segmentation. IEEE '\n",
      "                    'Transactions on Med-ical Imaging, 36(2):447{456, Feb '\n",
      "                    '2017.[8] M. Javanmardi and T. Tasdizen. Domain '\n",
      "                    'adap-tation for biomedical image segmentation '\n",
      "                    'usingadversarial training. In 2018 IEEE 15th '\n",
      "                    'Interna-tional Symposium on Biomedical Imaging '\n",
      "                    '(ISBI2018), pages 554{558, April 2018.[9] R. '\n",
      "                    'Berm(cid:19)udez-Chac(cid:19)on, P. '\n",
      "                    'M(cid:19)arquez-Neila, M. Salz-mann, and P. Fua. A '\n",
      "                    'domain-adaptive two-stream u-net for electron microscopy '\n",
      "                    'image seg-mentation.In 2018 IEEE 15th '\n",
      "                    'InternationalSymposium on Biomedical Imaging (ISBI '\n",
      "                    '2018),pages 400{404, April 2018.[10] Yaroslav Ganin and '\n",
      "                    'Victor S. Lempitsky. Unsu-pervised domain adaptation by '\n",
      "                    'backpropagation.In ICML, 2015.[11] Y.-H. Chen, W.-Y. '\n",
      "                    'Chen, Y.-T. Chen, B.-C. Tsai,Y.-C. F. Wang, and M. Sun. '\n",
      "                    'No more discrimi-nation: Cross city adaptation of road '\n",
      "                    'scene seg-menters. pages 2011{2020, 10 2017.[12] O. '\n",
      "                    'Ronneberger, P.Fischer, and T. Brox. U-net: Convolutional '\n",
      "                    'networks for biomedical im-age segmentation. In Medical '\n",
      "                    'Image Computingand Computer-Assisted Intervention '\n",
      "                    '(MICCAI),volume 9351 of LNCS, pages 234{241. '\n",
      "                    'Springer,2015. (available on arXiv:1505.04597 '\n",
      "                    '[cs.CV]).[13] F. Yu, V. Koltun, and T. Funkhouser. '\n",
      "                    'Dilatedresidual networks. In Computer Vision and Pat-tern '\n",
      "                    'Recognition (CVPR), 2017.',\n",
      "              '結論': '本論文では, 生体医用画像, 特に病理画像の semantic\\n'\n",
      "                    'segmentation において癌種の違いをドメインの違いと\\n'\n",
      "                    'した時にドメイン適応がどのような効果をもたらすの\\n'\n",
      "                    'かを検証した. 実験結果の通り,source data と target\\n'\n",
      "                    'data 双方の精度が向上し, 出力結果にもその効果が現\\n'\n",
      "                    'れた. これにより, 特定の癌種に教師データを付与し, ド\\n'\n",
      "                    'メイン適応を実施することで他の癌種に対してもその\\n'\n",
      "                    '学習モデルが応用できる可能性を示した. 今後は他の\\n'\n",
      "                    '癌種や, 撮影機器, 染色方法の違いに対して本手法の有\\n'\n",
      "                    '効性を検証する.'},\n",
      "  'date': '2000-01-01 00:00:00',\n",
      "  'paper_title': '病理画像の semantic segmentation における',\n",
      "  'pdf_name': 'IS1-19'}]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    dict_list = next(database_loader_generator)  # 一度目の検索結果\n",
    "    pprint.pprint(dict_list)\n",
    "except StopIteration:\n",
    "    print(\"finished find\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DocumentのリストからPaperのリストを取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T16:16:19.225224Z",
     "start_time": "2020-05-09T16:16:19.145421Z"
    }
   },
   "outputs": [],
   "source": [
    "class DictListPdfParser():\n",
    "    def __init__(self, pdf_parser):\n",
    "        self.pdf_parser = pdf_parser\n",
    "        \n",
    "    def parse_from_dict_list(self, dict_list):\n",
    "        paper_list = [self.pdf_parser.parse_by_dict(one_dict) for one_dict in dict_list]\n",
    "        return paper_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T16:16:19.339899Z",
     "start_time": "2020-05-09T16:16:19.271086Z"
    }
   },
   "outputs": [],
   "source": [
    "list_paper_parser = PdfParserCount(count_patterns=find_patterns)\n",
    "dict_list_pdf_parser = DictListPdfParser(pdf_parser=list_paper_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T16:16:19.471568Z",
     "start_time": "2020-05-09T16:16:19.383783Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IS1-17\n",
      "\n",
      "OrderedDict([(re.compile('ディープラーニング|深層学習'), 4), (re.compile('CNN|ニューラルネットワーク'), 0), (re.compile('VAE|変分オートエンコーダ'), 0), (re.compile('GAN'), 0)])\n",
      "2000-01-01 00:00:00, IS1-19\n",
      "病理画像の semantic segmentation における\n",
      "OrderedDict([(re.compile('ディープラーニング|深層学習'), 2), (re.compile('CNN|ニューラルネットワーク'), 0), (re.compile('VAE|変分オートエンコーダ'), 0), (re.compile('GAN'), 1)])\n",
      "2000-01-01 00:00:00, IS1-13\n",
      "車載向け FPGA 搭載に向けたコンパクトな Residual Network による人物検知\n",
      "OrderedDict([(re.compile('ディープラーニング|深層学習'), 1), (re.compile('CNN|ニューラルネットワーク'), 0), (re.compile('VAE|変分オートエンコーダ'), 0), (re.compile('GAN'), 0)])\n",
      "2000-01-01 00:00:00, IS1-16\n",
      "Toward Person Re-identiﬁcation in Half-body Shots\n",
      "OrderedDict([(re.compile('ディープラーニング|深層学習'), 0), (re.compile('CNN|ニューラルネットワーク'), 56), (re.compile('VAE|変分オートエンコーダ'), 0), (re.compile('GAN'), 0)])\n",
      "2000-01-01 00:00:00, IS1-11\n",
      "CNN を用いた固有画像分解による低光量画像の視認性改善\n",
      "OrderedDict([(re.compile('ディープラーニング|深層学習'), 0), (re.compile('CNN|ニューラルネットワーク'), 15), (re.compile('VAE|変分オートエンコーダ'), 0), (re.compile('GAN'), 5)])\n",
      "2000-01-01 00:00:00, IS1-09\n",
      "\n",
      "OrderedDict([(re.compile('ディープラーニング|深層学習'), 0), (re.compile('CNN|ニューラルネットワーク'), 4), (re.compile('VAE|変分オートエンコーダ'), 0), (re.compile('GAN'), 0)])\n",
      "2000-01-01 00:00:00, IS1-03\n",
      "超音波厚さ測定の効率化のためのエコー検出手法\n",
      "OrderedDict([(re.compile('ディープラーニング|深層学習'), 0), (re.compile('CNN|ニューラルネットワーク'), 3), (re.compile('VAE|変分オートエンコーダ'), 0), (re.compile('GAN'), 0)])\n",
      "2000-01-01 00:00:00, IS1-06\n",
      "ドローンによる牧場空撮画像における乳牛の個体識別\n",
      "OrderedDict([(re.compile('ディープラーニング|深層学習'), 0), (re.compile('CNN|ニューラルネットワーク'), 2), (re.compile('VAE|変分オートエンコーダ'), 0), (re.compile('GAN'), 0)])\n",
      "2000-01-01 00:00:00, IS1-02\n",
      "VAEGAN の再構成誤差と Discriminator の ROI を活用した異常検出\n",
      "OrderedDict([(re.compile('ディープラーニング|深層学習'), 0), (re.compile('CNN|ニューラルネットワーク'), 0), (re.compile('VAE|変分オートエンコーダ'), 15), (re.compile('GAN'), 16)])\n",
      "2000-01-01 00:00:00]\n"
     ]
    }
   ],
   "source": [
    "paper_list = dict_list_pdf_parser.parse_from_dict_list(dict_list)\n",
    "paper_list.sort(key=lambda paper_counter: tuple(paper_counter.counters.values()),reverse=True)\n",
    "print(paper_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdf_py37",
   "language": "python",
   "name": "pdf_py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
