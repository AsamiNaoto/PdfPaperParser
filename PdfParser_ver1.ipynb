{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T08:49:35.557574Z",
     "start_time": "2020-05-07T08:49:32.642833Z"
    }
   },
   "outputs": [],
   "source": [
    "from pdfminer.converter import PDFPageAggregator\n",
    "from pdfminer.layout import LAParams, LTContainer, LTTextBox\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter, PDFResourceManager\n",
    "from pdfminer.pdfpage import PDFPage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T08:49:36.258286Z",
     "start_time": "2020-05-07T08:49:35.568546Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "from tqdm.notebook import tqdm\n",
    "import pprint\n",
    "import json\n",
    "import abc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LayoutオブジェクトからLTTextBoxのリストを取得する関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "抜き出すのは，textデータを前提とするのでこの関数が必要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T08:49:36.289043Z",
     "start_time": "2020-05-07T08:49:36.267105Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_textboxes_recursively(layout):\n",
    "    \"\"\"\n",
    "    再帰的にテキストボックス（LTTextBox）を探して、テキストボックスのリストを取得する。\n",
    "    \"\"\"\n",
    "    # LTTextBoxを継承するオブジェクトの場合は1要素のリストを返す。\n",
    "    if isinstance(layout, LTTextBox):\n",
    "        text_boxes = [layout]\n",
    "        return text_boxes  # 返すのはリスト\n",
    "\n",
    "    # LTContainerを継承するオブジェクトは子要素を含むので、再帰的に探す。\n",
    "    if isinstance(layout, LTContainer):\n",
    "        text_boxes = []\n",
    "        for child in layout:\n",
    "            text_boxes.extend(find_textboxes_recursively(child))  # 再帰的にリストをextend\n",
    "            \n",
    "        return text_boxes\n",
    "\n",
    "    return []  # 何も取得できなかった場合は空リストを返す"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ソート用の関数 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "textboxのソートは，1段組みと2段組みで異なる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二段組用のソート "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T08:49:36.905248Z",
     "start_time": "2020-05-07T08:49:36.882303Z"
    }
   },
   "outputs": [],
   "source": [
    "class SortTextbox2Column():\n",
    "    \"\"\"\n",
    "    2段組み用，始めのソートは左側と右側\n",
    "    \"\"\"\n",
    "    def __init__(self, layout_x0, layout_x1):\n",
    "        self.half_x = (layout_x0 + layout_x1)/2\n",
    "    \n",
    "    def __call__(self, text_box):\n",
    "        if text_box.x0 < self.half_x:\n",
    "            left_or_right = -1  # it mean left\n",
    "            \n",
    "        else:\n",
    "            left_or_right = 1  # it mean right\n",
    "            \n",
    "        return (left_or_right, -text_box.y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1段組み用のソート "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T08:49:37.784685Z",
     "start_time": "2020-05-07T08:49:37.757754Z"
    }
   },
   "outputs": [],
   "source": [
    "class SortTextbox():\n",
    "    \"\"\"\n",
    "    textboxの左下の座標でソート\n",
    "    \"\"\"\n",
    "    def __init__(self,*args):\n",
    "        \"\"\"\n",
    "        2段組み用のソートクラスとの対応のため\n",
    "        \"\"\"\n",
    "        pass\n",
    "    def __call__(self, text_box):\n",
    "        return (-text_boxt.y1, text_box.x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 論文データのベースクラス "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pdfデータをパースして保存するときと，呼び出すときに利用する？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T09:07:49.525971Z",
     "start_time": "2020-05-07T09:07:49.433226Z"
    }
   },
   "outputs": [],
   "source": [
    "class PaperBase(metaclass=abc.ABCMeta):\n",
    "    \"\"\"\n",
    "    論文のデータクラスとPaser用のストラテジーを一つにしたもの.正直一つにする意味はない.\n",
    "    ただ変更するクラスをまとめただけ\n",
    "    \n",
    "    \"\"\"\n",
    "    @abc.abstractmethod\n",
    "    def toDict(self):\n",
    "        pass\n",
    "    \n",
    "    @classmethod\n",
    "    def parse_by_textboxes(cls, text_boxes, parse_info):\n",
    "        \"\"\"\n",
    "        text_boxesからパースする\n",
    "        \"\"\"\n",
    "        paper_title, parse_text_dict = cls.str_from_textboxes(text_boxes, parse_info)  # スタティクメソッド\n",
    "        paper = cls.parse_by_text_dict(paper_title=paper_title, \n",
    "                                       parse_text_dict=parse_text_dict, \n",
    "                                       parse_info=parse_info)  # クラスメソッド\n",
    "        \n",
    "        return paper\n",
    "    \n",
    "    @classmethod\n",
    "    def parse_by_text_dict(cls, paper_title, parse_text_dict, parse_info):\n",
    "        raise NotImplementedError(\"Implement parse_by_text_dict\")\n",
    "        \n",
    "        \n",
    "    @classmethod\n",
    "    def parse_by_dict(cls, content):\n",
    "        raise NotImplementedError(\"Implement parse_by_content\")\n",
    "        \n",
    "    @staticmethod\n",
    "    def str_from_textboxes(text_boxes, parse_info):\n",
    "        \"\"\"\n",
    "        共通するテキスト取得プログラム\n",
    "        \"\"\"\n",
    "        parse_text_flag = False  # このフラッグがTrueである部分を保存する        \n",
    "                \n",
    "        patterns_keys = parse_info[\"start_patterns\"].keys()  # キーのリスト(のようなもの)\n",
    "        patterns_key_iter = iter(patterns_keys)  # 長さの違うfor文内で回すので，キーをイテレーター化\n",
    "        pattern_key = next(patterns_key_iter)  # 最初のキーを取得\n",
    "        \n",
    "        parse_text_dict = {i:\"\" for i in patterns_keys}\n",
    "        \n",
    "        for i,box in enumerate(text_boxes):\n",
    "            text = box.get_text().strip()  # 末尾の文字を削除\n",
    "            if i == parse_info[\"title_position_number\"]:\n",
    "                paper_title = text\n",
    "\n",
    "            if parse_text_flag:  # flagがTrueのうちは，parse_textにtextを加え続ける\n",
    "                parse_text_dict[pattern_key] += text\n",
    "\n",
    "            if parse_info[\"start_patterns\"][pattern_key].search(text):  # マッチしたらフラッグをTrueに\n",
    "                parse_text_flag = True\n",
    "            \n",
    "            if parse_info[\"end_patterns\"][pattern_key] is not None:  # Noneだったら，最後までflagはTrue\n",
    "                if parse_info[\"end_patterns\"][pattern_key].search(text):\n",
    "                    try:\n",
    "                        parse_text_flag = False\n",
    "                        pattern_key = next(patterns_key_iter)  # end_patternがマッチしたらpatterns_key_iterをイテレーション\n",
    "                    except StopIteration:\n",
    "                        break  # 次のpattern_keyがなくなってStopIterationエラーが出たら終了\n",
    "        \n",
    "        return paper_title, parse_text_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### セーブデータ保存用の Paperクラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T09:07:50.711133Z",
     "start_time": "2020-05-07T09:07:50.609998Z"
    }
   },
   "outputs": [],
   "source": [
    "class PaperForSave(PaperBase):\n",
    "    \"\"\"\n",
    "    論文をテキストデータとして，保存するための論文データクラス\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 conf_name=None, \n",
    "                 pdf_name=None, \n",
    "                 paper_title=None, \n",
    "                 pdf_content=None,\n",
    "                 \n",
    "                ):\n",
    "        \"\"\"\n",
    "        一つのデータで\n",
    "        Parameters\n",
    "        ----------\n",
    "        conf_name: str\n",
    "            学会や論文集を表す文字列\n",
    "        pdf_name: str\n",
    "            対応するpdfファイルの名前を表す文字列\n",
    "        paper_title: str\n",
    "            論文のタイトル\n",
    "        pdf_content: dict\n",
    "            保存するテキストのdictionaly\n",
    "        \"\"\"\n",
    "        self.conf_name = conf_name\n",
    "        self.pdf_name = pdf_name\n",
    "        self.paper_title = paper_title\n",
    "        self.pdf_content = pdf_content\n",
    "        \n",
    "    def toDict(self):\n",
    "        out_dict = {\"pdf_name\": self.pdf_name,\n",
    "                    \"paper_title\": self.paper_title,\n",
    "                    \"content\":self.pdf_content\n",
    "                   }\n",
    "        return out_dict\n",
    "    \n",
    "    @classmethod\n",
    "    def parse_by_text_dict(cls, paper_title, parse_text_dict, parse_info):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        text_boxes: list of textbox\n",
    "            pdfをパースしたときに得られるtextboxのリスト\n",
    "        start_patterns\n",
    "        \"\"\"\n",
    "        #from IPython.core.debugger import Pdb; Pdb().set_trace()  # PaperForSave\n",
    "        paper_conf_name = parse_info[\"conf_name\"]\n",
    "        paper_pdf_name = parse_info[\"pdf_name\"]\n",
    "        \n",
    "        # Paperへのデータの付与\n",
    "        paper = cls(conf_name=paper_conf_name,\n",
    "                    paper_title=paper_title,\n",
    "                    pdf_name=paper_pdf_name,\n",
    "                    pdf_content=parse_text_dict\n",
    "                   )\n",
    "\n",
    "        return paper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### カウント用のPaperクラス "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paperクラスの拡張は以下のように行う，初期化メソッドはデータをアトリビュートとして保持するように実装．`toDict`と`parse_by_textboxes`,`parse_by_contents`は適宜実装する．その際，Parserクラスの`parse_info`と対応するように実装する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T09:09:45.912897Z",
     "start_time": "2020-05-07T09:09:45.732391Z"
    }
   },
   "outputs": [],
   "source": [
    "class PaperForCount(PaperBase):\n",
    "    def __init__(self, pdf_name=None, count_patterns=[], paper_title=None):\n",
    "        \"\"\"\n",
    "        countersは保存する文字列あるいはパターンのリスト\n",
    "        \"\"\"\n",
    "        self.pdf_name = pdf_name\n",
    "        self.paper_title = paper_title\n",
    "        self.counters = OrderedDict()\n",
    "        for i in count_patterns:\n",
    "            self.counters[i] = 0  # パターンオブジェクトはhashableでキーにできる．まず，0に初期化\n",
    "    \n",
    "    def toDict(self):\n",
    "        counters = {i.pattern:self.counters[i] for i in self.counters.keys()}  # キーを文字列へ\n",
    "        \n",
    "        out_dict = {\"pdf_name\":self.pdf_name,\n",
    "                    \"paper_title\":self.paper_title,\n",
    "                    \"counters\":counters\n",
    "                   }\n",
    "        return out_dict\n",
    "    \n",
    "    @classmethod\n",
    "    def parse_by_text_dict(cls, paper_title, parse_text_dict, parse_info):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        text_boxes: list of textbox\n",
    "            pdfをパースしたときに得られるtextboxのリスト\n",
    "        parse_info: dict \n",
    "            パースの時に必要な情報\n",
    "        \"\"\"\n",
    "        #from IPython.core.debugger import Pdb; Pdb().set_trace()  # PaperForCount\n",
    "        \n",
    "        paper_pdf_name = parse_info[\"pdf_name\"]\n",
    "                \n",
    "        # 以下Paperへのデータの付与\n",
    "        count_patterns = parse_info[\"count_patterns\"]\n",
    "        \n",
    "        paper = cls(pdf_name=paper_pdf_name,\n",
    "                    paper_title=paper_title,\n",
    "                    count_patterns=count_patterns\n",
    "                   )\n",
    "        \n",
    "        for pattern in count_patterns:\n",
    "            for text in parse_text_dict.values():\n",
    "                m = pattern.findall(text)\n",
    "                paper.counters[pattern] += len(m)\n",
    "                \n",
    "        return paper\n",
    "    \n",
    "    @classmethod\n",
    "    def parse_by_dict(cls, paper_dict, parse_info):\n",
    "        #from IPython.core.debugger import Pdb; Pdb().set_trace()  # PaperForCount\n",
    "        paper_title = paper_dict[\"paper_title\"]\n",
    "        paper_pdf_name = paper_dict[\"pdf_name\"]\n",
    "        parse_text_dict = paper_dict[\"contents\"]\n",
    "        \n",
    "        count_patterns = parse_infonfo[\"count_patterns\"]\n",
    "        \n",
    "        paper = cls(pdf_name=paper_pdf_name,\n",
    "                    paper_title=paper_title,\n",
    "                    count_patterns=count_patterns\n",
    "                   )\n",
    "        \n",
    "        for pattern in count_patterns:\n",
    "            for text in parse_text_dict.values():\n",
    "                m = pattern.findall(text)\n",
    "                paper.counters[pattern] += len(m)\n",
    "                \n",
    "        return paper\n",
    "    \n",
    "    def is_counted(self):\n",
    "        # 一つも含まれていないとき\n",
    "        if set(self.counters.values()) == {0}:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "        \n",
    "    def __repr__(self):\n",
    "        str_pdf_name = str(self.pdf_name)\n",
    "        str_paper_title = str(self.paper_title)\n",
    "        str_counters = str(self.counters)\n",
    "        return str_pdf_name+\"\\n\"+str_paper_title+\"\\n\"+str_counters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## あるpdfファイルをパースし，パースした内容をPaperオブジェクトで返すオブジェクト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T09:09:49.802411Z",
     "start_time": "2020-05-07T09:09:49.675753Z"
    }
   },
   "outputs": [],
   "source": [
    "class PdfParser():\n",
    "    def __init__(self, \n",
    "                 conference_name,\n",
    "                 start_patterns={\"all\":re.compile(\".*\")},\n",
    "                 end_patterns={\"all\":None},\n",
    "                 title_position_number=2,\n",
    "                 parse_page_numbers=[0],\n",
    "                 column_number=2,\n",
    "                 paper_data_class=PaperForSave()\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        conference_name: str\n",
    "            学会や論文集の名前\n",
    "        start_patterns: dict of patterns\n",
    "            Paperオブジェクトに保持するテキストの開始位置の辞書\n",
    "        end_patterns: dict of pattrens\n",
    "            Paperオブジェクトに保持するテキストの終了位置の辞書，Noneは最後まで\n",
    "        title_position_number: int\n",
    "            titleが与えられるtextboxのインデックス(ソート後)\n",
    "        parse_page_numbers: list of int\n",
    "            パースするページのリスト，Noneは最後まで\n",
    "        paper_data_class: Paper class\n",
    "            ペーパークラスのオブジェクトをストラテジーとして直接与える．\n",
    "        \"\"\"\n",
    "        \n",
    "        self.conference_name = conference_name\n",
    "        \n",
    "        if set(start_patterns.keys()) != set(end_patterns.keys()):\n",
    "            raise ValueError(\"start patterns and eend patterns are not correspondding\")\n",
    "        \n",
    "        self.title_position_number = title_position_number\n",
    "        self.parse_page_numbers = parse_page_numbers  \n",
    "        self.column_number = column_number\n",
    "        \n",
    "        self.paper_data_class = paper_data_class\n",
    "        \n",
    "        self.start_patterns = start_patterns\n",
    "        self.end_patterns = end_patterns\n",
    "        \n",
    "        # パースに必要なクラスの作成\n",
    "        # Layout Analysisのパラメーターを設定。縦書きの検出を有効にする。\n",
    "        laparams = LAParams(detect_vertical=True)\n",
    "\n",
    "        # 共有のリソースを管理するリソースマネージャーを作成。\n",
    "        resource_manager = PDFResourceManager(caching=False)\n",
    "\n",
    "        # ページを集めるPageAggregatorオブジェクトを作成。\n",
    "        self.device = PDFPageAggregator(resource_manager, laparams=laparams)\n",
    "\n",
    "        # Interpreterオブジェクトを作成。\n",
    "        self.interpreter = PDFPageInterpreter(resource_manager, self.device)\n",
    "        \n",
    "        if column_number==1:\n",
    "            self.SortFuncClass = SortTextbox  # クラスを変数として保持\n",
    "        elif column_number==2:\n",
    "            self.SortFuncClass = SortTextbox2Column\n",
    "        else:\n",
    "            raise ValueError(\"The column rather than two is not defined\")\n",
    "        \n",
    "    def parse(self, pdf_file_path):\n",
    "        \"\"\"\n",
    "        オーバーライドは原則禁止\n",
    "        \"\"\"\n",
    "        self.pdf_file_name = str(pdf_file_path.stem)  # 内部メソッドからの参照用\n",
    "        \n",
    "        with open(pdf_file_path, \"rb\") as f:\n",
    "\n",
    "            parse_text = \"\"\n",
    "            parse_text_flag = False  # このフラッグがTrueである部分を序論とする\n",
    "\n",
    "            for page in PDFPage.get_pages(f, pagenos=self.parse_page_numbers):\n",
    "                self.interpreter.process_page(page)  # ページを処理する。\n",
    "                layout = self.device.get_result()  # LTPageオブジェクトを取得。\n",
    "                text_boxes = find_textboxes_recursively(layout)      \n",
    "\n",
    "                # text_boxの座標値毎にソート，複数キーのソート\n",
    "                # 少なくともこのページは全て読み込む必要があるため，非効率\n",
    "                sort_func= self.SortFuncClass(layout_x0=layout.x0, layout_x1=layout.x1)\n",
    "                text_boxes.sort(key=sort_func)\n",
    "                \n",
    "                info_dict = self.parse_info()\n",
    "                paper = self.paper_data_class.parse_by_textboxes(text_boxes, info_dict)\n",
    "\n",
    "        return paper\n",
    "    \n",
    "    def parse_info(self):\n",
    "        \"\"\"\n",
    "        Paperオブジェクトによって要オーバーライド\n",
    "        \"\"\"\n",
    "        info_dict = {}\n",
    "        info_dict[\"conf_name\"] = self.conference_name\n",
    "        info_dict[\"pdf_name\"] = self.pdf_file_name\n",
    "        info_dict[\"start_patterns\"] = self.start_patterns\n",
    "        info_dict[\"end_patterns\"] = self.end_patterns\n",
    "        info_dict[\"title_position_number\"] = self.title_position_number\n",
    "        return info_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テストコード "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T09:28:15.420433Z",
     "start_time": "2020-05-07T09:28:11.758233Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'序論': '製造・物流分野での労働力安定供給に向け，人の物理的な非定型作業を代替できるロボットの開発が求められている．さまざまな作業環境での代替実現には，作業環境の照明状況によらずに種々の物体を\\n認識できる必要がある．そこで，本研究では，画像\\nの局所的な鮮明度が照明条件によって変わること\\nに着目し，局所鮮明度に応じて対象物の類似度\\n評価方法を適切に自動選択する認識アーキテク\\nチャを開発した．倉庫作業を模擬した実験系を組\\nみ，撮像画像内の複数対象物の照明条件がそれ\\nぞれ異なる状況下において，従来手法は認識率\\nが 90%未満であるのに対し，提案手法では 98%\\n以上の認識率を達成できることを確認した．'}\n"
     ]
    }
   ],
   "source": [
    "start_patterns = {\"序論\":re.compile(\"[1-9]*\\s*\\S*(背景|はじめに|Abstract|序論|概要|Introduction)\")}  # これが当てはまらないものも多い\n",
    "end_patterns = {\"序論\":re.compile(\"(関連研究|提案手法|従来手法|従来研究)\")}  # これが当てはまらないものも多い\n",
    "#end_patterns = {\"序論\":None}\n",
    "conference_name = \"SSII2019\"\n",
    "title_position_number = 2\n",
    "parse_page_numbers = [0]  # 正直これが一番重要(1枚目まで確認)\n",
    "pdf_paper_parser = PdfParser(\n",
    "                             conference_name=conference_name,\n",
    "                             start_patterns=start_patterns,\n",
    "                             end_patterns=end_patterns,\n",
    "                             title_position_number=title_position_number,\n",
    "                             parse_page_numbers=parse_page_numbers,\n",
    "                            )\n",
    "\n",
    "paper = pdf_paper_parser.parse(Path(\"../PDFs/IS1-20.pdf\"))\n",
    "print(paper.pdf_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## カウント用のパーサ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PdfParserの拡張はPdfParserを継承することによって行う．PdfParserクラスはPaperクラスと対のようになっており，対応するようにparse_infoに追加する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T10:53:36.980930Z",
     "start_time": "2020-05-07T10:53:36.922102Z"
    }
   },
   "outputs": [],
   "source": [
    "class PdfParserCount(PdfParser):\n",
    "    def __init__(self, count_patterns,**kwargs):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        count_patterns: list of pattern\n",
    "            検索したいパターンのリスト\n",
    "        conference_name: str\n",
    "            学会や論文集の名前\n",
    "        start_patterns: dict of patterns\n",
    "            Paperオブジェクトに保持するテキストの開始位置の辞書\n",
    "        end_patterns: dict of pattrens\n",
    "            Paperオブジェクトに保持するテキストの終了位置の辞書，Noneは最後まで\n",
    "        title_position_number: int\n",
    "            titleが与えられるtextboxのインデックス(ソート後)\n",
    "        parse_page_numbers: list of int\n",
    "            パースするページのリスト，Noneは最後まで\n",
    "        paper_data_class: Paper class\n",
    "            ペーパークラスのオブジェクトをストラテジーとして直接与える．\n",
    "        \"\"\"\n",
    "        kwargs[\"paper_data_class\"] = PaperForCount()  # カウント用のPaperクラス\n",
    "        super(PdfParserCount, self).__init__(**kwargs)  # 引数展開\n",
    "        self.count_patterns = count_patterns\n",
    "        \n",
    "    def parse_info(self):\n",
    "        info_dict = super(PdfParserCount, self).parse_info()\n",
    "        info_dict[\"count_patterns\"] = self.count_patterns\n",
    "        \n",
    "        return info_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テストコード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T10:53:41.064781Z",
     "start_time": "2020-05-07T10:53:38.239823Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'counters': {'CNN|ニューラルネットワーク': 0,\n",
      "              'GAN': 0,\n",
      "              'VAE|変分オートエンコーダ': 0,\n",
      "              'ディープラーニング|深層学習': 0},\n",
      " 'paper_title': 'ベクタ型レーザ投影における自己位置推定のためのマーカ埋め込み手法の検討',\n",
      " 'pdf_name': 'IS1-10'}\n"
     ]
    }
   ],
   "source": [
    "start_patterns = {\"序論\":re.compile(\"[1-9]*\\s*\\S*(背景|はじめに|Abstract|序論|概要|Introduction)\")}  # これが当てはまらないものも多い\n",
    "end_patterns = {\"序論\":re.compile(\"(関連研究|提案手法|従来手法|従来研究)\")}  # これが当てはまらないものも多い\n",
    "count_patterns = [re.compile(\"ディープラーニング|深層学習\"),\n",
    "                  re.compile(\"CNN|ニューラルネットワーク\"),\n",
    "                  re.compile(\"VAE|変分オートエンコーダ\"),\n",
    "                  re.compile(\"GAN\")\n",
    "                 ]\n",
    "\n",
    "#end_patterns = {\"序論\":None}\n",
    "conference_name = \"SSII2019\"\n",
    "title_position_number = 2\n",
    "parse_page_numbers = [0]  # 正直これが一番重要(1枚目まで確認)\n",
    "pdf_paper_parser = PdfParserCount(count_patterns=count_patterns,\n",
    "                                  conference_name=conference_name,\n",
    "                                  start_patterns=start_patterns,\n",
    "                                  end_patterns=end_patterns,\n",
    "                                  title_position_number=title_position_number,\n",
    "                                  parse_page_numbers=parse_page_numbers,\n",
    "                                  )\n",
    "\n",
    "paper = pdf_paper_parser.parse(Path(\"../PDFs/IS1-10.pdf\"))\n",
    "pprint.pprint(paper.toDict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## あるディレクトリ内のpdfをパース"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T09:10:13.184767Z",
     "start_time": "2020-05-07T09:10:13.079591Z"
    }
   },
   "outputs": [],
   "source": [
    "class DirectoryPdfParserJson():\n",
    "    def __init__(self, \n",
    "                 dir_path,\n",
    "                 pdf_parser\n",
    "                ):\n",
    "        \n",
    "        self.dir_path = Path(dir_path)\n",
    "        self.pdf_list = list(self.dir_path.glob(\"./*.pdf\"))  # 複数回パースする必要があるため、リスト化\n",
    "        \n",
    "        self.pdf_parser = pdf_parser\n",
    "        \n",
    "    def parse(self):\n",
    "        paper_list = []\n",
    "        for i in tqdm(self.pdf_list):\n",
    "            paper = self.pdf_parser.parse(i)\n",
    "            paper_list.append(paper)\n",
    "        \n",
    "        return paper_list\n",
    "    \n",
    "    def parse_dict(self):\n",
    "        \n",
    "        save_dict = {self.pdf_parser.conference_name:{}}\n",
    "        paper_list = self.parse()\n",
    "            \n",
    "        # 全てメモリで展開するので，非効率\n",
    "        for paper in paper_list:\n",
    "            save_dict[self.pdf_parser.conference_name][paper.pdf_name] = paper.toDict()\n",
    "            \n",
    "        return save_dict        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テストコード "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ディレクトリからのパース"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T09:10:43.804148Z",
     "start_time": "2020-05-07T09:10:15.306630Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df5412614370467c8fb42c8219b4ec98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=19.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-cfa91e9809d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m                                         \u001b[0mpdf_parser\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpdf_parser\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m                                        )\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mpaper_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdir_pdf_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-27-8dc5136f480f>\u001b[0m in \u001b[0;36mparse_dict\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0msave_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpdf_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconference_name\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mpaper_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;31m# 全てメモリで展開するので，非効率\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-27-8dc5136f480f>\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mpaper_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpdf_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m             \u001b[0mpaper\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpdf_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m             \u001b[0mpaper_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpaper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-a2c936bd37a1>\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self, pdf_file_path)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mpage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mPDFPage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_pages\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpagenos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_page_numbers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpreter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_page\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# ページを処理する。\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m                 \u001b[0mlayout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# LTPageオブジェクトを取得。\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m                 \u001b[0mtext_boxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_textboxes_recursively\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pdf_py37\\lib\\site-packages\\pdfminer\\pdfinterp.py\u001b[0m in \u001b[0;36mprocess_page\u001b[1;34m(self, page)\u001b[0m\n\u001b[0;32m    850\u001b[0m             \u001b[0mctm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0my0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    851\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_page\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 852\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender_contents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresources\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mctm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    853\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend_page\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pdf_py37\\lib\\site-packages\\pdfminer\\pdfinterp.py\u001b[0m in \u001b[0;36mrender_contents\u001b[1;34m(self, resources, streams, ctm)\u001b[0m\n\u001b[0;32m    862\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_resources\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresources\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    863\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 864\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstreams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    865\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    866\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pdf_py37\\lib\\site-packages\\pdfminer\\pdfinterp.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, streams)\u001b[0m\n\u001b[0;32m    886\u001b[0m                         \u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'exec: %s %r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 888\u001b[1;33m                             \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    889\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    890\u001b[0m                         \u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'exec: %s'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pdf_py37\\lib\\site-packages\\pdfminer\\pdfinterp.py\u001b[0m in \u001b[0;36mdo_TJ\u001b[1;34m(self, seq)\u001b[0m\n\u001b[0;32m    770\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mPDFInterpreterError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No font specified!'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    771\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 772\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mncs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraphicstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    773\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    774\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pdf_py37\\lib\\site-packages\\pdfminer\\pdfdevice.py\u001b[0m in \u001b[0;36mrender_string\u001b[1;34m(self, textstate, seq, ncs, graphicstate)\u001b[0m\n\u001b[0;32m     85\u001b[0m             textstate.linematrix = self.render_string_horizontal(\n\u001b[0;32m     86\u001b[0m                 \u001b[0mseq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtextstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinematrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfont\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m                 scaling, charspace, wordspace, rise, dxscale, ncs, graphicstate)\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pdf_py37\\lib\\site-packages\\pdfminer\\pdfdevice.py\u001b[0m in \u001b[0;36mrender_string_horizontal\u001b[1;34m(self, seq, matrix, pos, font, fontsize, scaling, charspace, wordspace, rise, dxscale, ncs, graphicstate)\u001b[0m\n\u001b[0;32m     98\u001b[0m                 \u001b[0mneedcharspace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mcid\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfont\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mneedcharspace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m                         \u001b[0mx\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcharspace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pdf_py37\\lib\\site-packages\\pdfminer\\pdffont.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, bytes)\u001b[0m\n\u001b[0;32m    742\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    743\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 744\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcmap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    745\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    746\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mchar_disp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pdf_py37\\lib\\site-packages\\pdfminer\\cmapdb.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, code)\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m//\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mstruct\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'>%dH'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dir_path = Path(\"../PDFs\")\n",
    "start_patterns = {\"序論\":re.compile(\"[1-9]\\s*\\S*(背景|はじめに|Abstract|序論|概要|Introduction|背景・目的)\")}  # これが当てはまらないものも多い\n",
    "end_patterns = {\"序論\":re.compile(\"[1-9]\\s*\\S*(関連研究|提案手法|従来手法|従来研究)\")}  # これが当てはまらないものも多い\n",
    "conference_name = \"SSII2019\"\n",
    "title_position_number = 2\n",
    "parse_page_numbers = [0]  # 正直これが一番重要(1枚目まで確認)\n",
    "pdf_parser = PdfParser(conference_name=conference_name,\n",
    "                       start_patterns=start_patterns,\n",
    "                       end_patterns=end_patterns,\n",
    "                       title_position_number=title_position_number,\n",
    "                       parse_page_numbers=parse_page_numbers,\n",
    "                       )\n",
    "\n",
    "dir_pdf_parser = DirectoryPdfParserJson(dir_path=dir_path,\n",
    "                                        pdf_parser=pdf_parser\n",
    "                                       )\n",
    "paper_dict = dir_pdf_parser.parse_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T09:10:43.818108Z",
     "start_time": "2020-05-07T09:10:16.441Z"
    }
   },
   "outputs": [],
   "source": [
    "pprint.pprint(paper_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### jsonへの保存 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T10:52:51.348105Z",
     "start_time": "2020-05-06T10:52:51.288270Z"
    }
   },
   "outputs": [],
   "source": [
    "save_path = Path(\"./papers.json\")\n",
    "with open(save_path,\"w\") as fw:\n",
    "    json.dump(paper_dict,fw,indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### カウント"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T10:56:48.272022Z",
     "start_time": "2020-05-06T10:55:36.523544Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a5dfc632aa1481a8f475de2d68503ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=19.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dir_path = Path(\"../PDFs\")\n",
    "start_patterns = {\"序論\":re.compile(\"[1-9]*\\s*\\S*(背景|はじめに|Abstract|序論|概要|Introduction)\")}  # これが当てはまらないものも多い\n",
    "end_patterns = {\"序論\":re.compile(\"(関連研究|提案手法|従来手法|従来研究)\")}  # これが当てはまらないものも多い\n",
    "count_patterns = [re.compile(\"ディープラーニング|深層学習\"),\n",
    "                  re.compile(\"CNN|ニューラルネットワーク\"),\n",
    "                  re.compile(\"VAE|変分オートエンコーダ\"),\n",
    "                  re.compile(\"GAN\")\n",
    "                 ]\n",
    "\n",
    "conference_name = \"SSII2019\"\n",
    "title_position_number = 2\n",
    "parse_page_numbers = [0]  # 正直これが一番重要(1枚目まで確認)\n",
    "pdf_paper_parser = PdfParserCount(count_patterns=count_patterns,\n",
    "                                  conference_name=conference_name,\n",
    "                                  start_patterns=start_patterns,\n",
    "                                  end_patterns=end_patterns,\n",
    "                                  title_position_number=title_position_number,\n",
    "                                  parse_page_numbers=parse_page_numbers,\n",
    "                                  )\n",
    "\n",
    "dir_pdf_parser = DirectoryPdfParserJson(dir_path=dir_path,\n",
    "                                        pdf_parser=pdf_paper_parser\n",
    "                                       )\n",
    "paper_list = dir_pdf_parser.parse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T10:31:04.161961Z",
     "start_time": "2020-05-06T10:31:04.083482Z"
    }
   },
   "source": [
    "#### paper_listのソート "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T10:56:48.305931Z",
     "start_time": "2020-05-06T10:56:48.283990Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IS1-19\n",
      "病理画像の semantic segmentation における\n",
      "OrderedDict([(re.compile('ディープラーニング|深層学習'), 2), (re.compile('CNN|ニューラルネットワーク'), 0), (re.compile('VAE|変分オートエンコーダ'), 0), (re.compile('GAN'), 1)]), IS1-17\n",
      "\n",
      "OrderedDict([(re.compile('ディープラーニング|深層学習'), 2), (re.compile('CNN|ニューラルネットワーク'), 0), (re.compile('VAE|変分オートエンコーダ'), 0), (re.compile('GAN'), 0)]), IS1-13\n",
      "車載向け FPGA 搭載に向けたコンパクトな Residual Network による人物検知\n",
      "OrderedDict([(re.compile('ディープラーニング|深層学習'), 1), (re.compile('CNN|ニューラルネットワーク'), 0), (re.compile('VAE|変分オートエンコーダ'), 0), (re.compile('GAN'), 0)]), IS1-11\n",
      "CNN を用いた固有画像分解による低光量画像の視認性改善\n",
      "OrderedDict([(re.compile('ディープラーニング|深層学習'), 0), (re.compile('CNN|ニューラルネットワーク'), 3), (re.compile('VAE|変分オートエンコーダ'), 0), (re.compile('GAN'), 1)]), IS1-16\n",
      "Toward Person Re-identiﬁcation in Half-body Shots\n",
      "OrderedDict([(re.compile('ディープラーニング|深層学習'), 0), (re.compile('CNN|ニューラルネットワーク'), 3), (re.compile('VAE|変分オートエンコーダ'), 0), (re.compile('GAN'), 0)]), IS1-03\n",
      "超音波厚さ測定の効率化のためのエコー検出手法\n",
      "OrderedDict([(re.compile('ディープラーニング|深層学習'), 0), (re.compile('CNN|ニューラルネットワーク'), 2), (re.compile('VAE|変分オートエンコーダ'), 0), (re.compile('GAN'), 0)]), IS1-06\n",
      "ドローンによる牧場空撮画像における乳牛の個体識別\n",
      "OrderedDict([(re.compile('ディープラーニング|深層学習'), 0), (re.compile('CNN|ニューラルネットワーク'), 2), (re.compile('VAE|変分オートエンコーダ'), 0), (re.compile('GAN'), 0)]), IS1-09\n",
      "\n",
      "OrderedDict([(re.compile('ディープラーニング|深層学習'), 0), (re.compile('CNN|ニューラルネットワーク'), 2), (re.compile('VAE|変分オートエンコーダ'), 0), (re.compile('GAN'), 0)]), IS1-02\n",
      "VAEGAN の再構成誤差と Discriminator の ROI を活用した異常検出\n",
      "OrderedDict([(re.compile('ディープラーニング|深層学習'), 0), (re.compile('CNN|ニューラルネットワーク'), 0), (re.compile('VAE|変分オートエンコーダ'), 6), (re.compile('GAN'), 7)]), IS1-20\n",
      "\n",
      "OrderedDict([(re.compile('ディープラーニング|深層学習'), 0), (re.compile('CNN|ニューラルネットワーク'), 0), (re.compile('VAE|変分オートエンコーダ'), 0), (re.compile('GAN'), 0)]), IS1-04\n",
      "歩行者群分割\n",
      "OrderedDict([(re.compile('ディープラーニング|深層学習'), 0), (re.compile('CNN|ニューラルネットワーク'), 0), (re.compile('VAE|変分オートエンコーダ'), 0), (re.compile('GAN'), 0)]), IS1-05\n",
      "\n",
      "OrderedDict([(re.compile('ディープラーニング|深層学習'), 0), (re.compile('CNN|ニューラルネットワーク'), 0), (re.compile('VAE|変分オートエンコーダ'), 0), (re.compile('GAN'), 0)]), IS1-07\n",
      "\n",
      "OrderedDict([(re.compile('ディープラーニング|深層学習'), 0), (re.compile('CNN|ニューラルネットワーク'), 0), (re.compile('VAE|変分オートエンコーダ'), 0), (re.compile('GAN'), 0)]), IS1-08\n",
      "\n",
      "OrderedDict([(re.compile('ディープラーニング|深層学習'), 0), (re.compile('CNN|ニューラルネットワーク'), 0), (re.compile('VAE|変分オートエンコーダ'), 0), (re.compile('GAN'), 0)]), IS1-10\n",
      "ベクタ型レーザ投影における自己位置推定のためのマーカ埋め込み手法の検討\n",
      "OrderedDict([(re.compile('ディープラーニング|深層学習'), 0), (re.compile('CNN|ニューラルネットワーク'), 0), (re.compile('VAE|変分オートエンコーダ'), 0), (re.compile('GAN'), 0)]), IS1-12\n",
      "冗長 DCT による高効率な周波数フィルタリング\n",
      "OrderedDict([(re.compile('ディープラーニング|深層学習'), 0), (re.compile('CNN|ニューラルネットワーク'), 0), (re.compile('VAE|変分オートエンコーダ'), 0), (re.compile('GAN'), 0)]), IS1-14\n",
      "ハイパースペクトル画像を用いた土壌の走破性の判定\n",
      "OrderedDict([(re.compile('ディープラーニング|深層学習'), 0), (re.compile('CNN|ニューラルネットワーク'), 0), (re.compile('VAE|変分オートエンコーダ'), 0), (re.compile('GAN'), 0)]), IS1-15\n",
      "個人情報が保護された人物対応付けにおける\n",
      "OrderedDict([(re.compile('ディープラーニング|深層学習'), 0), (re.compile('CNN|ニューラルネットワーク'), 0), (re.compile('VAE|変分オートエンコーダ'), 0), (re.compile('GAN'), 0)]), IS1-18\n",
      "単眼カメラによる物体輪郭線の 3 次元形状推定\n",
      "OrderedDict([(re.compile('ディープラーニング|深層学習'), 0), (re.compile('CNN|ニューラルネットワーク'), 0), (re.compile('VAE|変分オートエンコーダ'), 0), (re.compile('GAN'), 0)])]\n"
     ]
    }
   ],
   "source": [
    "paper_list.sort(key=lambda paper_counter: tuple(paper_counter.counters.values()),reverse=True)  # OrderdDictなのでvalues順に並べればよい\n",
    "print(paper_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdf_py37",
   "language": "python",
   "name": "pdf_py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "202.542px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
